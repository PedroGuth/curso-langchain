{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üåê **M√≥dulo 9: Deploy e Produ√ß√£o**\n",
       "\n",
       "## **Aula 9.1: Deploy B√°sico - Colocando na Rua**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© Deploy?**\n",
       "\n",
       "Imagina que voc√™ construiu uma **casa incr√≠vel** no seu computador:\n",
       "\n",
       "**Sem Deploy**: A casa fica s√≥ no seu computador. Ningu√©m mais pode ver ou usar.\n",
       "**Com Deploy**: A casa vai para a internet, todo mundo pode acessar e usar!\n",
       "\n",
       "**Deploy** = **Colocar seu projeto na internet** para que outras pessoas possam usar.\n",
       "\n",
       "### **Por que Deploy √© Importante?**\n",
       "\n",
       "**Problema**: Projetos incr√≠veis que ficam s√≥ no computador do desenvolvedor\n",
       "**Solu√ß√£o**: Deploy para que todo mundo possa usar e se beneficiar\n",
       "\n",
       "√â como a diferen√ßa entre **ter uma receita incr√≠vel guardada** vs **abrir um restaurante**! üçΩÔ∏è\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um diagrama mostrando o processo de deploy (desenvolvimento ‚Üí teste ‚Üí produ√ß√£o)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Instalando depend√™ncias para deploy\n",
       "!pip install streamlit gradio fastapi uvicorn\n",
       "\n",
       "# Importando bibliotecas\n",
       "import os\n",
       "import streamlit as st\n",
       "import gradio as gr\n",
       "from dotenv import load_dotenv\n",
       "from langchain_openai import ChatOpenAI\n",
       "\n",
       "# Carregando vari√°veis\n",
       "load_dotenv()\n",
       "\n",
       "print(\"üåê Setup completo para Deploy!\")\n",
       "print(\"üöÄ Streamlit, Gradio e FastAPI prontos\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Deploy com Streamlit - Interface Web Simples**\n",
       "\n",
       "Vamos criar uma interface web simples para nosso chatbot:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando app Streamlit\n",
       "# Como criar uma interface web simples\n",
       "\n",
       "def criar_app_streamlit():\n",
       "    \"\"\"Cria um app Streamlit para o chatbot\"\"\"\n",
       "    \n",
       "    # Configura√ß√£o da p√°gina\n",
       "    st.set_page_config(\n",
       "        page_title=\"Chatbot IA - Pedro Guth\",\n",
       "        page_icon=\"ü§ñ\",\n",
       "        layout=\"wide\"\n",
       "    )\n",
       "    \n",
       "    # T√≠tulo\n",
       "    st.title(\"ü§ñ Chatbot IA - Pedro Guth\")\n",
       "    st.markdown(\"---\")\n",
       "    \n",
       "    # Sidebar com configura√ß√µes\n",
       "    with st.sidebar:\n",
       "        st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
       "        \n",
       "        # API Key\n",
       "        api_key = st.text_input(\n",
       "            \"üîë OpenAI API Key\",\n",
       "            type=\"password\",\n",
       "            help=\"Sua chave da OpenAI\"\n",
       "        )\n",
       "        \n",
       "        # Modelo\n",
       "        modelo = st.selectbox(\n",
       "            \"ü§ñ Modelo\",\n",
       "            [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
       "            index=0\n",
       "        )\n",
       "        \n",
       "        # Temperature\n",
       "        temperature = st.slider(\n",
       "            \"üå°Ô∏è Criatividade\",\n",
       "            min_value=0.0,\n",
       "            max_value=1.0,\n",
       "            value=0.7,\n",
       "            step=0.1\n",
       "        )\n",
       "    \n",
       "    # √Årea principal\n",
       "    if api_key:\n",
       "        # Inicializando modelo\n",
       "        llm = ChatOpenAI(\n",
       "            model=modelo,\n",
       "            temperature=temperature,\n",
       "            api_key=api_key\n",
       "        )\n",
       "        \n",
       "        # Chat\n",
       "        st.header(\"üí¨ Conversa\")\n",
       "        \n",
       "        # Inicializar chat history\n",
       "        if \"messages\" not in st.session_state:\n",
       "            st.session_state.messages = []\n",
       "        \n",
       "        # Mostrar mensagens anteriores\n",
       "        for message in st.session_state.messages:\n",
       "            with st.chat_message(message[\"role\"]):\n",
       "                st.markdown(message[\"content\"])\n",
       "        \n",
       "        # Input do usu√°rio\n",
       "        if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
       "            # Adicionar mensagem do usu√°rio\n",
       "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
       "            with st.chat_message(\"user\"):\n",
       "                st.markdown(prompt)\n",
       "            \n",
       "            # Gerar resposta\n",
       "            with st.chat_message(\"assistant\"):\n",
       "                with st.spinner(\"ü§ñ Pensando...\"):\n",
       "                    try:\n",
       "                        response = llm.invoke([HumanMessage(content=prompt)])\n",
       "                        st.markdown(response.content)\n",
       "                        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
       "                    except Exception as e:\n",
       "                        st.error(f\"‚ùå Erro: {e}\")\n",
       "    else:\n",
       "        st.warning(\"‚ö†Ô∏è Configure sua API Key na sidebar para come√ßar!\")\n",
       "    \n",
       "    # Footer\n",
       "    st.markdown(\"---\")\n",
       "    st.markdown(\"*Desenvolvido com ‚ù§Ô∏è por Pedro Guth*\")\n",
       "\n",
       "print(\"üì± App Streamlit criado!\")\n",
       "print(\"üåê Interface web pronta para deploy\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Salvando o app Streamlit\n",
       "# Como salvar a interface web\n",
       "\n",
       "app_code = \"\"\"\n",
       "import streamlit as st\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain.schema import HumanMessage\n",
       "\n",
       "# Configura√ß√£o da p√°gina\n",
       "st.set_page_config(\n",
       "    page_title=\"Chatbot IA - Pedro Guth\",\n",
       "    page_icon=\"ü§ñ\",\n",
       "    layout=\"wide\"\n",
       ")\n",
       "\n",
       "# T√≠tulo\n",
       "st.title(\"ü§ñ Chatbot IA - Pedro Guth\")\n",
       "st.markdown(\"---\")\n",
       "\n",
       "# Sidebar com configura√ß√µes\n",
       "with st.sidebar:\n",
       "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
       "    \n",
       "    # API Key\n",
       "    api_key = st.text_input(\n",
       "        \"üîë OpenAI API Key\",\n",
       "        type=\"password\",\n",
       "        help=\"Sua chave da OpenAI\"\n",
       "    )\n",
       "    \n",
       "    # Modelo\n",
       "    modelo = st.selectbox(\n",
       "        \"ü§ñ Modelo\",\n",
       "        [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
       "        index=0\n",
       "    )\n",
       "    \n",
       "    # Temperature\n",
       "    temperature = st.slider(\n",
       "        \"üå°Ô∏è Criatividade\",\n",
       "        min_value=0.0,\n",
       "        max_value=1.0,\n",
       "        value=0.7,\n",
       "        step=0.1\n",
       "    )\n",
       "\n",
       "# √Årea principal\n",
       "if api_key:\n",
       "    # Inicializando modelo\n",
       "    llm = ChatOpenAI(\n",
       "        model=modelo,\n",
       "        temperature=temperature,\n",
       "        api_key=api_key\n",
       "    )\n",
       "    \n",
       "    # Chat\n",
       "    st.header(\"üí¨ Conversa\")\n",
       "    \n",
       "    # Inicializar chat history\n",
       "    if \"messages\" not in st.session_state:\n",
       "        st.session_state.messages = []\n",
       "    \n",
       "    # Mostrar mensagens anteriores\n",
       "    for message in st.session_state.messages:\n",
       "        with st.chat_message(message[\"role\"]):\n",
       "            st.markdown(message[\"content\"])\n",
       "    \n",
       "    # Input do usu√°rio\n",
       "    if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
       "        # Adicionar mensagem do usu√°rio\n",
       "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
       "        with st.chat_message(\"user\"):\n",
       "            st.markdown(prompt)\n",
       "        \n",
       "        # Gerar resposta\n",
       "        with st.chat_message(\"assistant\"):\n",
       "            with st.spinner(\"ü§ñ Pensando...\"):\n",
       "                try:\n",
       "                    response = llm.invoke([HumanMessage(content=prompt)])\n",
       "                    st.markdown(response.content)\n",
       "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
       "                except Exception as e:\n",
       "                    st.error(f\"‚ùå Erro: {e}\")\n",
       "else:\n",
       "    st.warning(\"‚ö†Ô∏è Configure sua API Key na sidebar para come√ßar!\")\n",
       "\n",
       "# Footer\n",
       "st.markdown(\"---\")\n",
       "st.markdown(\"*Desenvolvido com ‚ù§Ô∏è por Pedro Guth*\")\n",
       "\"\"\"\n",
       "\n",
       "# Salvando o arquivo\n",
       "with open('app_streamlit.py', 'w', encoding='utf-8') as f:\n",
       "    f.write(app_code)\n",
       "\n",
       "print(\"üíæ App Streamlit salvo como 'app_streamlit.py'\")\n",
       "print(\"üöÄ Para executar: streamlit run app_streamlit.py\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 9.2: Deploy com Gradio - Interface R√°pida**\n",
       "\n",
       "### **Criando Interface com Gradio**\n",
       "\n",
       "Gradio √© mais simples que Streamlit, ideal para prot√≥tipos r√°pidos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando interface Gradio\n",
       "# Como criar uma interface simples e r√°pida\n",
       "\n",
       "def chatbot_gradio(message, history):\n",
       "    \"\"\"Fun√ß√£o do chatbot para Gradio\"\"\"\n",
       "    try:\n",
       "        # Inicializando modelo\n",
       "        llm = ChatOpenAI(\n",
       "            model=\"gpt-3.5-turbo\",\n",
       "            temperature=0.7,\n",
       "            api_key=os.getenv('OPENAI_API_KEY')\n",
       "        )\n",
       "        \n",
       "        # Gerando resposta\n",
       "        response = llm.invoke([HumanMessage(content=message)])\n",
       "        return response.content\n",
       "        \n",
       "    except Exception as e:\n",
       "        return f\"‚ùå Erro: {e}\"\n",
       "\n",
       "# Criando interface Gradio\n",
       "demo = gr.ChatInterface(\n",
       "    fn=chatbot_gradio,\n",
       "    title=\"ü§ñ Chatbot IA - Pedro Guth\",\n",
       "    description=\"Chatbot inteligente desenvolvido com LangChain\",\n",
       "    examples=[\n",
       "        [\"O que √© LangChain?\"],\n",
       "        [\"Como funciona a IA?\"],\n",
       "        [\"Me conte uma piada sobre programa√ß√£o\"]\n",
       "    ],\n",
       "    theme=\"soft\"\n",
       ")\n",
       "\n",
       "print(\"üé® Interface Gradio criada!\")\n",
       "print(\"üöÄ Para executar: demo.launch()\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Salvando app Gradio\n",
       "# Como salvar a interface Gradio\n",
       "\n",
       "gradio_code = \"\"\"\n",
       "import gradio as gr\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain.schema import HumanMessage\n",
       "\n",
       "# Carregando vari√°veis\n",
       "load_dotenv()\n",
       "\n",
       "def chatbot_gradio(message, history):\n",
       "    \"\"\"Fun√ß√£o do chatbot para Gradio\"\"\"\n",
       "    try:\n",
       "        # Inicializando modelo\n",
       "        llm = ChatOpenAI(\n",
       "            model=\"gpt-3.5-turbo\",\n",
       "            temperature=0.7,\n",
       "            api_key=os.getenv('OPENAI_API_KEY')\n",
       "        )\n",
       "        \n",
       "        # Gerando resposta\n",
       "        response = llm.invoke([HumanMessage(content=message)])\n",
       "        return response.content\n",
       "        \n",
       "    except Exception as e:\n",
       "        return f\"‚ùå Erro: {e}\"\n",
       "\n",
       "# Criando interface Gradio\n",
       "demo = gr.ChatInterface(\n",
       "    fn=chatbot_gradio,\n",
       "    title=\"ü§ñ Chatbot IA - Pedro Guth\",\n",
       "    description=\"Chatbot inteligente desenvolvido com LangChain\",\n",
       "    examples=[\n",
       "        [\"O que √© LangChain?\"],\n",
       "        [\"Como funciona a IA?\"],\n",
       "        [\"Me conte uma piada sobre programa√ß√£o\"]\n",
       "    ],\n",
       "    theme=\"soft\"\n",
       ")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    demo.launch()\n",
       "\"\"\"\n",
       "\n",
       "# Salvando o arquivo\n",
       "with open('app_gradio.py', 'w', encoding='utf-8') as f:\n",
       "    f.write(gradio_code)\n",
       "\n",
       "print(\"üíæ App Gradio salvo como 'app_gradio.py'\")\n",
       "print(\"üöÄ Para executar: python app_gradio.py\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 9.3: Deploy com FastAPI - API Profissional**\n",
       "\n",
       "### **Criando API com FastAPI**\n",
       "\n",
       "FastAPI √© ideal para criar APIs profissionais:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando API com FastAPI\n",
       "# Como criar uma API profissional\n",
       "\n",
       "from pydantic import BaseModel\n",
       "from typing import Optional\n",
       "\n",
       "# Modelo de dados\n",
       "class ChatRequest(BaseModel):\n",
       "    message: str\n",
       "    model: Optional[str] = \"gpt-3.5-turbo\"\n",
       "    temperature: Optional[float] = 0.7\n",
       "\n",
       "class ChatResponse(BaseModel):\n",
       "    response: str\n",
       "    model: str\n",
       "    tokens_used: Optional[int] = None\n",
       "\n",
       "fastapi_code = \"\"\"\n",
       "from fastapi import FastAPI, HTTPException\n",
       "from pydantic import BaseModel\n",
       "from typing import Optional\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain.schema import HumanMessage\n",
       "\n",
       "# Carregando vari√°veis\n",
       "load_dotenv()\n",
       "\n",
       "# Criando app FastAPI\n",
       "app = FastAPI(\n",
       "    title=\"Chatbot IA API\",\n",
       "    description=\"API do Chatbot IA desenvolvido com LangChain\",\n",
       "    version=\"1.0.0\"\n",
       ")\n",
       "\n",
       "# Modelos de dados\n",
       "class ChatRequest(BaseModel):\n",
       "    message: str\n",
       "    model: Optional[str] = \"gpt-3.5-turbo\"\n",
       "    temperature: Optional[float] = 0.7\n",
       "\n",
       "class ChatResponse(BaseModel):\n",
       "    response: str\n",
       "    model: str\n",
       "    tokens_used: Optional[int] = None\n",
       "\n",
       "# Rota principal\n",
       "@app.get(\"/\")\n",
       "async def root():\n",
       "    return {\"message\": \"Chatbot IA API - Pedro Guth\"}\n",
       "\n",
       "# Rota de chat\n",
       "@app.post(\"/chat\", response_model=ChatResponse)\n",
       "async def chat(request: ChatRequest):\n",
       "    try:\n",
       "        # Verificar API key\n",
       "        api_key = os.getenv('OPENAI_API_KEY')\n",
       "        if not api_key:\n",
       "            raise HTTPException(status_code=500, detail=\"API key n√£o configurada\")\n",
       "        \n",
       "        # Inicializando modelo\n",
       "        llm = ChatOpenAI(\n",
       "            model=request.model,\n",
       "            temperature=request.temperature,\n",
       "            api_key=api_key\n",
       "        )\n",
       "        \n",
       "        # Gerando resposta\n",
       "        response = llm.invoke([HumanMessage(content=request.message)])\n",
       "        \n",
       "        return ChatResponse(\n",
       "            response=response.content,\n",
       "            model=request.model\n",
       "        )\n",
       "        \n",
       "    except Exception as e:\n",
       "        raise HTTPException(status_code=500, detail=str(e))\n",
       "\n",
       "# Rota de sa√∫de\n",
       "@app.get(\"/health\")\n",
       "async def health_check():\n",
       "    return {\"status\": \"healthy\", \"service\": \"chatbot-ia\"}\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    import uvicorn\n",
       "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
       "\"\"\"\n",
       "\n",
       "# Salvando o arquivo\n",
       "with open('app_fastapi.py', 'w', encoding='utf-8') as f:\n",
       "    f.write(fastapi_code)\n",
       "\n",
       "print(\"üíæ API FastAPI salva como 'app_fastapi.py'\")\n",
       "print(\"üöÄ Para executar: python app_fastapi.py\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 9.4: Monitoramento e Otimiza√ß√£o**\n",
       "\n",
       "### **Boas Pr√°ticas de Produ√ß√£o**\n",
       "\n",
       "Vamos criar um sistema de monitoramento b√°sico:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando sistema de monitoramento\n",
       "# Como monitorar o desempenho da aplica√ß√£o\n",
       "\n",
       "import time\n",
       "import logging\n",
       "from datetime import datetime\n",
       "\n",
       "# Configurando logging\n",
       "logging.basicConfig(\n",
       "    level=logging.INFO,\n",
       "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
       "    handlers=[\n",
       "        logging.FileHandler('app.log'),\n",
       "        logging.StreamHandler()\n",
       "    ]\n",
       ")\n",
       "\n",
       "logger = logging.getLogger(__name__)\n",
       "\n",
       "class Monitoramento:\n",
       "    def __init__(self):\n",
       "        self.requests = 0\n",
       "        self.errors = 0\n",
       "        self.start_time = datetime.now()\n",
       "    \n",
       "    def log_request(self, success=True):\n",
       "        \"\"\"Registra uma requisi√ß√£o\"\"\"\n",
       "        self.requests += 1\n",
       "        if not success:\n",
       "            self.errors += 1\n",
       "        \n",
       "        logger.info(f\"Requisi√ß√£o {self.requests}: {'Sucesso' if success else 'Erro'}\")\n",
       "    \n",
       "    def get_stats(self):\n",
       "        \"\"\"Retorna estat√≠sticas\"\"\"\n",
       "        uptime = datetime.now() - self.start_time\n",
       "        error_rate = (self.errors / self.requests * 100) if self.requests > 0 else 0\n",
       "        \n",
       "        return {\n",
       "            \"total_requests\": self.requests,\n",
       "            \"total_errors\": self.errors,\n",
       "            \"error_rate\": f\"{error_rate:.2f}%\",\n",
       "            \"uptime\": str(uptime).split('.')[0]\n",
       "        }\n",
       "\n",
       "# Fun√ß√£o de chat com monitoramento\n",
       "monitor = Monitoramento()\n",
       "\n",
       "def chat_com_monitoramento(message):\n",
       "    \"\"\"Chat com monitoramento de performance\"\"\"\n",
       "    start_time = time.time()\n",
       "    \n",
       "    try:\n",
       "        # Inicializando modelo\n",
       "        llm = ChatOpenAI(\n",
       "            model=\"gpt-3.5-turbo\",\n",
       "            temperature=0.7,\n",
       "            api_key=os.getenv('OPENAI_API_KEY')\n",
       "        )\n",
       "        \n",
       "        # Gerando resposta\n",
       "        response = llm.invoke([HumanMessage(content=message)])\n",
       "        \n",
       "        # Calculando tempo\n",
       "        execution_time = time.time() - start_time\n",
       "        \n",
       "        # Logging\n",
       "        logger.info(f\"Resposta gerada em {execution_time:.2f}s\")\n",
       "        monitor.log_request(success=True)\n",
       "        \n",
       "        return response.content\n",
       "        \n",
       "    except Exception as e:\n",
       "        logger.error(f\"Erro: {e}\")\n",
       "        monitor.log_request(success=False)\n",
       "        return f\"‚ùå Erro: {e}\"\n",
       "\n",
       "print(\"üìä Sistema de monitoramento criado!\")\n",
       "print(\"üìà Logs salvos em 'app.log'\")\n",
       "print(\"üìã Estat√≠sticas dispon√≠veis via monitor.get_stats()\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Testando o sistema de monitoramento\n",
       "# Vamos ver o monitoramento em a√ß√£o!\n",
       "\n",
       "print(\"üìä TESTE: SISTEMA DE MONITORAMENTO\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Testando algumas requisi√ß√µes\n",
       "testes = [\n",
       "    \"Ol√°! Como voc√™ est√°?\",\n",
       "    \"O que √© LangChain?\",\n",
       "    \"Me conte uma piada\"\n",
       "]\n",
       "\n",
       "for i, teste in enumerate(testes, 1):\n",
       "    print(f\"\\nüß™ Teste {i}: {teste}\")\n",
       "    \n",
       "    try:\n",
       "        resposta = chat_com_monitoramento(teste)\n",
       "        print(f\"ü§ñ Resposta: {resposta[:100]}...\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 30)\n",
       "\n",
       "# Mostrando estat√≠sticas\n",
       "print(\"\\nüìà ESTAT√çSTICAS:\")\n",
       "stats = monitor.get_stats()\n",
       "for key, value in stats.items():\n",
       "    print(f\"üìä {key}: {value}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Na Pr√°tica, Meu Consagrado!** üí™\n",
       "\n",
       "**O que aprendemos sobre Deploy:**\n",
       "\n",
       "1. ‚úÖ **Streamlit** - Interface web simples e bonita\n",
       "2. ‚úÖ **Gradio** - Interface r√°pida para prot√≥tipos\n",
       "3. ‚úÖ **FastAPI** - API profissional e escal√°vel\n",
       "4. ‚úÖ **Monitoramento** - Logs e estat√≠sticas de performance\n",
       "\n",
       "### **Compara√ß√£o das Ferramentas**\n",
       "\n",
       "| Ferramenta | Uso | Complexidade | Escalabilidade |\n",
       "|------------|-----|--------------|----------------|\n",
       "| **Streamlit** | Prot√≥tipos, Dashboards | Baixa | M√©dia |\n",
       "| **Gradio** | Demos r√°pidos | Muito Baixa | Baixa |\n",
       "| **FastAPI** | APIs profissionais | M√©dia | Alta |\n",
       "\n",
       "### **Pr√≥ximos Passos**\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Screenshots das interfaces funcionando\n",
       "\n",
       "**üéØ Pr√≥ximo m√≥dulo**: Vamos aprender sobre **T√≥picos Avan√ßados** - LangGraph e integra√ß√µes!\n",
       "\n",
       "**üí° Resumo do M√≥dulo 9**:\n",
       "- ‚úÖ Deploy com Streamlit, Gradio e FastAPI\n",
       "- ‚úÖ Monitoramento e otimiza√ß√£o\n",
       "- ‚úÖ Boas pr√°ticas de produ√ß√£o\n",
       "- ‚úÖ Sistemas prontos para produ√ß√£o\n",
       "\n",
       "**üåê Agora voc√™ sabe colocar projetos na internet!**"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 