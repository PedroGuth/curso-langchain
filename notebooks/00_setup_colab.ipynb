{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbo-B1k5-Ft3"
      },
      "source": [
        "# üöÄ **Setup Google Colab - Aprenda LangChain Gratuito!**\n",
        "\n",
        "## **üéØ Por que Google Colab?**\n",
        "\n",
        "O Google Colab √© **perfeito** para aprender LangChain porque:\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Sem instala√ß√£o** - funciona no navegador\n",
        "- ‚úÖ **GPU gratuita** dispon√≠vel\n",
        "- ‚úÖ **Compartilhamento f√°cil**\n",
        "- ‚úÖ **Backup autom√°tico** no Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "## **üÜì Op√ß√µes Dispon√≠veis no Colab:**\n",
        "\n",
        "### **1. üé≠ Mock LLM (Recomendado para Iniciantes)**\n",
        "- ‚úÖ **Funciona imediatamente**\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Respostas simuladas realistas**\n",
        "- ‚úÖ **Perfeito para aprender conceitos**\n",
        "\n",
        "### **2. ü§ñ Google AI Studio (Recomendado para o projeto)**\n",
        "- ‚úÖ **Funciona imediatamente**\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Respostas r√°pidas**\n",
        "- ‚úÖ **Perfeito para o curso**\n",
        "\n",
        "### **3. üåê Hugging Face (Modelos Reais)**\n",
        "- ‚úÖ **30.000 requisi√ß√µes/m√™s gratuitas**\n",
        "- ‚úÖ **Modelos reais de IA**\n",
        "- ‚úÖ **F√°cil configura√ß√£o**\n",
        "- ‚úÖ **Boa qualidade**\n",
        "\n",
        "### **4. üîë OpenAI (Para Quem Quiser)**\n",
        "- ‚úÖ **Melhor qualidade**\n",
        "- ‚úÖ **Modelos mais avan√ßados**\n",
        "- ‚ùå **Custo por uso**\n",
        "- ‚ùå **Precisa de API key**\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Dica do Pedro**: Comece com Mock LLM para aprender os conceitos. Quando estiver confort√°vel, migre para Hugging Face ou OpenAI para respostas reais!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ku76N6O-Ft5"
      },
      "source": [
        "## **üîß Instala√ß√£o das Depend√™ncias**\n",
        "\n",
        "Primeiro, vamos instalar tudo que precisamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPNmMhub-Ft6"
      },
      "outputs": [],
      "source": [
        "# Instalando as depend√™ncias necess√°rias\n",
        "# Execute esta c√©lula primeiro!\n",
        "\n",
        "!pip install --quiet langchain openai python-dotenv\n",
        "!pip install --quiet langchain-community langchain-core\n",
        "!pip install --quiet --upgrade langchain-huggingface transformers sentencepiece torch\n",
        "!pip install --quiet huggingface_hub\n",
        "!pip install --quiet langchain-openai openai\n",
        "\n",
        "# Para document loaders\n",
        "!pip install --quiet pypdf python-docx beautifulsoup4 requests youtube-transcript-api\n",
        "\n",
        "# Para vector stores\n",
        "!pip install --quiet chromadb faiss-cpu\n",
        "\n",
        "# Para agents e ferramentas\n",
        "!pip install --quiet wikipedia duckduckgo-search\n",
        "\n",
        "# Para deploy e interfaces\n",
        "!pip install --quiet streamlit gradio fastapi uvicorn\n",
        "\n",
        "# Para processamento de dados\n",
        "!pip install --quiet pandas numpy matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Todas as depend√™ncias instaladas com sucesso!\")\n",
        "print(\"üöÄ Agora vamos configurar o LLM...\")\n",
        "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
        "print(\"üöÄ Agora vamos importar o que precisamos...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM_sV8VX-Ft6"
      },
      "source": [
        "## **üé≠ Op√ß√£o 1: Mock LLM (Recomendado para Iniciantes)**\n",
        "\n",
        "**Perfeito para:**\n",
        "- Aprender conceitos do LangChain\n",
        "- Testar estruturas de c√≥digo\n",
        "- Quando n√£o tem internet\n",
        "- Demonstra√ß√µes em sala de aula\n",
        "\n",
        "**Como funciona:**\n",
        "- Respostas pr√©-definidas realistas\n",
        "- Simula comportamento de IA real\n",
        "- 100% gratuito e offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muhCVkJU-Ft7"
      },
      "outputs": [],
      "source": [
        "# Mock LLM - Respostas simuladas para aprender\n",
        "from langchain.llms.fake import FakeListLLM\n",
        "\n",
        "# Criando respostas simuladas realistas para o curso\n",
        "respostas_mock = [\n",
        "    \"Aqui est√° a resposta para sua pergunta sobre LangChain:\",\n",
        "    \"Vou te ajudar a entender esse conceito:\",\n",
        "    \"Essa √© uma excelente pergunta sobre IA!\",\n",
        "    \"Deixe-me explicar como LangChain funciona:\",\n",
        "    \"Aqui est√£o as informa√ß√µes que voc√™ precisa:\",\n",
        "    \"Vou criar um exemplo pr√°tico para voc√™:\",\n",
        "    \"Essa solu√ß√£o vai funcionar perfeitamente:\",\n",
        "    \"Aqui est√° o que voc√™ precisa saber sobre isso:\",\n",
        "    \"Vou te mostrar como implementar isso:\",\n",
        "    \"Essa √© a melhor abordagem para resolver:\",\n",
        "    \"Baseado no seu c√≥digo, aqui est√° a solu√ß√£o:\",\n",
        "    \"Vou te explicar passo a passo como fazer:\",\n",
        "    \"Aqui est√° um exemplo completo:\",\n",
        "    \"Essa t√©cnica √© muito √∫til para:\",\n",
        "    \"Vou te mostrar as melhores pr√°ticas:\"\n",
        "]\n",
        "\n",
        "# Criando o Mock LLM\n",
        "llm_mock = FakeListLLM(responses=respostas_mock)\n",
        "\n",
        "print(\"‚úÖ Mock LLM configurado com sucesso!\")\n",
        "print(\"üé≠ Usando respostas simuladas para demonstra√ß√£o\")\n",
        "print(f\"üìù Total de respostas: {len(respostas_mock)}\")\n",
        "\n",
        "# Teste r√°pido\n",
        "print(\"\\nüß™ Testando Mock LLM:\")\n",
        "for i in range(3):\n",
        "    response = llm_mock.invoke(f\"Pergunta teste {i+1}\")\n",
        "    print(f\"üí¨ Resposta {i+1}: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNa9rt_XdfGl"
      },
      "source": [
        "## **üåê Op√ß√£o 2: Google AI Studio (Modelos Reais)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [aistudio.google](http://aistudio.google.com/)\n",
        "2. Escolha Get API Key quando criar a conta\n",
        "3. Clique em Cria chave de API (https://aistudio.google.com/apikey)\n",
        "4. Exemplo: AIzaSyA11ri4NmJ2p1DlXxYxUseyHtTSczWS-pk\n",
        "\n",
        "### **Limites gratuitos:**\n",
        "- 30.000 requisi√ß√µes/m√™s\n",
        "- Modelos menores (mas funcionais)\n",
        "- Sempre online"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configurar API key do Google\n",
        "google_key = \"Coloca a chave aqiu meu bem\"\n",
        "if google_key:\n",
        "    genai.configure(api_key=google_key)\n",
        "\n",
        "    # Criar modelo Gemini\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash') # Usar sempre esse\n",
        "\n",
        "    # Criar wrapper para LangChain\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    llm_google = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        temperature=0.7,\n",
        "        google_api_key=google_key\n",
        "    )\n",
        "    print(\"‚úÖ Google Gemini configurado!\")"
      ],
      "metadata": {
        "id": "3qADhGLZeLKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki4t9mbB-Ft7"
      },
      "source": [
        "## **üåê Op√ß√£o 3: Hugging Face (Modelos Reais)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [huggingface.co](https://huggingface.co)\n",
        "2. Escolha um modelo com menos de 4B de par√¢metros\n",
        "3. Copie seu t√≠tulo\n",
        "4. Exemplo: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
        "\n",
        "### **Limites gratuitos:**\n",
        "- 30.000 requisi√ß√µes/m√™s\n",
        "- Modelos menores (mas funcionais)\n",
        "- Sempre online"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Colocar modelo escolhido aqui!\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\")"
      ],
      "metadata": {
        "id": "Gdci8gJj-H5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARRk1NAH-Ft7"
      },
      "outputs": [],
      "source": [
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0, # Usar GPU\n",
        "    max_new_tokens=100,  # Aumentado\n",
        "    do_sample=True,\n",
        "    temperature=0.8,  # Aumentado para mais criatividade\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,  # Evita repeti√ß√£o\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm_hf = HuggingFacePipeline(pipeline=text_generator)\n",
        "print(\"‚úÖ Hugging Face (local) configurado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoiLGju--Ft8"
      },
      "source": [
        "## **üîë Op√ß√£o 4: OpenAI (Para Quem Quiser)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [platform.openai.com](https://platform.openai.com)\n",
        "2. V√° em API Keys\n",
        "3. Crie uma nova API key\n",
        "4. Cole a chave abaixo\n",
        "\n",
        "### **Custos:**\n",
        "- 0.002 d√≥lares por 1.000 tokens\n",
        "- 5-20 d√≥lares para todo o curso\n",
        "- Melhor qualidade dispon√≠vel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zYIiZDl-Ft8"
      },
      "outputs": [],
      "source": [
        "# Configurando OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Cole sua API key da OpenAI aqui\n",
        "OPENAI_API_KEY = \"\"  # Substitua pela sua API key\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "    try:\n",
        "        # Modelo da OpenAI\n",
        "        llm_openai = ChatOpenAI(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            temperature=0.7,\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ OpenAI configurado com sucesso!\")\n",
        "        print(f\"ü§ñ Modelo: {llm_openai.model_name}\")\n",
        "\n",
        "        # Teste r√°pido\n",
        "        from langchain.schema import HumanMessage\n",
        "        response = llm_openai.invoke([HumanMessage(content=\"Diga ol√° em portugu√™s\")])\n",
        "        print(f\"üí¨ Resposta: {response.content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro na OpenAI: {e}\")\n",
        "        print(\"üîë Verifique se a API key est√° correta e tem cr√©ditos\")\n",
        "else:\n",
        "    print(\"üîë API key da OpenAI n√£o configurada\")\n",
        "    print(\"üìù Para usar, cole sua API key na vari√°vel OPENAI_API_KEY acima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRYg67UQ-Ft9"
      },
      "source": [
        "## **üìã C√©lula Universal para Outros Notebooks**\n",
        "\n",
        "Agora que voc√™ tem o LLM configurado, use esta c√©lula em **todos os outros notebooks** do curso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT6wA6Nh-Ft9"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas principais\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Carregando vari√°veis de ambiente (vamos configurar isso depois)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"üì¶ Bibliotecas importadas com sucesso!\")\n",
        "print(\"üîß Pr√≥ximo passo: configurar a API key\")\n",
        "\n",
        "# Criando arquivo .env (se n√£o existir)\n",
        "if not os.path.exists('.env'):\n",
        "    with open('.env', 'w') as f:\n",
        "        f.write('# Suas chaves de API aqui\\n')\n",
        "        f.write('GOOGLE_API_KEY=\"XXXXCHAVEAQUIXXXXX\"\\n')\n",
        "        f.write('OPENAI_API_KEY=\"XXXXCHAVEAQUIXXXXX\"\\n')\n",
        "    print(\"üìù Arquivo .env criado!\")\n",
        "    print(\"ÔøΩÔøΩ Agora adicione sua API key no arquivo .env\")\n",
        "else:\n",
        "    print(\"‚úÖ Arquivo .env j√° existe!\")\n",
        "\n",
        "# Verificando se a API key est√° configurada\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "if google_api_key and google_api_key != 'XXXXCHAVEAQUIXXXXX':\n",
        "    print(\"üéâ Google API key configurada com sucesso!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Configure sua Google API key no arquivo .env\")\n",
        "\n",
        "# Carregando vari√°veis do .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm_colab():\n",
        "    \"\"\"Retorna o melhor LLM dispon√≠vel no Colab\"\"\"\n",
        "\n",
        "    # --- Tentativa 1: Google Gemini (Primeira op√ß√£o) ---\n",
        "    try:\n",
        "        import google.generativeai as genai\n",
        "\n",
        "        # Configurar API key do Google\n",
        "        google_key = os.getenv('GOOGLE_API_KEY')\n",
        "        if google_key:\n",
        "            genai.configure(api_key=google_key)\n",
        "\n",
        "            # Criar modelo Gemini\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "            # Criar wrapper para LangChain\n",
        "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "            llm_google = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                temperature=0.7,\n",
        "                google_api_key=google_key\n",
        "            )\n",
        "            print(\"‚úÖ Google Gemini configurado!\")\n",
        "            return llm_google\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google Gemini falhou: {e}\")\n",
        "\n",
        "    # --- Tentativa 2: Hugging Face Local ---\n",
        "    try:\n",
        "        # Modelo local\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=-1,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        llm_hf = HuggingFacePipeline(pipeline=text_generator)\n",
        "        print(\"‚úÖ Hugging Face (local) configurado!\")\n",
        "        return llm_hf\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Hugging Face falhou: {e}\")\n",
        "\n",
        "    # --- Tentativa 3: OpenAI ---\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if openai_key:\n",
        "            llm_openai = ChatOpenAI(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.7,\n",
        "                api_key=openai_key\n",
        "            )\n",
        "            print(\"‚úÖ OpenAI configurado!\")\n",
        "            return llm_openai\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è OpenAI falhou: {e}\")\n",
        "\n",
        "    # --- Op√ß√£o 4: Mock LLM ---\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta:\", \"Vou te ajudar:\", \"Essa √© uma boa pergunta!\",\n",
        "        \"Deixe-me explicar:\", \"Aqui est√£o as informa√ß√µes:\", \"Vou criar um exemplo:\"\n",
        "    ]\n",
        "    print(\"‚ÑπÔ∏è Usando FakeListLLM como fallback\")\n",
        "    return FakeListLLM(responses=respostas)\n",
        "\n",
        "# --- Configurando o LLM ---\n",
        "llm = get_llm_colab()\n",
        "\n",
        "print(\"ÔøΩÔøΩ LLM configurado no Colab!\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"üí° Agora voc√™ pode usar 'llm' em todos os exemplos do curso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "032Nxb8X-Ft9"
      },
      "source": [
        "## **üí∞ Compara√ß√£o de Custos no Colab**\n",
        "\n",
        "| Op√ß√£o | Custo | Limites | Qualidade | Facilidade |\n",
        "|-------|-------|---------|-----------|------------|\n",
        "| **Mock LLM** | \\$0 | Nenhum | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Hugging Face** | \\$0 | 30K/m√™s | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Google AI Studio** | \\$0 | 20K/m√™s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **OpenAI** | \\~$5-20 | Sem limite | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Recomenda√ß√£o para o Curso**\n",
        "\n",
        "### **Para Iniciantes:**\n",
        "1. **Comece com Mock LLM** - Aprenda os conceitos\n",
        "2. **Configure Google AI Studio** - Para respostas reais\n",
        "3. **Use OpenAI** - Se quiser melhor qualidade\n",
        "\n",
        "### **Para Desenvolvedores:**\n",
        "1. **Hugging Face** - Para desenvolvimento\n",
        "2. **OpenAI** - Para produ√ß√£o\n",
        "3. **Mock LLM** - Para testes r√°pidos\n",
        "\n",
        "---\n",
        "\n",
        "## **üöÄ Pr√≥ximos Passos**\n",
        "\n",
        "1. **Teste o LLM configurado** executando a c√©lula acima\n",
        "2. **Copie a vari√°vel 'llm'** para usar nos outros notebooks\n",
        "3. **Comece o curso** com o m√≥dulo 1\n",
        "4. **Divirta-se aprendendo LangChain!**\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Parab√©ns! Voc√™ est√° pronto para aprender LangChain no Google Colab!**\n",
        "\n",
        "**üí° Dica do Pedro**: O Mock LLM √© perfeito para aprender os conceitos. Quando estiver confort√°vel, configure Google AI Studio ou OpenAI para respostas reais!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_setup_colab",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}