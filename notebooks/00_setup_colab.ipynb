{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbo-B1k5-Ft3"
      },
      "source": [
        "# üöÄ **Setup Google Colab - Aprenda LangChain Gratuito!**\n",
        "\n",
        "## **üéØ Por que Google Colab?**\n",
        "\n",
        "O Google Colab √© **perfeito** para aprender LangChain porque:\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Sem instala√ß√£o** - funciona no navegador\n",
        "- ‚úÖ **GPU gratuita** dispon√≠vel\n",
        "- ‚úÖ **Compartilhamento f√°cil**\n",
        "- ‚úÖ **Backup autom√°tico** no Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "## **üÜì Op√ß√µes Dispon√≠veis no Colab:**\n",
        "\n",
        "### **1. üé≠ Mock LLM (Recomendado para Iniciantes)**\n",
        "- ‚úÖ **Funciona imediatamente**\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Respostas simuladas realistas**\n",
        "- ‚úÖ **Perfeito para aprender conceitos**\n",
        "\n",
        "### **2. üåê Hugging Face (Modelos Reais)**\n",
        "- ‚úÖ **30.000 requisi√ß√µes/m√™s gratuitas**\n",
        "- ‚úÖ **Modelos reais de IA**\n",
        "- ‚úÖ **F√°cil configura√ß√£o**\n",
        "- ‚úÖ **Boa qualidade**\n",
        "\n",
        "### **3. üîë OpenAI (Para Quem Quiser)**\n",
        "- ‚úÖ **Melhor qualidade**\n",
        "- ‚úÖ **Modelos mais avan√ßados**\n",
        "- ‚ùå **Custo por uso**\n",
        "- ‚ùå **Precisa de API key**\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Dica do Pedro**: Comece com Mock LLM para aprender os conceitos. Quando estiver confort√°vel, migre para Hugging Face ou OpenAI para respostas reais!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ku76N6O-Ft5"
      },
      "source": [
        "## **üîß Instala√ß√£o das Depend√™ncias**\n",
        "\n",
        "Primeiro, vamos instalar tudo que precisamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPNmMhub-Ft6"
      },
      "outputs": [],
      "source": [
        "# Instalando todas as depend√™ncias necess√°rias\n",
        "!pip install langchain langchain-community langchain-core python-dotenv\n",
        "!pip install huggingface_hub\n",
        "!pip install langchain-openai openai\n",
        "\n",
        "# Para document loaders\n",
        "!pip install pypdf python-docx beautifulsoup4 requests youtube-transcript-api\n",
        "\n",
        "# Para vector stores\n",
        "!pip install chromadb faiss-cpu\n",
        "\n",
        "# Para agents e ferramentas\n",
        "!pip install wikipedia duckduckgo-search\n",
        "\n",
        "# Para deploy e interfaces\n",
        "!pip install streamlit gradio fastapi uvicorn\n",
        "\n",
        "# Para processamento de dados\n",
        "!pip install pandas numpy matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Todas as depend√™ncias instaladas com sucesso!\")\n",
        "print(\"üöÄ Agora vamos configurar o LLM...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM_sV8VX-Ft6"
      },
      "source": [
        "## **üé≠ Op√ß√£o 1: Mock LLM (Recomendado para Iniciantes)**\n",
        "\n",
        "**Perfeito para:**\n",
        "- Aprender conceitos do LangChain\n",
        "- Testar estruturas de c√≥digo\n",
        "- Quando n√£o tem internet\n",
        "- Demonstra√ß√µes em sala de aula\n",
        "\n",
        "**Como funciona:**\n",
        "- Respostas pr√©-definidas realistas\n",
        "- Simula comportamento de IA real\n",
        "- 100% gratuito e offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muhCVkJU-Ft7"
      },
      "outputs": [],
      "source": [
        "# Mock LLM - Respostas simuladas para aprender\n",
        "from langchain.llms.fake import FakeListLLM\n",
        "\n",
        "# Criando respostas simuladas realistas para o curso\n",
        "respostas_mock = [\n",
        "    \"Aqui est√° a resposta para sua pergunta sobre LangChain:\",\n",
        "    \"Vou te ajudar a entender esse conceito:\",\n",
        "    \"Essa √© uma excelente pergunta sobre IA!\",\n",
        "    \"Deixe-me explicar como LangChain funciona:\",\n",
        "    \"Aqui est√£o as informa√ß√µes que voc√™ precisa:\",\n",
        "    \"Vou criar um exemplo pr√°tico para voc√™:\",\n",
        "    \"Essa solu√ß√£o vai funcionar perfeitamente:\",\n",
        "    \"Aqui est√° o que voc√™ precisa saber sobre isso:\",\n",
        "    \"Vou te mostrar como implementar isso:\",\n",
        "    \"Essa √© a melhor abordagem para resolver:\",\n",
        "    \"Baseado no seu c√≥digo, aqui est√° a solu√ß√£o:\",\n",
        "    \"Vou te explicar passo a passo como fazer:\",\n",
        "    \"Aqui est√° um exemplo completo:\",\n",
        "    \"Essa t√©cnica √© muito √∫til para:\",\n",
        "    \"Vou te mostrar as melhores pr√°ticas:\"\n",
        "]\n",
        "\n",
        "# Criando o Mock LLM\n",
        "llm_mock = FakeListLLM(responses=respostas_mock)\n",
        "\n",
        "print(\"‚úÖ Mock LLM configurado com sucesso!\")\n",
        "print(\"üé≠ Usando respostas simuladas para demonstra√ß√£o\")\n",
        "print(f\"üìù Total de respostas: {len(respostas_mock)}\")\n",
        "\n",
        "# Teste r√°pido\n",
        "print(\"\\nüß™ Testando Mock LLM:\")\n",
        "for i in range(3):\n",
        "    response = llm_mock.invoke(f\"Pergunta teste {i+1}\")\n",
        "    print(f\"üí¨ Resposta {i+1}: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki4t9mbB-Ft7"
      },
      "source": [
        "## **üåê Op√ß√£o 2: Hugging Face (Modelos Reais)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [huggingface.co](https://huggingface.co)\n",
        "2. V√° em Settings > Access Tokens\n",
        "3. Crie um token gratuito\n",
        "4. Cole o token abaixo\n",
        "\n",
        "### **Limites gratuitos:**\n",
        "- 30.000 requisi√ß√µes/m√™s\n",
        "- Modelos menores (mas funcionais)\n",
        "- Sempre online"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Modelo Flan-T5 local\n",
        "model_name = \"google/flan-t5-small\"  # ou \"flan-t5-large\" se Colab tiver mem√≥ria suficiente\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Criar pipeline de texto\n",
        "text_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0  # GPU do Colab (0), ou -1 para CPU\n",
        ")"
      ],
      "metadata": {
        "id": "Gdci8gJj-H5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARRk1NAH-Ft7"
      },
      "outputs": [],
      "source": [
        "# Configurando Hugging Face\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "import os\n",
        "\n",
        "# Cole seu token do Hugging Face aqui\n",
        "HUGGINGFACE_TOKEN = \"\"  # Substitua pelo seu token\n",
        "\n",
        "if HUGGINGFACE_TOKEN:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_TOKEN\n",
        "\n",
        "    try:\n",
        "        # Modelo gratuito do Google\n",
        "        llm_hf = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "        # Teste\n",
        "        response = llm_hf.predict(\"Translate to Portuguese: Hello world\")\n",
        "        print(response)\n",
        "\n",
        "        print(\"‚úÖ Hugging Face configurado com sucesso!\")\n",
        "        print(f\"ü§ñ Modelo: {llm_hf.repo_id}\")\n",
        "\n",
        "        # Teste r√°pido\n",
        "        response = llm_hf.invoke(\"Translate to Portuguese: Hello world\")\n",
        "        print(f\"üí¨ Resposta: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro no Hugging Face: {e}\")\n",
        "        print(\"üîë Verifique se o token est√° correto\")\n",
        "else:\n",
        "    print(\"üîë Token do Hugging Face n√£o configurado\")\n",
        "    print(\"üìù Para usar, cole seu token na vari√°vel HUGGINGFACE_TOKEN acima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoiLGju--Ft8"
      },
      "source": [
        "## **üîë Op√ß√£o 3: OpenAI (Para Quem Quiser)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [platform.openai.com](https://platform.openai.com)\n",
        "2. V√° em API Keys\n",
        "3. Crie uma nova API key\n",
        "4. Cole a chave abaixo\n",
        "\n",
        "### **Custos:**\n",
        "- ~$0.002 por 1.000 tokens\n",
        "- ~$5-20 para todo o curso\n",
        "- Melhor qualidade dispon√≠vel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zYIiZDl-Ft8"
      },
      "outputs": [],
      "source": [
        "# Configurando OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Cole sua API key da OpenAI aqui\n",
        "OPENAI_API_KEY = \"\"  # Substitua pela sua API key\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "    try:\n",
        "        # Modelo da OpenAI\n",
        "        llm_openai = ChatOpenAI(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            temperature=0.7,\n",
        "            api_key=OPENAI_API_KEY\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ OpenAI configurado com sucesso!\")\n",
        "        print(f\"ü§ñ Modelo: {llm_openai.model_name}\")\n",
        "\n",
        "        # Teste r√°pido\n",
        "        from langchain.schema import HumanMessage\n",
        "        response = llm_openai.invoke([HumanMessage(content=\"Diga ol√° em portugu√™s\")])\n",
        "        print(f\"üí¨ Resposta: {response.content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro na OpenAI: {e}\")\n",
        "        print(\"üîë Verifique se a API key est√° correta e tem cr√©ditos\")\n",
        "else:\n",
        "    print(\"üîë API key da OpenAI n√£o configurada\")\n",
        "    print(\"üìù Para usar, cole sua API key na vari√°vel OPENAI_API_KEY acima\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQHpBGo-Ft8"
      },
      "source": [
        "## **üîß Configura√ß√£o Universal para o Curso**\n",
        "\n",
        "Agora vou criar uma fun√ß√£o que detecta automaticamente qual op√ß√£o est√° dispon√≠vel e configura o melhor LLM para voc√™:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj7HSgsT-Ft8"
      },
      "outputs": [],
      "source": [
        "# Fun√ß√£o para detectar e configurar o melhor LLM dispon√≠vel\n",
        "def configurar_llm_colab():\n",
        "    \"\"\"\n",
        "    Detecta automaticamente qual op√ß√£o est√° dispon√≠vel\n",
        "    e configura o melhor LLM para o curso no Colab\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîç Detectando op√ß√µes dispon√≠veis no Colab...\")\n",
        "\n",
        "    # Tentativa 1: OpenAI (melhor qualidade)\n",
        "    if os.getenv(\"OPENAI_API_KEY\") or OPENAI_API_KEY:\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "            api_key = os.getenv(\"OPENAI_API_KEY\") or OPENAI_API_KEY\n",
        "            llm = ChatOpenAI(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.7,\n",
        "                api_key=api_key\n",
        "            )\n",
        "\n",
        "            # Teste r√°pido\n",
        "            from langchain.schema import HumanMessage\n",
        "            test_response = llm.invoke([HumanMessage(content=\"Teste\")])\n",
        "\n",
        "            print(\"‚úÖ OpenAI detectado e configurado!\")\n",
        "            print(\"üöÄ Usando modelo da OpenAI (melhor qualidade)\")\n",
        "            return llm, \"openai\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OpenAI n√£o dispon√≠vel: {e}\")\n",
        "\n",
        "    # Tentativa 2: Hugging Face\n",
        "    if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") or HUGGINGFACE_TOKEN:\n",
        "        try:\n",
        "            from langchain_community.llms import HuggingFaceHub\n",
        "            token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") or HUGGINGFACE_TOKEN\n",
        "            os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
        "\n",
        "            llm = HuggingFaceHub(\n",
        "                repo_id=\"google/flan-t5-base\",\n",
        "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "            )\n",
        "\n",
        "            # Teste r√°pido\n",
        "            test_response = llm.invoke(\"Test\")\n",
        "\n",
        "            print(\"‚úÖ Hugging Face detectado e configurado!\")\n",
        "            print(\"üåê Usando modelo na nuvem (gratuito com limites)\")\n",
        "            return llm, \"huggingface\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Hugging Face n√£o dispon√≠vel: {e}\")\n",
        "\n",
        "    # Op√ß√£o 3: Mock LLM (sempre funciona)\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "\n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta para sua pergunta:\",\n",
        "        \"Vou te ajudar com isso:\",\n",
        "        \"Essa √© uma excelente pergunta!\",\n",
        "        \"Deixe-me explicar isso:\",\n",
        "        \"Aqui est√£o as informa√ß√µes que voc√™ precisa:\",\n",
        "        \"Vou criar um exemplo para voc√™:\",\n",
        "        \"Essa solu√ß√£o vai funcionar perfeitamente:\",\n",
        "        \"Aqui est√° o que voc√™ precisa saber:\",\n",
        "        \"Vou te mostrar como fazer isso:\",\n",
        "        \"Essa √© a melhor abordagem:\"\n",
        "    ]\n",
        "\n",
        "    llm = FakeListLLM(responses=respostas)\n",
        "\n",
        "    print(\"‚úÖ Mock LLM configurado!\")\n",
        "    print(\"üé≠ Usando respostas simuladas (100% gratuito)\")\n",
        "    print(\"üí° Para respostas reais, configure OpenAI ou Hugging Face acima\")\n",
        "\n",
        "    return llm, \"mock\"\n",
        "\n",
        "# Configurando o LLM\n",
        "llm, tipo = configurar_llm_colab()\n",
        "\n",
        "print(f\"\\nüéØ LLM configurado: {tipo.upper()}\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"\\nüöÄ Pronto para usar em todos os m√≥dulos do curso!\")\n",
        "print(\"\\nüí° Dica: Copie a vari√°vel 'llm' para usar nos outros notebooks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRYg67UQ-Ft9"
      },
      "source": [
        "## **üìã C√©lula Universal para Outros Notebooks**\n",
        "\n",
        "Agora que voc√™ tem o LLM configurado, use esta c√©lula em **todos os outros notebooks** do curso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT6wA6Nh-Ft9"
      },
      "outputs": [],
      "source": [
        "# C√âLULA PARA USAR EM TODOS OS M√ìDULOS\n",
        "# Copie e cole esta c√©lula no in√≠cio de cada notebook\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carregando vari√°veis (se existirem)\n",
        "load_dotenv()\n",
        "\n",
        "# Fun√ß√£o para configurar LLM no Colab\n",
        "def get_llm_colab():\n",
        "    \"\"\"Retorna o melhor LLM dispon√≠vel no Colab\"\"\"\n",
        "\n",
        "    # Tentativa 1: OpenAI\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if api_key:\n",
        "            return ChatOpenAI(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.7,\n",
        "                api_key=api_key\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Tentativa 2: Hugging Face\n",
        "    try:\n",
        "        from langchain_community.llms import HuggingFaceHub\n",
        "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "        if token:\n",
        "            return HuggingFaceHub(\n",
        "                repo_id=\"google/flan-t5-base\",\n",
        "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Op√ß√£o 3: Mock LLM\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta:\", \"Vou te ajudar:\", \"Essa √© uma boa pergunta!\",\n",
        "        \"Deixe-me explicar:\", \"Aqui est√£o as informa√ß√µes:\", \"Vou criar um exemplo:\"\n",
        "    ]\n",
        "    return FakeListLLM(responses=respostas)\n",
        "\n",
        "# Configurando o LLM\n",
        "llm = get_llm_colab()\n",
        "\n",
        "print(\"üöÄ LLM configurado com sucesso no Colab!\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"üí° Agora voc√™ pode usar 'llm' em todos os exemplos do curso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "032Nxb8X-Ft9"
      },
      "source": [
        "## **üí∞ Compara√ß√£o de Custos no Colab**\n",
        "\n",
        "| Op√ß√£o | Custo | Limites | Qualidade | Facilidade |\n",
        "|-------|-------|---------|-----------|------------|\n",
        "| **Mock LLM** | $0 | Nenhum | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Hugging Face** | $0 | 30K/m√™s | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **OpenAI** | ~$5-20 | Sem limite | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Recomenda√ß√£o para o Curso**\n",
        "\n",
        "### **Para Iniciantes:**\n",
        "1. **Comece com Mock LLM** - Aprenda os conceitos\n",
        "2. **Configure Hugging Face** - Para respostas reais\n",
        "3. **Use OpenAI** - Se quiser melhor qualidade\n",
        "\n",
        "### **Para Desenvolvedores:**\n",
        "1. **Hugging Face** - Para desenvolvimento\n",
        "2. **OpenAI** - Para produ√ß√£o\n",
        "3. **Mock LLM** - Para testes r√°pidos\n",
        "\n",
        "---\n",
        "\n",
        "## **üöÄ Pr√≥ximos Passos**\n",
        "\n",
        "1. **Teste o LLM configurado** executando a c√©lula acima\n",
        "2. **Copie a vari√°vel 'llm'** para usar nos outros notebooks\n",
        "3. **Comece o curso** com o m√≥dulo 1\n",
        "4. **Divirta-se aprendendo LangChain!**\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Parab√©ns! Voc√™ est√° pronto para aprender LangChain no Google Colab!**\n",
        "\n",
        "**üí° Dica do Pedro**: O Mock LLM √© perfeito para aprender os conceitos. Quando estiver confort√°vel, configure Hugging Face ou OpenAI para respostas reais!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "00_setup_colab",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}