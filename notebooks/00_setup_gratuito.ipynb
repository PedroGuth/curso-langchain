{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üÜì **Setup Gratuito - Aprenda LangChain Sem Gastar Nada!**\n",
        "\n",
        "## **Por que este setup?**\n",
        "\n",
        "A OpenAI cobra por uso da API, mas voc√™ pode aprender LangChain **100% gratuito**! Vou te mostrar as melhores alternativas.\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Op√ß√µes Gratuitas Dispon√≠veis:**\n",
        "\n",
        "### **1. Ollama (Recomendado) - Modelos Locais**\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Funciona offline**\n",
        "- ‚úÖ **Modelos poderosos** (Llama2, Mistral, CodeLlama)\n",
        "- ‚ùå Precisa instalar localmente\n",
        "\n",
        "### **2. Hugging Face - Modelos na Nuvem**\n",
        "- ‚úÖ **Gratuito** (com limites)\n",
        "- ‚úÖ **Muitos modelos dispon√≠veis**\n",
        "- ‚úÖ **F√°cil de usar**\n",
        "- ‚ùå Limite de requisi√ß√µes\n",
        "\n",
        "### **3. Mock LLM - Para Demonstra√ß√£o**\n",
        "- ‚úÖ **100% gratuito**\n",
        "- ‚úÖ **Funciona sempre**\n",
        "- ‚úÖ **Perfeito para aprender**\n",
        "- ‚ùå Respostas simuladas\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Dica do Pedro**: Comece com Mock LLM para aprender os conceitos, depois migre para Ollama quando quiser respostas reais!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üöÄ Op√ß√£o 1: Ollama (Modelos Locais)**\n",
        "\n",
        "### **Como instalar Ollama:**\n",
        "\n",
        "1. **Windows/Mac**: Baixe em [ollama.ai](https://ollama.ai)\n",
        "2. **Linux**: `curl -fsSL https://ollama.ai/install.sh | sh`\n",
        "3. **Baixe um modelo**: `ollama pull llama2`\n",
        "\n",
        "### **Vantagens:**\n",
        "- Funciona offline\n",
        "- Sem limites de uso\n",
        "- Modelos poderosos\n",
        "- Privacidade total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalando depend√™ncias para Ollama\n",
        "!!pip install langchain-community\n",
        "\n",
        "# Importando Ollama\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Criando modelo local (precisa ter Ollama instalado)\n",
        "try:\n",
        "    llm_ollama = Ollama(model=\"llama2\")\n",
        "    print(\"‚úÖ Ollama configurado com sucesso!\")\n",
        "    print(f\"ü§ñ Modelo: {llm_ollama.model}\")\n",
        "    \n",
        "    # Teste r√°pido\n",
        "    response = llm_ollama.invoke(\"Diga ol√° em portugu√™s\")\n",
        "    print(f\"üí¨ Resposta: {response}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ollama n√£o encontrado: {e}\")\n",
        "    print(\"üì• Instale Ollama em: https://ollama.ai\")\n",
        "    print(\"üê≥ Ou use a op√ß√£o Mock LLM abaixo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üåê Op√ß√£o 2: Hugging Face (Modelos Gratuitos)**\n",
        "\n",
        "### **Como configurar:**\n",
        "\n",
        "1. Crie conta em [huggingface.co](https://huggingface.co)\n",
        "2. V√° em Settings > Access Tokens\n",
        "3. Crie um token gratuito\n",
        "4. Use modelos gratuitos como `google/flan-t5-base`\n",
        "\n",
        "### **Limites gratuitos:**\n",
        "- 30.000 requisi√ß√µes/m√™s\n",
        "- Modelos menores (mas funcionais)\n",
        "- Sempre online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalando depend√™ncias para Hugging Face\n",
        "!!pip install langchain-community huggingface_hub\n",
        "\n",
        "# Importando Hugging Face\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "import os\n",
        "\n",
        "# Configurando token (substitua pelo seu)\n",
        "HUGGINGFACE_TOKEN = \"seu_token_aqui\"  # Crie em huggingface.co\n",
        "\n",
        "if HUGGINGFACE_TOKEN != \"seu_token_aqui\":\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_TOKEN\n",
        "    \n",
        "    try:\n",
        "        # Modelo gratuito do Google\n",
        "        llm_hf = HuggingFaceHub(\n",
        "            repo_id=\"google/flan-t5-base\",\n",
        "            model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Hugging Face configurado com sucesso!\")\n",
        "        print(f\"ü§ñ Modelo: {llm_hf.repo_id}\")\n",
        "        \n",
        "        # Teste r√°pido\n",
        "        response = llm_hf.invoke(\"Translate to Portuguese: Hello world\")\n",
        "        print(f\"üí¨ Resposta: {response}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro no Hugging Face: {e}\")\n",
        "        print(\"üîë Configure seu token em huggingface.co\")\n",
        "else:\n",
        "    print(\"üîë Configure seu token do Hugging Face primeiro!\")\n",
        "    print(\"üìù V√° em huggingface.co > Settings > Access Tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üé≠ Op√ß√£o 3: Mock LLM (Para Aprender)**\n",
        "\n",
        "### **Perfeito para:**\n",
        "- Aprender conceitos do LangChain\n",
        "- Testar estruturas de c√≥digo\n",
        "- Demonstra√ß√µes em sala de aula\n",
        "- Quando n√£o tem internet\n",
        "\n",
        "### **Como funciona:**\n",
        "- Respostas pr√©-definidas\n",
        "- Simula comportamento real\n",
        "- 100% gratuito e offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock LLM - Respostas simuladas para aprender\n",
        "from langchain.llms.fake import FakeListLLM\n",
        "\n",
        "# Criando respostas simuladas realistas\n",
        "respostas_simuladas = [\n",
        "    \"Ol√°! Como posso te ajudar hoje?\",\n",
        "    \"Essa √© uma pergunta interessante. Deixe-me pensar...\",\n",
        "    \"Baseado no que voc√™ perguntou, posso sugerir...\",\n",
        "    \"Aqui est√° uma an√°lise detalhada do seu problema:\",\n",
        "    \"Vou te ajudar a resolver isso passo a passo:\",\n",
        "    \"Essa √© uma excelente pergunta! A resposta √©:\",\n",
        "    \"Deixe-me explicar isso de forma simples:\",\n",
        "    \"Aqui est√£o as principais informa√ß√µes:\",\n",
        "    \"Vou criar um exemplo pr√°tico para voc√™:\",\n",
        "    \"Essa solu√ß√£o vai funcionar perfeitamente para seu caso:\"\n",
        "]\n",
        "\n",
        "# Criando o Mock LLM\n",
        "llm_mock = FakeListLLM(responses=respostas_simuladas)\n",
        "\n",
        "print(\"‚úÖ Mock LLM configurado com sucesso!\")\n",
        "print(\"üé≠ Usando respostas simuladas para demonstra√ß√£o\")\n",
        "print(f\"üìù Total de respostas: {len(respostas_simuladas)}\")\n",
        "\n",
        "# Teste r√°pido\n",
        "for i in range(3):\n",
        "    response = llm_mock.invoke(f\"Pergunta teste {i+1}\")\n",
        "    print(f\"üí¨ Resposta {i+1}: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üîß Configura√ß√£o Universal para o Curso**\n",
        "\n",
        "Agora vou criar uma fun√ß√£o que detecta automaticamente qual op√ß√£o est√° dispon√≠vel e configura o melhor LLM para voc√™:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para detectar e configurar o melhor LLM dispon√≠vel\n",
        "def configurar_llm_gratuito():\n",
        "    \"\"\"\n",
        "    Detecta automaticamente qual op√ß√£o gratuita est√° dispon√≠vel\n",
        "    e configura o melhor LLM para o curso\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"üîç Detectando op√ß√µes gratuitas dispon√≠veis...\")\n",
        "    \n",
        "    # Tentativa 1: Ollama\n",
        "    try:\n",
        "        from langchain_community.llms import Ollama\n",
        "        llm = Ollama(model=\"llama2\")\n",
        "        \n",
        "        # Teste r√°pido\n",
        "        test_response = llm.invoke(\"Teste\")\n",
        "        \n",
        "        print(\"‚úÖ Ollama detectado e configurado!\")\n",
        "        print(\"üöÄ Usando modelo local (100% gratuito)\")\n",
        "        return llm, \"ollama\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ollama n√£o dispon√≠vel: {e}\")\n",
        "    \n",
        "    # Tentativa 2: Hugging Face\n",
        "    try:\n",
        "        from langchain_community.llms import HuggingFaceHub\n",
        "        import os\n",
        "        \n",
        "        # Verifica se tem token configurado\n",
        "        if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "            llm = HuggingFaceHub(\n",
        "                repo_id=\"google/flan-t5-base\",\n",
        "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "            )\n",
        "            \n",
        "            # Teste r√°pido\n",
        "            test_response = llm.invoke(\"Test\")\n",
        "            \n",
        "            print(\"‚úÖ Hugging Face detectado e configurado!\")\n",
        "            print(\"üåê Usando modelo na nuvem (gratuito com limites)\")\n",
        "            return llm, \"huggingface\"\n",
        "        else:\n",
        "            print(\"‚ùå Token do Hugging Face n√£o configurado\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Hugging Face n√£o dispon√≠vel: {e}\")\n",
        "    \n",
        "    # Op√ß√£o 3: Mock LLM (sempre funciona)\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "    \n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta para sua pergunta:\",\n",
        "        \"Vou te ajudar com isso:\",\n",
        "        \"Essa √© uma excelente pergunta!\",\n",
        "        \"Deixe-me explicar isso:\",\n",
        "        \"Aqui est√£o as informa√ß√µes que voc√™ precisa:\",\n",
        "        \"Vou criar um exemplo para voc√™:\",\n",
        "        \"Essa solu√ß√£o vai funcionar perfeitamente:\",\n",
        "        \"Aqui est√° o que voc√™ precisa saber:\",\n",
        "        \"Vou te mostrar como fazer isso:\",\n",
        "        \"Essa √© a melhor abordagem:\"\n",
        "    ]\n",
        "    \n",
        "    llm = FakeListLLM(responses=respostas)\n",
        "    \n",
        "    print(\"‚úÖ Mock LLM configurado!\")\n",
        "    print(\"üé≠ Usando respostas simuladas (100% gratuito)\")\n",
        "    print(\"üí° Para respostas reais, instale Ollama ou configure Hugging Face\")\n",
        "    \n",
        "    return llm, \"mock\"\n",
        "\n",
        "# Configurando o LLM\n",
        "llm, tipo = configurar_llm_gratuito()\n",
        "\n",
        "print(f\"\\nüéØ LLM configurado: {tipo.upper()}\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"\\nüöÄ Pronto para usar em todos os m√≥dulos do curso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üìã Como Usar nos Outros M√≥dulos**\n",
        "\n",
        "Agora que voc√™ tem o LLM configurado, use esta c√©lula em **todos os outros notebooks** do curso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA PARA USAR EM TODOS OS M√ìDULOS\n",
        "# Copie e cole esta c√©lula no in√≠cio de cada notebook\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carregando vari√°veis (se existirem)\n",
        "load_dotenv()\n",
        "\n",
        "# Fun√ß√£o para configurar LLM gratuito\n",
        "def get_llm():\n",
        "    \"\"\"Retorna o melhor LLM dispon√≠vel gratuitamente\"\"\"\n",
        "    \n",
        "    # Tentativa 1: Ollama\n",
        "    try:\n",
        "        from langchain_community.llms import Ollama\n",
        "        return Ollama(model=\"llama2\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Tentativa 2: Hugging Face\n",
        "    try:\n",
        "        from langchain_community.llms import HuggingFaceHub\n",
        "        if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "            return HuggingFaceHub(\n",
        "                repo_id=\"google/flan-t5-base\",\n",
        "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Op√ß√£o 3: Mock LLM\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta:\", \"Vou te ajudar:\", \"Essa √© uma boa pergunta!\",\n",
        "        \"Deixe-me explicar:\", \"Aqui est√£o as informa√ß√µes:\", \"Vou criar um exemplo:\"\n",
        "    ]\n",
        "    return FakeListLLM(responses=respostas)\n",
        "\n",
        "# Configurando o LLM\n",
        "llm = get_llm()\n",
        "\n",
        "print(\"üöÄ LLM gratuito configurado com sucesso!\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"üí° Agora voc√™ pode usar 'llm' em todos os exemplos do curso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üí∞ Compara√ß√£o de Custos**\n",
        "\n",
        "| Op√ß√£o | Custo | Limites | Qualidade |\n",
        "|-------|-------|---------|-----------|\n",
        "| **OpenAI** | $0.002/1K tokens | Sem limite | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Ollama** | $0 (local) | Sem limite | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Hugging Face** | $0 | 30K req/m√™s | ‚≠ê‚≠ê‚≠ê |\n",
        "| **Mock LLM** | $0 | Sem limite | ‚≠ê‚≠ê |\n",
        "\n",
        "---\n",
        "\n",
        "## **üéØ Recomenda√ß√£o Final**\n",
        "\n",
        "### **Para Aprender (Recomendado):**\n",
        "1. **Comece com Mock LLM** - Aprenda os conceitos\n",
        "2. **Instale Ollama** - Para respostas reais\n",
        "3. **Configure Hugging Face** - Como backup\n",
        "\n",
        "### **Para Produ√ß√£o:**\n",
        "- Use **Ollama** para projetos pessoais\n",
        "- Use **OpenAI** para projetos comerciais\n",
        "- Use **Hugging Face** para experimentos\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ Agora voc√™ pode aprender LangChain 100% gratuito!**\n",
        "\n",
        "**üí° Dica do Pedro**: O importante √© aprender os conceitos. As respostas simuladas s√£o suficientes para entender como LangChain funciona!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "00_setup_gratuito",
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}