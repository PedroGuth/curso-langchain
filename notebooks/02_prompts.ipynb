{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ **M√≥dulo 2: Prompts - A Arte de Falar com IA**\n",
        "\n",
        "## **Aula 2.1: Prompts B√°sicos - Como Pedir as Coisas Direito**\n",
        "\n",
        "---\n",
        "\n",
        "### **T√°, mas o que √© um Prompt?**\n",
        "\n",
        "Imagina que voc√™ est√° em um restaurante. Se voc√™ disser \"quero comida\", o gar√ßom vai ficar perdido. Mas se voc√™ disser \"quero um X-Burger com batata frita e Coca-Cola\", a√≠ sim ele sabe exatamente o que trazer.\n",
        "\n",
        "**Prompt √© exatamente isso!** √â a forma como voc√™ \"pede\" as coisas para a IA. üçî\n",
        "\n",
        "**Prompt ruim**: \"Escreve algo sobre programa√ß√£o\"\n",
        "**Prompt bom**: \"Escreve um artigo de 300 palavras sobre Python para iniciantes, com exemplos pr√°ticos e linguagem simples\"\n",
        "\n",
        "A diferen√ßa √© **ABISMAL**! √â como a diferen√ßa entre pedir \"um carro\" e pedir \"um Honda Civic 2023, cor prata, autom√°tico, com ar condicionado\".\n",
        "\n",
        "### **Por que Prompts Importam Tanto?**\n",
        "\n",
        "**Sem bons prompts**: A IA fica perdida, responde coisas gen√©ricas, n√£o entende o que voc√™ quer\n",
        "**Com bons prompts**: A IA vira seu assistente pessoal, entende exatamente o que voc√™ precisa\n",
        "\n",
        "√â como a diferen√ßa entre ter um **estagi√°rio perdido** e um **funcion√°rio experiente**! üòÖ\n",
        "\n",
        "---\n",
        "\n",
        "<img src='https://www.mygreatlearning.com/blog/wp-content/uploads/2025/02/good-prommpts-vs-bad-prompts.png' width='1200'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Setup Inicial - Preparando o Terreno**\n",
        "\n",
        "Vamos come√ßar importando o que precisamos e configurando nosso modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importando as bibliotecas necess√°rias",
        "# üí° Google Colab - Funciona no navegador!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Carregando vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# Criando nosso modelo (como \"ligar\" o motor da IA)\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    api_key=os.getenv('OPENAI_API_KEY')  # Configure sua API key no Colab\n",
        ")\n",
        "\n",
        "print(\"üöÄ Modelo configurado e pronto para os testes de prompts!\")\n",
        "print(f\"ü§ñ Modelo: {llm.model_name}\")\n",
        "print(f\"üå°Ô∏è  Temperature: {llm.temperature}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Teste 1: Prompt Ruim vs Prompt Bom**\n",
        "\n",
        "Vamos ver na pr√°tica a diferen√ßa que um bom prompt faz. √â como comparar um **pedido vago** com um **pedido espec√≠fico** no restaurante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste 1: Prompt RUIM (vago e gen√©rico)\n",
        "print(\"‚ùå PROMPT RUIM - Vago e gen√©rico:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "prompt_ruim = \"Fala sobre programa√ß√£o\"\n",
        "print(f\"ü§î Pergunta: {prompt_ruim}\")\n",
        "\n",
        "try:\n",
        "    response_ruim = llm.invoke([HumanMessage(content=prompt_ruim)])\n",
        "    print(f\"ü§ñ Resposta: {response_ruim.content[:200]}...\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nüí≠ O que aconteceu: A IA deu uma resposta gen√©rica, sem foco espec√≠fico\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste 2: Prompt BOM (espec√≠fico e detalhado)\n",
        "print(\"‚úÖ PROMPT BOM - Espec√≠fico e detalhado:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "prompt_bom = \"\"\"\n",
        "Escreve um artigo de 200 palavras sobre Python para iniciantes, incluindo:\n",
        "- O que √© Python e por que √© popular\n",
        "- 3 exemplos pr√°ticos de c√≥digo simples\n",
        "- Linguagem informal e descontra√≠da\n",
        "- Foco em aplica√ß√µes do mundo real\n",
        "\n",
        "Use analogias do dia a dia e seja did√°tico como o Pedro Guth.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"ü§î Pergunta: {prompt_bom.strip()}\")\n",
        "\n",
        "try:\n",
        "    response_bom = llm.invoke([HumanMessage(content=prompt_bom)])\n",
        "    print(f\"ü§ñ Resposta:\\n{response_bom.content}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nüéØ O que aconteceu: A IA deu uma resposta focada, estruturada e √∫til!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Parou aqui e entendeu!** üéØ\n",
        "\n",
        "**A diferen√ßa √© BRUTAL, n√©?**\n",
        "\n",
        "**Prompt ruim**: Resposta gen√©rica, sem foco, in√∫til\n",
        "**Prompt bom**: Resposta espec√≠fica, estruturada, √∫til\n",
        "\n",
        "√â como a diferen√ßa entre pedir \"um carro\" e pedir \"um carro para fam√≠lia de 4 pessoas, com porta-malas grande, econ√¥mico e seguro\".\n",
        "\n",
        "---\n",
        "\n",
        "### **Regras de Ouro para Prompts Bons** üìã\n",
        "\n",
        "1. **Seja espec√≠fico** - N√£o deixe a IA adivinhar\n",
        "2. **Defina o formato** - Como voc√™ quer a resposta\n",
        "3. **D√™ contexto** - Explique o que voc√™ precisa\n",
        "4. **Use exemplos** - Mostre o que voc√™ quer\n",
        "5. **Defina o tom** - Formal, informal, t√©cnico, etc.\n",
        "\n",
        "**üí° Dica do Pedro**: Pense que voc√™ est√° explicando para um **estagi√°rio muito inteligente, mas que n√£o l√™ sua mente**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Aula 2.2: Templates de Prompts - Como Ter Receitas Prontas**\n",
        "\n",
        "### **O que s√£o Templates?**\n",
        "\n",
        "Templates s√£o como **receitas de bolo** que voc√™ pode reutilizar. Em vez de escrever o prompt do zero toda vez, voc√™ cria um modelo e s√≥ muda os ingredientes.\n",
        "\n",
        "**Exemplo pr√°tico**:\n",
        "- **Sem template**: Escrever o prompt completo toda vez\n",
        "- **Com template**: \"Traduza {texto} do {idioma_origem} para {idioma_destino}\"\n",
        "\n",
        "√â como ter um **moldinho de bolo** - voc√™ s√≥ muda os ingredientes, mas a forma √© sempre a mesma! üç∞\n",
        "\n",
        "### **Criando Nosso Primeiro Template**\n",
        "\n",
        "Vamos criar um template para tradu√ß√£o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um template de tradu√ß√£o\n",
        "# √â como criar um molde que pode ser reutilizado\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Template para tradu√ß√£o\n",
        "template_traducao = PromptTemplate(\n",
        "    input_variables=[\"texto\", \"idioma_origem\", \"idioma_destino\"],\n",
        "    template=\"\"\"\n",
        "    Traduza o seguinte texto do {idioma_origem} para {idioma_destino}:\n",
        "    \n",
        "    TEXTO: {texto}\n",
        "    \n",
        "    INSTRU√á√ïES:\n",
        "    - Mantenha o tom e estilo original\n",
        "    - Preserve a formata√ß√£o\n",
        "    - Seja preciso e natural\n",
        "    - Responda apenas com a tradu√ß√£o\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"üìù Template de tradu√ß√£o criado!\")\n",
        "print(f\"üîß Vari√°veis: {template_traducao.input_variables}\")\n",
        "print(f\"üìÑ Template:\\n{template_traducao.template}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando nosso template de tradu√ß√£o\n",
        "# √â como usar o molde com ingredientes diferentes\n",
        "\n",
        "print(\"üåç Testando Template de Tradu√ß√£o:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Formatando o template com dados espec√≠ficos\n",
        "prompt_formatado = template_traducao.format(\n",
        "    texto=\"Hello, how are you? I love programming with Python!\",\n",
        "    idioma_origem=\"ingl√™s\",\n",
        "    idioma_destino=\"portugu√™s\"\n",
        ")\n",
        "\n",
        "print(f\"üìù Prompt formatado:\\n{prompt_formatado}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # Enviando para a IA\n",
        "    response = llm.invoke([HumanMessage(content=prompt_formatado)])\n",
        "    print(f\"ü§ñ Tradu√ß√£o:\\n{response.content}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o mesmo template com outro idioma\n",
        "# Reutilizando o molde com ingredientes diferentes\n",
        "\n",
        "print(\"üá™üá∏ Testando com Espanhol:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "prompt_espanhol = template_traducao.format(\n",
        "    texto=\"Python √© uma linguagem de programa√ß√£o incr√≠vel!\",\n",
        "    idioma_origem=\"portugu√™s\",\n",
        "    idioma_destino=\"espanhol\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = llm.invoke([HumanMessage(content=prompt_espanhol)])\n",
        "    print(f\"ü§ñ Tradu√ß√£o para espanhol:\\n{response.content}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Chutando com Eleg√¢ncia!** ‚öΩ\n",
        "\n",
        "**O que acabamos de fazer:**\n",
        "\n",
        "1. ‚úÖ **Criamos um template** - Como uma receita de bolo\n",
        "2. ‚úÖ **Definimos vari√°veis** - Os \"ingredientes\" que mudam\n",
        "3. ‚úÖ **Reutilizamos o template** - Com dados diferentes\n",
        "4. ‚úÖ **Mantivemos consist√™ncia** - Mesmo formato, resultados diferentes\n",
        "\n",
        "**Vantagens dos Templates:**\n",
        "- **Reutiliza√ß√£o**: N√£o precisa reescrever o prompt\n",
        "- **Consist√™ncia**: Mesmo formato sempre\n",
        "- **Manuten√ß√£o**: Muda em um lugar, muda em todos\n",
        "- **Organiza√ß√£o**: C√≥digo mais limpo e profissional\n",
        "\n",
        "√â como ter um **menu de restaurante** - voc√™ n√£o precisa explicar como fazer cada prato, s√≥ escolhe o que quer! üçΩÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Criando Templates Mais Avan√ßados**\n",
        "\n",
        "Agora vamos criar um template para an√°lise de sentimentos. √â como criar um **detector de emo√ß√µes** para textos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template para an√°lise de sentimentos\n",
        "# Como criar um \"detector de emo√ß√µes\" para textos\n",
        "\n",
        "template_sentimento = PromptTemplate(\n",
        "    input_variables=[\"texto\"],\n",
        "    template=\"\"\"\n",
        "    Analise o sentimento do seguinte texto:\n",
        "    \n",
        "    TEXTO: {texto}\n",
        "    \n",
        "    Responda no seguinte formato JSON:\n",
        "    {{\n",
        "        \"sentimento\": \"positivo/negativo/neutro\",\n",
        "        \"intensidade\": \"baixa/m√©dia/alta\",\n",
        "        \"explicacao\": \"breve explica√ß√£o do porqu√™\",\n",
        "        \"palavras_chave\": [\"palavra1\", \"palavra2\", \"palavra3\"]\n",
        "    }}\n",
        "    \n",
        "    Seja preciso e objetivo na an√°lise.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"üòä Template de an√°lise de sentimentos criado!\")\n",
        "print(f\"üîß Vari√°veis: {template_sentimento.input_variables}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando an√°lise de sentimentos\n",
        "# Vamos analisar diferentes tipos de texto\n",
        "\n",
        "import json\n",
        "\n",
        "textos_teste = [\n",
        "    \"Adorei o novo filme! Foi incr√≠vel e emocionante!\",\n",
        "    \"Odeio quando o tr√¢nsito est√° congestionado. Que inferno!\",\n",
        "    \"O caf√© est√° na temperatura ideal para beber.\"\n",
        "]\n",
        "\n",
        "for i, texto in enumerate(textos_teste, 1):\n",
        "    print(f\"\\nüìù Teste {i} - An√°lise de Sentimento:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìÑ Texto: {texto}\")\n",
        "    \n",
        "    try:\n",
        "        # Formatando o template\n",
        "        prompt = template_sentimento.format(texto=texto)\n",
        "        \n",
        "        # Enviando para an√°lise\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        # Tentando fazer parse do JSON\n",
        "        try:\n",
        "            resultado = json.loads(response.content)\n",
        "            print(f\"üòä Sentimento: {resultado['sentimento']}\")\n",
        "            print(f\"üìä Intensidade: {resultado['intensidade']}\")\n",
        "            print(f\"üí≠ Explica√ß√£o: {resultado['explicacao']}\")\n",
        "            print(f\"üîë Palavras-chave: {', '.join(resultado['palavras_chave'])}\")\n",
        "        except:\n",
        "            print(f\"ü§ñ Resposta: {response.content}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {e}\")\n",
        "    \n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Aula 2.3: Few-Shot Examples - Ensinando com Exemplos**\n",
        "\n",
        "### **O que s√£o Few-Shot Examples?**\n",
        "\n",
        "Few-shot examples s√£o como **mostrar exemplos** para a IA antes de pedir algo. √â como ensinar algu√©m a cozinhar mostrando como fazer o prato primeiro.\n",
        "\n",
        "**Analogia**: Se voc√™ quer que algu√©m aprenda a fazer bolo, voc√™:\n",
        "1. **Mostra** como fazer um bolo\n",
        "2. **Explica** cada passo\n",
        "3. **Deixa** a pessoa fazer sozinha\n",
        "\n",
        "Com IA √© a mesma coisa! üéÇ\n",
        "\n",
        "### **Criando um Few-Shot Prompt**\n",
        "\n",
        "Vamos criar um prompt que ensina a IA a categorizar produtos de e-commerce:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um prompt com few-shot examples\n",
        "# Como ensinar a IA com exemplos pr√°ticos\n",
        "\n",
        "few_shot_template = PromptTemplate(\n",
        "    input_variables=[\"produto\"],\n",
        "    template=\"\"\"\n",
        "    Categorize o seguinte produto de e-commerce:\n",
        "    \n",
        "    EXEMPLOS:\n",
        "    \n",
        "    Produto: iPhone 14 Pro Max 256GB\n",
        "    Categoria: Tecnologia > Smartphones > Apple\n",
        "    Pre√ßo estimado: R$ 8.000 - R$ 12.000\n",
        "    \n",
        "    Produto: T√™nis Nike Air Max 270\n",
        "    Categoria: Esportes > Cal√ßados > T√™nis\n",
        "    Pre√ßo estimado: R$ 400 - R$ 800\n",
        "    \n",
        "    Produto: Livro \"O Senhor dos An√©is\"\n",
        "    Categoria: Livros > Literatura > Fantasia\n",
        "    Pre√ßo estimado: R$ 30 - R$ 80\n",
        "    \n",
        "    Produto: Panela Tramontina 5L\n",
        "    Categoria: Casa > Cozinha > Utens√≠lios\n",
        "    Pre√ßo estimado: R$ 150 - R$ 300\n",
        "    \n",
        "    Agora categorize este produto:\n",
        "    Produto: {produto}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"üìö Template few-shot criado!\")\n",
        "print(\"üéØ Este template ensina a IA a categorizar produtos com exemplos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o few-shot prompt\n",
        "# Vamos ver se a IA aprendeu com os exemplos\n",
        "\n",
        "produtos_teste = [\n",
        "    \"Samsung Galaxy S23 Ultra\",\n",
        "    \"Bicicleta Caloi Aro 26\",\n",
        "    \"Blender Philips Walita\",\n",
        "    \"Camiseta Adidas Original\"\n",
        "]\n",
        "\n",
        "print(\"üõçÔ∏è Testando Categoriza√ß√£o de Produtos:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, produto in enumerate(produtos_teste, 1):\n",
        "    print(f\"\\nüì¶ Produto {i}: {produto}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Formatando o prompt\n",
        "        prompt = few_shot_template.format(produto=produto)\n",
        "        \n",
        "        # Enviando para categoriza√ß√£o\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        print(f\"üè∑Ô∏è  Categoriza√ß√£o:\\n{response.content}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {e}\")\n",
        "    \n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Chain of Thought - Pensando em Voz Alta**\n",
        "\n",
        "Chain of Thought √© como fazer a IA **pensar em voz alta**. Em vez de dar a resposta direta, ela explica o processo de pensamento.\n",
        "\n",
        "**Por que isso √© √∫til?**\n",
        "- **Transpar√™ncia**: Voc√™ v√™ como a IA chegou na resposta\n",
        "- **Debugging**: Fica mais f√°cil identificar erros\n",
        "- **Aprendizado**: Voc√™ entende o processo\n",
        "- **Confian√ßa**: Voc√™ confia mais na resposta\n",
        "\n",
        "√â como ter um **professor que explica o passo a passo** em vez de s√≥ dar a resposta! üß†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um prompt com Chain of Thought\n",
        "# Fazendo a IA pensar em voz alta\n",
        "\n",
        "cot_template = PromptTemplate(\n",
        "    input_variables=[\"problema\"],\n",
        "    template=\"\"\"\n",
        "    Resolva o seguinte problema passo a passo, explicando seu racioc√≠nio:\n",
        "    \n",
        "    PROBLEMA: {problema}\n",
        "    \n",
        "    INSTRU√á√ïES:\n",
        "    1. Primeiro, identifique o que est√° sendo pedido\n",
        "    2. Depois, pense nas informa√ß√µes necess√°rias\n",
        "    3. Em seguida, aplique a l√≥gica passo a passo\n",
        "    4. Por fim, d√™ a resposta final\n",
        "    \n",
        "    Use frases como \"Primeiro, vou...\", \"Em seguida...\", \"Portanto...\"\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(\"üß† Template Chain of Thought criado!\")\n",
        "print(\"üí≠ Este template faz a IA explicar seu processo de pensamento\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando Chain of Thought\n",
        "# Vamos ver a IA pensando em voz alta\n",
        "\n",
        "problema_teste = \"\"\"\n",
        "Uma empresa tem 150 funcion√°rios. 60% s√£o homens e 40% s√£o mulheres. \n",
        "Dos homens, 30% trabalham em TI. Das mulheres, 20% trabalham em TI.\n",
        "Quantas pessoas trabalham em TI no total?\n",
        "\"\"\"\n",
        "\n",
        "print(\"üßÆ Testando Chain of Thought:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìù Problema:\\n{problema_teste}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # Formatando o prompt\n",
        "    prompt = cot_template.format(problema=problema_teste)\n",
        "    \n",
        "    # Enviando para resolu√ß√£o\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    print(f\"ü§ñ Resolu√ß√£o passo a passo:\\n{response.content}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Na Pr√°tica, Meu Consagrado!** üí™\n",
        "\n",
        "**O que aprendemos sobre Prompts:**\n",
        "\n",
        "1. ‚úÖ **Prompts espec√≠ficos** s√£o MUITO melhores que vagos\n",
        "2. ‚úÖ **Templates** permitem reutiliza√ß√£o e consist√™ncia\n",
        "3. ‚úÖ **Few-shot examples** ensinam a IA com exemplos\n",
        "4. ‚úÖ **Chain of Thought** torna o processo transparente\n",
        "5. ‚úÖ **Formata√ß√£o estruturada** (JSON, listas) melhora resultados\n",
        "\n",
        "### **Compara√ß√£o: Com vs Sem LangChain**\n",
        "\n",
        "**Sem LangChain (c√≥digo manual):**\n",
        "```python\n",
        "# Voc√™ teria que fazer isso manualmente:\n",
        "prompt = f\"\"\"\n",
        "Traduza: {texto}\n",
        "Do: {idioma_origem}\n",
        "Para: {idioma_destino}\n",
        "\"\"\"\n",
        "\n",
        "# E repetir isso para cada tipo de prompt...\n",
        "```\n",
        "\n",
        "**Com LangChain:**\n",
        "```python\n",
        "# Tudo organizado e reutiliz√°vel:\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"texto\", \"idioma_origem\", \"idioma_destino\"],\n",
        "    template=\"Traduza {texto} do {idioma_origem} para {idioma_destino}\"\n",
        ")\n",
        "```\n",
        "\n",
        "**Diferen√ßa**: C√≥digo mais limpo, reutiliz√°vel e profissional!\n",
        "\n",
        "---\n",
        "\n",
        "### **Desafio para Casa** üè†\n",
        "\n",
        "Crie um template para **an√°lise de reviews de produtos** que:\n",
        "1. Identifique o sentimento\n",
        "2. Extraia pontos positivos e negativos\n",
        "3. Sugira melhorias\n",
        "4. D√™ uma nota de 1 a 5\n",
        "\n",
        "**Exemplo de uso**:\n",
        "```python\n",
        "review = \"Adorei o produto! Entrega r√°pida, mas poderia ter mais cores.\"\n",
        "resultado = analisar_review(review)\n",
        "```\n",
        "\n",
        "**üñºÔ∏è Sugest√£o de imagem**: Um diagrama mostrando a evolu√ß√£o de prompts (ruim ‚Üí bom ‚Üí template ‚Üí few-shot ‚Üí chain of thought)\n",
        "\n",
        "**üéØ Pr√≥ximo m√≥dulo**: Vamos aprender sobre **Chains** - como conectar diferentes funcionalidades!\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Resumo do M√≥dulo 2**:\n",
        "- ‚úÖ Prompts bons vs ruins\n",
        "- ‚úÖ Templates reutiliz√°veis\n",
        "- ‚úÖ Few-shot examples\n",
        "- ‚úÖ Chain of Thought\n",
        "- ‚úÖ Formata√ß√£o estruturada\n",
        "\n",
        "**üöÄ Agora voc√™ sabe falar com IA de forma profissional!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "02_prompts",
      "private_outputs": true,
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
