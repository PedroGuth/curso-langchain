{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmGmJaxebva6"
      },
      "source": [
        "# üöÄ **M√≥dulo 1: Introdu√ß√£o ao LangChain - O Quebra-Cabe√ßa da IA**\n",
        "\n",
        "## **Aula 1.1: O que √© LangChain e por que todo mundo t√° falando disso?**\n",
        "\n",
        "---\n",
        "\n",
        "### **T√°, mas o que √© LangChain mesmo?**\n",
        "\n",
        "Imagina que voc√™ √© um pedreiro e tem um monte de tijolos, cimento, areia e ferramentas espalhadas pelo terreno. Voc√™ **pode** construir uma casa, mas vai ser um trabalho do caralho e vai demorar uma eternidade.\n",
        "\n",
        "Agora imagina que algu√©m te d√° um **kit de constru√ß√£o** com tudo organizado, instru√ß√µes claras e at√© uns moldes prontos. Muito mais f√°cil, n√©?\n",
        "\n",
        "**LangChain √© exatamente isso para IA!** üß±\n",
        "\n",
        "Sem LangChain, voc√™ tem que:\n",
        "- Conectar APIs manualmente\n",
        "- Gerenciar mem√≥ria de conversas\n",
        "- Implementar prompts do zero\n",
        "- Fazer parsing de respostas\n",
        "- E mais um monte de coisa chata\n",
        "\n",
        "Com LangChain, voc√™ tem **componentes prontos** que se encaixam como pe√ßas de Lego. √â tipo ter um **\"Lego da IA\"** - voc√™ junta as pe√ßas e faz coisas incr√≠veis sem reinventar a roda.\n",
        "\n",
        "### **Por que LangChain √© tipo um \"pedreiro inteligente\"?**\n",
        "\n",
        "LangChain n√£o √© s√≥ uma biblioteca, √© um **framework completo** que te ajuda a:\n",
        "\n",
        "1. **Organizar prompts** (como ter receitas de bolo prontas)\n",
        "2. **Conectar diferentes IAs** (como ter um tradutor que fala com um analista)\n",
        "3. **Lembrar de conversas** (como um gar√ßom que nunca esquece seu pedido)\n",
        "4. **Usar ferramentas externas** (como dar superpoderes para a IA)\n",
        "5. **Processar documentos** (como ter um assistente que l√™ tudo pra voc√™)\n",
        "\n",
        "### **ChatGPT vs LangChain - A Diferen√ßa na Pr√°tica**\n",
        "\n",
        "**ChatGPT**: √â como ter um amigo super inteligente, mas que s√≥ pode conversar. Ele n√£o pode:\n",
        "- Acessar internet\n",
        "- Executar c√≥digo\n",
        "- Lembrar de conversas antigas\n",
        "- Conectar com outros sistemas\n",
        "\n",
        "**LangChain**: √â como ter um **ex√©rcito de amigos inteligentes** cada um com uma especialidade, e voc√™ √© o chefe que coordena tudo!\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Dica do Pedro**: LangChain √© especialmente √∫til quando voc√™ quer fazer algo mais complexo que uma simples conversa. √â tipo a diferen√ßa entre pedir um Uber e ter um motorista particular que tamb√©m √© seu assistente pessoal.\n",
        "\n",
        "<img src='https://cdn.hashnode.com/res/hashnode/image/upload/v1681565061148/fe236ec9-c9cb-4325-8af4-b3b193faadb3.png' width='1200'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnmirKEbva7"
      },
      "source": [
        "## **Aula 1.2: Setup do Ambiente - Preparando o Terreno**\n",
        "\n",
        "### **Instala√ß√£o e Configura√ß√£o (Sem Complica√ß√£o)**\n",
        "\n",
        "Antes de come√ßar a construir, precisamos preparar o terreno. √â como aprender a dirigir - primeiro voc√™ liga o carro, depois aprende a andar.\n",
        "\n",
        "Vamos instalar tudo que precisamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okcDb-wJbva7"
      },
      "outputs": [],
      "source": [
        "# Instalando as depend√™ncias necess√°rias\n",
        "# Execute esta c√©lula primeiro!\n",
        "\n",
        "!pip install --quiet langchain openai python-dotenv\n",
        "!pip install --quiet langchain-community langchain-core\n",
        "!pip install --quiet --upgrade langchain-huggingface transformers sentencepiece torch\n",
        "!pip install --quiet huggingface_hub\n",
        "!pip install --quiet langchain-openai openai\n",
        "\n",
        "# Para document loaders\n",
        "!pip install --quiet pypdf python-docx beautifulsoup4 requests youtube-transcript-api\n",
        "\n",
        "# Para vector stores\n",
        "!pip install --quiet chromadb faiss-cpu\n",
        "\n",
        "# Para agents e ferramentas\n",
        "!pip install --quiet wikipedia duckduckgo-search\n",
        "\n",
        "# Para deploy e interfaces\n",
        "!pip install --quiet streamlit gradio fastapi uvicorn\n",
        "\n",
        "# Para processamento de dados\n",
        "!pip install --quiet pandas numpy matplotlib seaborn\n",
        "\n",
        "print(\"‚úÖ Todas as depend√™ncias instaladas com sucesso!\")\n",
        "print(\"üöÄ Agora vamos configurar o LLM...\")\n",
        "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
        "print(\"üöÄ Agora vamos importar o que precisamos...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD0IVEuobva7"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas principais\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Carregando vari√°veis de ambiente (vamos configurar isso depois)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"üì¶ Bibliotecas importadas com sucesso!\")\n",
        "print(\"üîß Pr√≥ximo passo: configurar a API key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMxxCmMDbva7"
      },
      "source": [
        "### **Configurando a API Key - O \"Cart√£o de Cr√©dito\" da IA**\n",
        "\n",
        "Para usar LangChain com modelos da OpenAI (como GPT), voc√™ precisa de uma API key. √â como ter um cart√£o de cr√©dito para pagar pelos servi√ßos de IA.\n",
        "\n",
        "**üí° Dica importante**: Nunca coloque sua API key diretamente no c√≥digo! Sempre use vari√°veis de ambiente.\n",
        "\n",
        "Vamos criar um arquivo `.env` para guardar suas chaves de forma segura:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epoh5Vklbva7"
      },
      "outputs": [],
      "source": [
        "# Criando arquivo .env (se n√£o existir)\n",
        "if not os.path.exists('.env'):\n",
        "    with open('.env', 'w') as f:\n",
        "        f.write('# Suas chaves de API aqui\\n')\n",
        "        f.write('GOOGLE_API_KEY=\"XXXXCHAVEAQUIXXXXX\"\\n')\n",
        "        f.write('OPENAI_API_KEY=\"XXXXCHAVEAQUIXXXXX\"\\n')\n",
        "    print(\"üìù Arquivo .env criado!\")\n",
        "    print(\"ÔøΩÔøΩ Agora adicione sua API key no arquivo .env\")\n",
        "else:\n",
        "    print(\"‚úÖ Arquivo .env j√° existe!\")\n",
        "\n",
        "# Verificando se a API key est√° configurada\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "if google_api_key and google_api_key != 'XXXXCHAVEAQUIXXXXX':\n",
        "    print(\"üéâ Google API key configurada com sucesso!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Configure sua Google API key no arquivo .env\")\n",
        "\n",
        "# Carregando vari√°veis do .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm_colab():\n",
        "    \"\"\"Retorna o melhor LLM dispon√≠vel no Colab\"\"\"\n",
        "\n",
        "    # --- Tentativa 1: Google Gemini (Primeira op√ß√£o) ---\n",
        "    try:\n",
        "        import google.generativeai as genai\n",
        "\n",
        "        # Configurar API key do Google\n",
        "        google_key = os.getenv('GOOGLE_API_KEY')\n",
        "        if google_key:\n",
        "            genai.configure(api_key=google_key)\n",
        "\n",
        "            # Criar modelo Gemini\n",
        "            model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "            # Criar wrapper para LangChain\n",
        "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "            llm_google = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                temperature=0.7,\n",
        "                google_api_key=google_key\n",
        "            )\n",
        "            print(\"‚úÖ Google Gemini configurado!\")\n",
        "            return llm_google\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google Gemini falhou: {e}\")\n",
        "\n",
        "    # --- Tentativa 2: Hugging Face Local ---\n",
        "    try:\n",
        "        # Modelo local\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "        text_generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=-1,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        llm_hf = HuggingFacePipeline(pipeline=text_generator)\n",
        "        print(\"‚úÖ Hugging Face (local) configurado!\")\n",
        "        return llm_hf\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Hugging Face falhou: {e}\")\n",
        "\n",
        "    # --- Tentativa 3: OpenAI ---\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if openai_key:\n",
        "            llm_openai = ChatOpenAI(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.7,\n",
        "                api_key=openai_key\n",
        "            )\n",
        "            print(\"‚úÖ OpenAI configurado!\")\n",
        "            return llm_openai\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è OpenAI falhou: {e}\")\n",
        "\n",
        "    # --- Op√ß√£o 4: Mock LLM ---\n",
        "    from langchain.llms.fake import FakeListLLM\n",
        "    respostas = [\n",
        "        \"Aqui est√° a resposta:\", \"Vou te ajudar:\", \"Essa √© uma boa pergunta!\",\n",
        "        \"Deixe-me explicar:\", \"Aqui est√£o as informa√ß√µes:\", \"Vou criar um exemplo:\"\n",
        "    ]\n",
        "    print(\"‚ÑπÔ∏è Usando FakeListLLM como fallback\")\n",
        "    return FakeListLLM(responses=respostas)\n",
        "\n",
        "# --- Configurando o LLM ---\n",
        "llm = get_llm_colab()\n",
        "\n",
        "print(\"ÔøΩÔøΩ LLM configurado no Colab!\")\n",
        "print(f\"ü§ñ Tipo: {type(llm).__name__}\")\n",
        "print(\"üí° Agora voc√™ pode usar 'llm' em todos os exemplos do curso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novCr7GObva7"
      },
      "source": [
        "### **Primeiro \"Hello World\" com LangChain**\n",
        "\n",
        "Agora vamos fazer nosso primeiro teste! √â como dar a primeira volta no carro - simples, mas emocionante.\n",
        "\n",
        "Vamos criar um modelo b√°sico e fazer uma pergunta simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydhkOOqFbva8"
      },
      "outputs": [],
      "source": [
        "# Fazendo nossa primeira pergunta!\n",
        "# √â como fazer a primeira pergunta para um amigo novo\n",
        "\n",
        "try:\n",
        "    message = HumanMessage(content=\"O que √© LangChain?\")\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    print(\"üí¨ Chat com DeepSeek:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\" Pergunta: {message.content}\")\n",
        "    print(f\"ü§ñ Resposta: {response}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nxCRJzNbva8"
      },
      "source": [
        "### **Parou aqui e entendeu!** üéØ\n",
        "\n",
        "Se voc√™ conseguiu ver a resposta da IA, **parab√©ns!** Voc√™ acabou de fazer seu primeiro app com LangChain!\n",
        "\n",
        "**O que acabamos de fazer:**\n",
        "1. ‚úÖ Instalamos o LangChain\n",
        "2. ‚úÖ Configuramos a API key\n",
        "3. ‚úÖ Criamos um modelo\n",
        "4. ‚úÖ Fizemos uma pergunta e recebemos resposta\n",
        "\n",
        "**Por que isso √© diferente do ChatGPT normal?**\n",
        "- No ChatGPT, voc√™ s√≥ conversa\n",
        "- Com LangChain, voc√™ **programa** a conversa\n",
        "- Voc√™ pode automatizar, integrar, personalizar\n",
        "- √â como a diferen√ßa entre **usar** um app e **criar** um app\n",
        "\n",
        "---\n",
        "\n",
        "### **Teste R√°pido - Vamos Experimentar Mais!**\n",
        "\n",
        "Agora vamos fazer um teste mais interessante. Vamos criar um \"assistente de programa√ß√£o\" que fala como o Pedro Guth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0uG7Gc3bva8"
      },
      "outputs": [],
      "source": [
        "# Criando um assistente com personalidade\n",
        "# √â como configurar um amigo com caracter√≠sticas espec√≠ficas\n",
        "\n",
        "try:\n",
        "    # Definindo a personalidade do assistente (System Message)\n",
        "    system_message = SystemMessage(content=\"\"\"\n",
        "    Voc√™ √© o Pedro Guth, um instrutor de programa√ß√£o descontra√≠do e direto.\n",
        "    Use linguagem informal, fa√ßa piadas leves e use analogias do dia a dia.\n",
        "    Explique conceitos t√©cnicos de forma simples e engra√ßada.\n",
        "    Use palavr√µes com modera√ß√£o e fun√ß√£o pedag√≥gica.\n",
        "    \"\"\")\n",
        "\n",
        "    # Pergunta do usu√°rio\n",
        "    user_message = HumanMessage(content=\"\"\"\n",
        "    Explique o que √© uma vari√°vel em programa√ß√£o de forma descontra√≠da,\n",
        "    como se fosse o Pedro Guth explicando.\n",
        "    \"\"\")\n",
        "\n",
        "    # Enviando as duas mensagens (sistema + usu√°rio)\n",
        "    response = llm.invoke([system_message, user_message])\n",
        "\n",
        "    print(\"üé≠ Assistente com Personalidade:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ü§ñ Pedro Guth: {response.content}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osbWBbNxbva8"
      },
      "source": [
        "### **Na Pr√°tica, Meu Consagrado!** üí™\n",
        "\n",
        "**O que voc√™ acabou de ver:**\n",
        "\n",
        "1. **System Message**: √â como dar instru√ß√µes para um ator antes da pe√ßa. \"Voc√™ vai interpretar o Pedro Guth\"\n",
        "2. **Human Message**: √â o que o usu√°rio pergunta\n",
        "3. **Response**: √â a resposta personalizada\n",
        "\n",
        "**Por que isso √© poderoso:**\n",
        "- Voc√™ pode criar **diferentes personalidades** para diferentes usos\n",
        "- Um assistente para crian√ßas, outro para CEOs, outro para programadores\n",
        "- √â como ter **m√∫ltiplos funcion√°rios especializados** em uma IA s√≥\n",
        "\n",
        "---\n",
        "\n",
        "### **Compara√ß√£o: Com vs Sem LangChain**\n",
        "\n",
        "**Sem LangChain (c√≥digo manual):**\n",
        "```python\n",
        "# Voc√™ teria que fazer isso tudo manualmente:\n",
        "import requests\n",
        "import json\n",
        "\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {api_key}',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "data = {\n",
        "    'model': 'gpt-3.5-turbo',\n",
        "    'messages': [\n",
        "        {'role': 'system', 'content': 'Seja o Pedro Guth...'},\n",
        "        {'role': 'user', 'content': 'Explique vari√°veis...'}\n",
        "    ],\n",
        "    'temperature': 0.7\n",
        "}\n",
        "\n",
        "response = requests.post('https://api.openai.com/v1/chat/completions',\n",
        "                        headers=headers, json=data)\n",
        "result = response.json()\n",
        "answer = result['choices'][0]['message']['content']\n",
        "```\n",
        "\n",
        "**Com LangChain:**\n",
        "```python\n",
        "# Tudo isso em 3 linhas:\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "response = llm.invoke([system_message, user_message])\n",
        "print(response.content)\n",
        "```\n",
        "\n",
        "**Diferen√ßa**: 20 linhas vs 3 linhas. √â como a diferen√ßa entre **andar a p√©** e **pegar um Uber**! üöó\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumo do que Aprendemos** üìö\n",
        "\n",
        "‚úÖ **O que √© LangChain**: Framework que simplifica o desenvolvimento com IA\n",
        "‚úÖ **Por que usar**: Economiza tempo, c√≥digo e dor de cabe√ßa\n",
        "‚úÖ **Setup b√°sico**: Instala√ß√£o, configura√ß√£o de API key\n",
        "‚úÖ **Primeiro app**: Conversa simples com IA\n",
        "‚úÖ **Personaliza√ß√£o**: Como criar assistentes com personalidade\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/1*8a-WY00_dm43wchuMD9fJg.png' width='1200'>\n",
        "\n",
        "**üéØ Pr√≥ximo m√≥dulo**: Vamos aprender sobre **Prompts** - a arte de falar com IA de forma eficiente!\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Desafio para casa**: Tente criar um assistente que fale como um chef de cozinha e explique como fazer um bolo de chocolate. Use a mesma t√©cnica que aprendemos!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "name": "01_introducao_langchain",
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}