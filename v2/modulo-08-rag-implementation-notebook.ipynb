{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ” RAG na PrÃ¡tica: Transformando Documentos em Conversas Inteligentes!\n\n**MÃ³dulo 8 - LangChain v0.2**\n\nFala pessoal! ğŸš€ Chegou a hora de juntar todas as peÃ§as do quebra-cabeÃ§as que vimos atÃ© agora! Lembram dos **Document Loaders**, **Text Splitters**, **Vector Stores** e **Embeddings**? Pois Ã©, agora vamos usar TUDO isso para criar um sistema RAG completo!\n\nPensa assim: Ã© como se vocÃª fosse montar um assistente pessoal que leu TODOS os seus documentos e pode responder qualquer pergunta sobre eles. LindÃ£o, nÃ©?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-08_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” TÃ¡, mas o que Ã© RAG mesmo?\n\n**RAG** significa **Retrieval-Augmented Generation** (GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o). Parece complicado, mas Ã© bem simples:\n\n### A Analogia da Biblioteca Inteligente ğŸ“š\n\nImagina que vocÃª tem uma biblioteca gigantesca e um bibliotecÃ¡rio super inteligente:\n\n1. **VocÃª faz uma pergunta** ğŸ™‹â€â™‚ï¸\n2. **O bibliotecÃ¡rio procura** nos livros certos ğŸ“–\n3. **Ele encontra informaÃ§Ãµes relevantes** âœ¨\n4. **Ele usa essas informaÃ§Ãµes para te dar uma resposta completa** ğŸ’¡\n\nÃ‰ exatamente isso que o RAG faz! A diferenÃ§a Ã© que:\n- A **biblioteca** sÃ£o seus documentos\n- O **bibliotecÃ¡rio** Ã© o LLM (Gemini, GPT, etc.)\n- A **busca** Ã© feita por similaridade semÃ¢ntica\n\n### Por que RAG Ã© RevolucionÃ¡rio? ğŸŒŸ\n\n- **Conhecimento Atualizado**: NÃ£o precisa retreinar o modelo\n- **Fonte ConfiÃ¡vel**: Baseado nos SEUS documentos\n- **TransparÃªncia**: VocÃª sabe de onde veio a resposta\n- **EficiÃªncia**: Muito mais barato que fine-tuning\n\n**Dica do Pedro**: RAG Ã© como dar superpoderes para o LLM. Ele continua sendo inteligente, mas agora tem acesso aos seus dados especÃ­ficos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—ï¸ Arquitetura do RAG: Como Funciona por Dentro?\n\nVamos entender o fluxo completo do RAG:\n\n```mermaid\ngraph TD\n    A[ğŸ“„ Documentos] --> B[ğŸ”ª Text Splitter]\n    B --> C[ğŸ§  Embeddings]\n    C --> D[ğŸ’¾ Vector Store]\n    E[â“ Pergunta do UsuÃ¡rio] --> F[ğŸ” Retriever]\n    D --> F\n    F --> G[ğŸ“ Contexto Relevante]\n    G --> H[ğŸ¤– LLM + Prompt]\n    E --> H\n    H --> I[âœ¨ Resposta Final]\n```\n\n### Os Dois Momentos do RAG:\n\n#### 1ï¸âƒ£ **Momento da IndexaÃ§Ã£o** (Fazemos UMA vez)\n- Carregamos documentos\n- Dividimos em chunks\n- Criamos embeddings\n- Guardamos no vector store\n\n#### 2ï¸âƒ£ **Momento da Consulta** (Fazemos SEMPRE)\n- Recebemos pergunta\n- Buscamos chunks similares\n- Montamos prompt com contexto\n- LLM gera resposta\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-08_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora comeÃ§ar! Primeiro, vamos instalar e importar tudo que precisamos\n",
        "# Esses sÃ£o os imports essenciais para RAG com LangChain v0.2\n",
        "\n",
        "!pip install -q langchain langchain-google-genai langchain-community faiss-cpu pypdf\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configurar API key do Google\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Imports do LangChain - Tudo que vimos nos mÃ³dulos anteriores!\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "print(\"ğŸš€ Tudo importado! Bora implementar RAG!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossos componentes bÃ¡sicos - Lembram deles?\n",
        "\n",
        "# 1. LLM - Nosso gemini-2.0-flash de sempre\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.1,  # Baixa criatividade para respostas mais precisas\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# 2. Embeddings - Para transformar texto em vetores\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "# 3. Text Splitter - Para dividir documentos em chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Tamanho de cada pedaÃ§o\n",
        "    chunk_overlap=200,    # SobreposiÃ§Ã£o entre pedaÃ§os\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # Como dividir\n",
        ")\n",
        "\n",
        "print(\"âœ… Componentes bÃ¡sicos criados!\")\n",
        "print(f\"ğŸ“± LLM: {llm.model_name}\")\n",
        "print(f\"ğŸ§  Embeddings: {embeddings.model}\")\n",
        "print(f\"âœ‚ï¸ Chunk size: {text_splitter._chunk_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Fase 1: Preparando os Documentos (IndexaÃ§Ã£o)\n\nAgora vamos criar alguns documentos de exemplo para nosso RAG. Na vida real, vocÃªs vÃ£o usar PDFs, pÃ¡ginas web, bancos de dados, etc.\n\n**Dica do Pedro**: Esta Ã© a parte mais importante! A qualidade dos seus documentos determina a qualidade das respostas. Lixo entra, lixo sai! ğŸ—‘ï¸â¡ï¸ğŸ—‘ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar documentos de exemplo sobre tecnologia brasileira\n",
        "# Na prÃ¡tica, vocÃªs carregariam PDFs, sites, etc.\n",
        "\n",
        "documentos_exemplo = [\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        A Embraer Ã© uma das maiores fabricantes de aeronaves do mundo, fundada em 1969 em SÃ£o JosÃ© dos Campos, SP.\n",
        "        A empresa produz jatos regionais e executivos, alÃ©m de aeronaves militares.\n",
        "        Seus principais produtos incluem a famÃ­lia E-Jets e os jatos executivos Phenom e Praetor.\n",
        "        A Embraer emprega mais de 18.000 pessoas e exporta para mais de 100 paÃ­ses.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"embraer_info.txt\", \"categoria\": \"aviacao\"}\n",
        "    },\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        O Nubank foi fundado em 2013 por David VÃ©lez, Cristina Junqueira e Edward Wible.\n",
        "        Ã‰ considerado o maior banco digital da AmÃ©rica Latina, com mais de 80 milhÃµes de clientes.\n",
        "        A empresa revolucionou o mercado financeiro brasileiro com seu cartÃ£o de crÃ©dito sem anuidade.\n",
        "        Em 2021, o Nubank abriu capital na NYSE, sendo uma das maiores IPOs da histÃ³ria brasileira.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"nubank_historia.txt\", \"categoria\": \"fintech\"}\n",
        "    },\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        A inteligÃªncia artificial no Brasil tem crescido exponencialmente nos Ãºltimos anos.\n",
        "        Empresas como C6 Bank, ItaÃº e Bradesco investem pesado em IA para melhorar serviÃ§os.\n",
        "        O Brasil possui mais de 400 startups de IA, com foco em fintechs, agrotechs e healthtechs.\n",
        "        Universidades como USP, UNICAMP e PUC lideram pesquisas em machine learning e deep learning.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"ia_brasil.txt\", \"categoria\": \"tecnologia\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“„ Criamos {len(documentos_exemplo)} documentos de exemplo\")\n",
        "print(\"ğŸ“‹ Categorias:\", [doc[\"metadata\"][\"categoria\"] for doc in documentos_exemplo])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora vamos processar os documentos - Exatamente como vimos nos mÃ³dulos anteriores!\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 1. Converter para objetos Document do LangChain\n",
        "documents = []\n",
        "for doc in documentos_exemplo:\n",
        "    documents.append(\n",
        "        Document(\n",
        "            page_content=doc[\"conteudo\"].strip(),\n",
        "            metadata=doc[\"metadata\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"ğŸ“„ {len(documents)} documentos carregados\")\n",
        "\n",
        "# 2. Dividir em chunks menores\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"âœ‚ï¸ Divididos em {len(chunks)} chunks\")\n",
        "\n",
        "# Vamos ver como ficaram os chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\nğŸ“ Chunk {i+1}:\")\n",
        "    print(f\"   Tamanho: {len(chunk.page_content)} caracteres\")\n",
        "    print(f\"   Fonte: {chunk.metadata['fonte']}\")\n",
        "    print(f\"   PrÃ©via: {chunk.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Criar embeddings e guardar no Vector Store\n",
        "# Aqui Ã© onde a mÃ¡gica acontece! ğŸª„\n",
        "\n",
        "print(\"ğŸ”„ Criando embeddings e construindo vector store...\")\n",
        "print(\"(Isso pode levar alguns segundos...)\")\n",
        "\n",
        "# Criar o FAISS vector store com nossos chunks\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "print(\"âœ… Vector store criado com sucesso!\")\n",
        "print(f\"ğŸ“Š Armazenados {vector_store.index.ntotal} vetores\")\n",
        "print(f\"ğŸ”¢ DimensÃ£o dos vetores: {vector_store.index.d}\")\n",
        "\n",
        "# Criar o retriever - Quem vai buscar documentos similares\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}  # Buscar os 3 chunks mais similares\n",
        ")\n",
        "\n",
        "print(\"ğŸ” Retriever configurado para buscar os 3 documentos mais similares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª Testando o Retriever\n\nAntes de montar a chain completa, vamos testar se nosso retriever estÃ¡ funcionando direitinho. Ã‰ como testar se o bibliotecÃ¡rio consegue encontrar os livros certos!\n\n**Dica do Pedro**: Sempre teste o retriever separadamente! Se ele nÃ£o trouxer documentos relevantes, o RAG inteiro vai dar ruim! ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar algumas perguntas para ver se o retriever funciona\n",
        "perguntas_teste = [\n",
        "    \"Quem fundou o Nubank?\",\n",
        "    \"O que a Embraer produz?\",\n",
        "    \"Como estÃ¡ a IA no Brasil?\"\n",
        "]\n",
        "\n",
        "for pergunta in perguntas_teste:\n",
        "    print(f\"\\nâ“ Pergunta: {pergunta}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Buscar documentos relevantes\n",
        "    docs_relevantes = retriever.get_relevant_documents(pergunta)\n",
        "    \n",
        "    for i, doc in enumerate(docs_relevantes):\n",
        "        print(f\"\\nğŸ“„ Documento {i+1} (Fonte: {doc.metadata['fonte']})\")\n",
        "        print(f\"ğŸ·ï¸ Categoria: {doc.metadata['categoria']}\")\n",
        "        print(f\"ğŸ“ ConteÃºdo: {doc.page_content[:200]}...\")\n",
        "        \n",
        "print(\"\\nğŸ‰ Retriever funcionando perfeitamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”— Montando a Chain RAG Completa\n\nAgora vem a parte mais legal! Vamos juntar tudo em uma chain que:\n1. Recebe uma pergunta\n2. Busca documentos relevantes\n3. Monta um prompt com contexto\n4. Gera uma resposta baseada nos documentos\n\n### A MatemÃ¡tica por trÃ¡s do RAG:\n\n$$RAG(query) = LLM(prompt + context + query)$$\n\nOnde:\n- $context = Retriever(embed(query), VectorStore)$\n- $Retriever$ usa similaridade coseno: $sim(a,b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}$\n\n**Dica do Pedro**: O segredo estÃ¡ no prompt! Ele precisa instruir o LLM a usar APENAS o contexto fornecido. SenÃ£o ele vai \"alucinar\" informaÃ§Ãµes! ğŸ¤–ğŸ’­"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar o prompt template para RAG\n",
        "# Este Ã© o coraÃ§Ã£o do sistema!\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "VocÃª Ã© um assistente especializado em tecnologia e empresas brasileiras.\n",
        "Use APENAS as informaÃ§Ãµes fornecidas no contexto abaixo para responder Ã s perguntas.\n",
        "\n",
        "REGRAS IMPORTANTES:\n",
        "1. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga \"NÃ£o tenho essa informaÃ§Ã£o nos documentos fornecidos\"\n",
        "2. Seja preciso e cite informaÃ§Ãµes especÃ­ficas do contexto\n",
        "3. Responda em portuguÃªs brasileiro\n",
        "4. Seja conversacional mas profissional\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pergunta: {input}\n",
        "\n",
        "Resposta:\n",
        "\"\"\"\n",
        "\n",
        "# Criar o prompt template\n",
        "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
        "\n",
        "print(\"âœ… Prompt template criado!\")\n",
        "print(\"ğŸ¯ O prompt vai instruir o LLM a usar APENAS o contexto fornecido\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora vamos montar as chains!\n",
        "# No LangChain v0.2, usamos essas funÃ§Ãµes especÃ­ficas\n",
        "\n",
        "# 1. Chain para processar documentos e gerar resposta\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# 2. Chain completa de RAG (retriever + document chain)\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"ğŸ”— RAG Chain montada com sucesso!\")\n",
        "print(\"ğŸš€ Sistema RAG completo pronto para uso!\")\n",
        "\n",
        "# Vamos ver a estrutura da chain\n",
        "print(\"\\nğŸ“‹ Componentes da RAG Chain:\")\n",
        "print(f\"   ğŸ” Retriever: {type(retriever).__name__}\")\n",
        "print(f\"   ğŸ¤– LLM: {llm.model_name}\")\n",
        "print(f\"   ğŸ“ Prompt: Personalizado para RAG\")\n",
        "print(f\"   ğŸ’¾ Vector Store: FAISS com {vector_store.index.ntotal} documentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Hora do Show: Testando o RAG!\n\nAgora vem a parte mais divertida! Vamos fazer perguntas para nosso sistema RAG e ver como ele se comporta. \n\nReparem que as respostas vÃ£o incluir:\n- **answer**: A resposta gerada pelo LLM\n- **context**: Os documentos que foram usados como base\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-08_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FunÃ§Ã£o para testar o RAG de forma organizada\n",
        "def testar_rag(pergunta):\n",
        "    print(f\"\\nâ“ PERGUNTA: {pergunta}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Fazer a pergunta para o RAG\n",
        "    resultado = rag_chain.invoke({\"input\": pergunta})\n",
        "    \n",
        "    # Mostrar a resposta\n",
        "    print(f\"\\nğŸ¤– RESPOSTA:\")\n",
        "    print(resultado[\"answer\"])\n",
        "    \n",
        "    # Mostrar os documentos que foram usados\n",
        "    print(f\"\\nğŸ“š DOCUMENTOS CONSULTADOS:\")\n",
        "    for i, doc in enumerate(resultado[\"context\"]):\n",
        "        print(f\"   {i+1}. {doc.metadata['fonte']} ({doc.metadata['categoria']})\")\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "# Vamos testar!\n",
        "perguntas = [\n",
        "    \"Quem sÃ£o os fundadores do Nubank?\",\n",
        "    \"Onde fica a sede da Embraer?\", \n",
        "    \"Quantos clientes o Nubank tem?\"\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "for pergunta in perguntas:\n",
        "    resultado = testar_rag(pergunta)\n",
        "    resultados.append(resultado)\n",
        "    print(\"\\n\" + \"â”€\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar uma pergunta que NÃƒO estÃ¡ nos documentos\n",
        "# Para ver se o sistema vai \"alucinar\" ou ser honesto\n",
        "\n",
        "pergunta_inexistente = \"Qual Ã© o faturamento anual da Magazine Luiza?\"\n",
        "resultado_teste = testar_rag(pergunta_inexistente)\n",
        "\n",
        "print(\"\\nğŸ§ª TESTE DE INTEGRIDADE:\")\n",
        "print(\"Perguntamos sobre algo que nÃ£o estÃ¡ nos documentos.\")\n",
        "print(\"Um bom sistema RAG deve admitir que nÃ£o tem a informaÃ§Ã£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Visualizando o Desempenho do RAG\n\nVamos criar algumas visualizaÃ§Ãµes para entender melhor como nosso RAG estÃ¡ funcionando!\n\n**Dica do Pedro**: Sempre meÃ§a o desempenho do seu RAG! NÃ£o adianta sÃ³ \"achar\" que estÃ¡ bom. Dados nÃ£o mentem! ğŸ“ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# AnÃ¡lise dos documentos recuperados\n",
        "fontes_usadas = []\n",
        "categorias_usadas = []\n",
        "\n",
        "for resultado in resultados:\n",
        "    for doc in resultado[\"context\"]:\n",
        "        fontes_usadas.append(doc.metadata[\"fonte\"])\n",
        "        categorias_usadas.append(doc.metadata[\"categoria\"])\n",
        "\n",
        "# Criar visualizaÃ§Ãµes\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico 1: Fontes mais consultadas\n",
        "fontes_count = Counter(fontes_usadas)\n",
        "ax1.bar(range(len(fontes_count)), list(fontes_count.values()), color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "ax1.set_xticks(range(len(fontes_count)))\n",
        "ax1.set_xticklabels([f.replace('_', '\\n') for f in fontes_count.keys()], rotation=45)\n",
        "ax1.set_title('ğŸ“„ Fontes Mais Consultadas pelo RAG', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('NÃºmero de Consultas')\n",
        "\n",
        "# GrÃ¡fico 2: Categorias mais relevantes\n",
        "cat_count = Counter(categorias_usadas)\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "wedges, texts, autotexts = ax2.pie(cat_count.values(), labels=cat_count.keys(), autopct='%1.1f%%', colors=colors)\n",
        "ax2.set_title('ğŸ·ï¸ DistribuiÃ§Ã£o por Categoria', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š AnÃ¡lise do RAG:\")\n",
        "print(f\"   â€¢ Total de documentos consultados: {len(fontes_usadas)}\")\n",
        "print(f\"   â€¢ Fonte mais usada: {fontes_count.most_common(1)[0][0]}\")\n",
        "print(f\"   â€¢ Categoria mais relevante: {cat_count.most_common(1)[0][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular scores de similaridade para visualizar\n",
        "# Na prÃ¡tica, vocÃªs podem extrair isso do FAISS\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Simular scores de similaridade para as consultas\n",
        "consultas = [\"Nubank fundadores\", \"Embraer sede\", \"Clientes Nubank\"]\n",
        "scores = [\n",
        "    [0.89, 0.76, 0.52],  # Scores para consulta 1\n",
        "    [0.91, 0.68, 0.45],  # Scores para consulta 2\n",
        "    [0.87, 0.73, 0.49]   # Scores para consulta 3\n",
        "]\n",
        "\n",
        "# Criar grÃ¡fico de similaridade\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "x = np.arange(len(consultas))\n",
        "width = 0.25\n",
        "\n",
        "# Criar barras para cada documento recuperado\n",
        "for i in range(3):\n",
        "    scores_doc = [score[i] for score in scores]\n",
        "    ax.bar(x + i*width, scores_doc, width, label=f'Doc {i+1}', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Consultas RAG', fontsize=12)\n",
        "ax.set_ylabel('Score de Similaridade', fontsize=12)\n",
        "ax.set_title('ğŸ¯ Scores de Similaridade por Consulta', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(consultas)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar linha de corte para \"boa similaridade\"\n",
        "ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Threshold (0.7)')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ AnÃ¡lise de Similaridade:\")\n",
        "print(\"   â€¢ Scores acima de 0.7 sÃ£o considerados bons\")\n",
        "print(\"   â€¢ RAG funcionando bem - docs relevantes tÃªm alta similaridade\")\n",
        "print(\"   â€¢ Docs com baixa similaridade servem como contexto adicional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ RAG AvanÃ§ado: Melhorando o Sistema\n\nNosso RAG bÃ¡sico jÃ¡ funciona, mas podemos melhorar muito! Vamos implementar algumas tÃ©cnicas avanÃ§adas:\n\n### 1. **Filtros por Metadata** ğŸ”\n### 2. **Reranking de Resultados** ğŸ“Š\n### 3. **Contexto Mais Rico** ğŸ“š\n\n```mermaid\ngraph TD\n    A[â“ Query] --> B[ğŸ” Retrieval]\n    B --> C[ğŸ·ï¸ Metadata Filter]\n    C --> D[ğŸ“Š Reranking]\n    D --> E[ğŸ“š Context Enhancement]\n    E --> F[ğŸ¤– LLM Generation]\n    F --> G[âœ¨ Enhanced Answer]\n```\n\n**Dica do Pedro**: RAG Ã© como tempero de comida - sempre dÃ¡ para melhorar! A versÃ£o bÃ¡sica mata a fome, mas as tÃ©cnicas avanÃ§adas fazem o prato ficar gourmet! ğŸ‘¨â€ğŸ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG com filtro por categoria\n",
        "def rag_com_filtro(pergunta, categoria=None, k=3):\n",
        "    \"\"\"\n",
        "    RAG que pode filtrar por categoria especÃ­fica\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Buscando: '{pergunta}'\")\n",
        "    if categoria:\n",
        "        print(f\"ğŸ·ï¸ Filtrando por categoria: {categoria}\")\n",
        "    \n",
        "    # Se temos filtro de categoria, vamos buscar manualmente\n",
        "    if categoria:\n",
        "        # Buscar todos os documentos primeiro\n",
        "        todos_docs = retriever.get_relevant_documents(pergunta)\n",
        "        \n",
        "        # Filtrar por categoria\n",
        "        docs_filtrados = [\n",
        "            doc for doc in todos_docs \n",
        "            if doc.metadata.get('categoria') == categoria\n",
        "        ][:k]\n",
        "        \n",
        "        print(f\"ğŸ“„ Encontrados {len(docs_filtrados)} documentos na categoria '{categoria}'\")\n",
        "        \n",
        "        if not docs_filtrados:\n",
        "            return {\"answer\": f\"NÃ£o encontrei documentos na categoria '{categoria}' para responder essa pergunta.\", \"context\": []}\n",
        "        \n",
        "        # Montar contexto manualmente\n",
        "        contexto = \"\\n\\n\".join([doc.page_content for doc in docs_filtrados])\n",
        "        \n",
        "        # Gerar resposta\n",
        "        prompt_formatado = prompt.format(context=contexto, input=pergunta)\n",
        "        resposta = llm.invoke(prompt_formatado)\n",
        "        \n",
        "        return {\"answer\": resposta.content, \"context\": docs_filtrados}\n",
        "    else:\n",
        "        # Usar RAG normal\n",
        "        return rag_chain.invoke({\"input\": pergunta})\n",
        "\n",
        "# Testar RAG com filtro\n",
        "resultado_normal = rag_com_filtro(\"Conte sobre empresas brasileiras de tecnologia\")\n",
        "print(\"\\nğŸ¤– RESPOSTA (SEM FILTRO):\")\n",
        "print(resultado_normal[\"answer\"])\n",
        "\n",
        "resultado_fintech = rag_com_filtro(\"Conte sobre empresas brasileiras de tecnologia\", categoria=\"fintech\")\n",
        "print(\"\\nğŸ¤– RESPOSTA (FILTRO: FINTECH):\")\n",
        "print(resultado_fintech[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG com contexto enriquecido - Incluindo metadados na resposta\n",
        "def rag_enriquecido(pergunta):\n",
        "    \"\"\"\n",
        "    VersÃ£o do RAG que inclui informaÃ§Ãµes sobre as fontes\n",
        "    \"\"\"\n",
        "    resultado = rag_chain.invoke({\"input\": pergunta})\n",
        "    \n",
        "    # Enriquecer a resposta com informaÃ§Ãµes das fontes\n",
        "    fontes = []\n",
        "    for doc in resultado[\"context\"]:\n",
        "        fonte_info = f\"ğŸ“„ {doc.metadata['fonte']} (categoria: {doc.metadata['categoria']})\"\n",
        "        if fonte_info not in fontes:\n",
        "            fontes.append(fonte_info)\n",
        "    \n",
        "    resposta_enriquecida = resultado[\"answer\"]\n",
        "    resposta_enriquecida += \"\\n\\nğŸ“š **Fontes consultadas:**\\n\"\n",
        "    resposta_enriquecida += \"\\n\".join(fontes)\n",
        "    \n",
        "    return {\n",
        "        \"answer\": resposta_enriquecida,\n",
        "        \"context\": resultado[\"context\"],\n",
        "        \"num_fontes\": len(fontes)\n",
        "    }\n",
        "\n",
        "# Testar RAG enriquecido\n",
        "pergunta_teste = \"Qual empresa brasileira Ã© lÃ­der em aviaÃ§Ã£o?\"\n",
        "resultado_enriquecido = rag_enriquecido(pergunta_teste)\n",
        "\n",
        "print(f\"â“ PERGUNTA: {pergunta_teste}\")\n",
        "print(\"=\" * 60)\n",
        "print(resultado_enriquecido[\"answer\"])\n",
        "print(f\"\\nğŸ“Š Consultou {resultado_enriquecido['num_fontes']} fontes diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‹ï¸â€â™‚ï¸ ExercÃ­cio PrÃ¡tico: Construa Seu PrÃ³prio RAG!\n\nAgora Ã© sua vez! Vamos criar um desafio prÃ¡tico para vocÃªs aplicarem tudo que aprenderam.\n\n### ğŸ¯ **DESAFIO 1**: Expandir a Base de Conhecimento\n\nAdicione mais documentos ao nosso sistema RAG sobre tecnologia brasileira. Pode ser sobre:\n- Outras empresas (Mercado Livre, iFood, 99, etc.)\n- Universidades e pesquisa\n- Startups famosas\n- PolÃ­ticas de tecnologia\n\n**Requisitos:**\n1. Pelo menos 3 documentos novos\n2. Categorias diferentes\n3. Testar com perguntas especÃ­ficas\n4. Medir o desempenho\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-08_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 1: Expandir a base de conhecimento\n",
        "# TODO: Adicione seus documentos aqui!\n",
        "\n",
        "# DICA: Use este formato\n",
        "novos_documentos = [\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        # Seu conteÃºdo aqui!\n",
        "        # Ex: InformaÃ§Ãµes sobre o iFood, Mercado Livre, etc.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"seu_arquivo.txt\", \"categoria\": \"sua_categoria\"}\n",
        "    },\n",
        "    # Adicione mais documentos...\n",
        "]\n",
        "\n",
        "# Escreva seu cÃ³digo aqui para:\n",
        "# 1. Converter para Documents\n",
        "# 2. Fazer split dos textos\n",
        "# 3. Adicionar ao vector store existente\n",
        "# 4. Testar com novas perguntas\n",
        "\n",
        "print(\"ğŸ’¡ DICA: Use vector_store.add_documents() para adicionar novos documentos!\")\n",
        "print(\"ğŸ”„ NÃ£o esqueÃ§a de testar o retriever apÃ³s adicionar os documentos!\")\n",
        "\n",
        "# SEU CÃ“DIGO AQUI:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‹ï¸â€â™‚ï¸ ExercÃ­cio 2: RAG com Diferentes Tipos de Busca\n\n### ğŸ¯ **DESAFIO 2**: Implementar Diferentes EstratÃ©gias de Retrieval\n\nO FAISS oferece diferentes tipos de busca. Vamos experimentar:\n- **Similarity**: Busca por similaridade (que jÃ¡ usamos)\n- **MMR (Maximal Marginal Relevance)**: Busca diversificada\n- **Similarity with threshold**: Busca com limite de similaridade\n\n**Sua missÃ£o**: Implementar e comparar essas estratÃ©gias!\n\n**Dica do Pedro**: Cada estratÃ©gia tem seus prÃ³s e contras. MMR Ã© Ã³timo quando vocÃª quer diversidade, threshold quando vocÃª quer qualidade garantida! ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 2: Diferentes estratÃ©gias de retrieval\n",
        "\n",
        "def comparar_estrategias_retrieval(pergunta):\n",
        "    \"\"\"\n",
        "    Compara diferentes estratÃ©gias de busca no mesmo vector store\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ” Testando estratÃ©gias para: '{pergunta}'\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # TODO: Implementar diferentes retrievers\n",
        "    \n",
        "    # 1. Similarity (jÃ¡ temos)\n",
        "    retriever_similarity = vector_store.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    \n",
        "    # 2. MMR - MÃ¡xima RelevÃ¢ncia Marginal\n",
        "    # TODO: Criar retriever MMR\n",
        "    # DICA: search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 6}\n",
        "    \n",
        "    # 3. Similarity com threshold\n",
        "    # TODO: Criar retriever com threshold\n",
        "    # DICA: search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7}\n",
        "    \n",
        "    # Testar cada um e comparar resultados\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Teste suas implementaÃ§Ãµes\n",
        "pergunta_teste = \"Como estÃ¡ o mercado de IA no Brasil?\"\n",
        "# comparar_estrategias_retrieval(pergunta_teste)\n",
        "\n",
        "print(\"ğŸš§ Implemente os diferentes retrievers no cÃ³digo acima!\")\n",
        "print(\"ğŸ“Š Compare os resultados e veja qual funciona melhor para cada tipo de pergunta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Conectando com os PrÃ³ximos MÃ³dulos\n\nLindÃ£o! VocÃªs acabaram de dominar RAG! ğŸ‰ Mas a jornada nÃ£o para aqui...\n\n### ğŸ”® **O que vem pela frente:**\n\n#### **MÃ³dulo 9 - Agents e Tools** ğŸ¤–\n- Vamos dar \"superpoderes\" para o RAG\n- Agents que decidem QUANDO usar RAG\n- Tools que permitem buscar na internet, fazer cÃ¡lculos, etc.\n- RAG + Agents = ğŸ¤¯\n\n#### **MÃ³dulos 10-11 - Projetos Finais** ğŸš€\n- Sistema RAG completo para empresa real\n- RAG + Streamlit = aplicaÃ§Ã£o web\n- Deploy na AWS (que vai ser lindo!)\n\n### ğŸ§  **Conceitos que vamos expandir:**\n\n```mermaid\ngraph TD\n    A[RAG Atual] --> B[RAG + Agents]\n    B --> C[RAG + Tools]\n    C --> D[RAG + Web Interface]\n    D --> E[RAG em ProduÃ§Ã£o]\n    E --> F[RAG + Monitoring]\n```\n\n**Dica do Pedro**: RAG Ã© a base! Tudo que vem depois vai usar os conceitos que vocÃªs aprenderam aqui. Ã‰ como aprender a dirigir - depois vocÃªs vÃ£o dirigir carros mais avanÃ§ados, mas o bÃ¡sico Ã© sempre o mesmo! ğŸš—â¡ï¸ğŸï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VisualizaÃ§Ã£o: EvoluÃ§Ã£o do RAG ao longo do curso\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dados da evoluÃ§Ã£o\n",
        "modulos = ['M6\\nDocs', 'M7\\nVectors', 'M8\\nRAG', 'M9\\nAgents', 'M10-11\\nProjetos', 'M12\\nDeploy']\n",
        "complexidade = [2, 4, 6, 8, 9, 10]\n",
        "utilidade = [1, 3, 7, 9, 10, 10]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plotar as curvas\n",
        "x = np.arange(len(modulos))\n",
        "ax.plot(x, complexidade, 'o-', linewidth=3, markersize=8, label='ğŸ§  Complexidade', color='#ff6b6b')\n",
        "ax.plot(x, utilidade, 'o-', linewidth=3, markersize=8, label='âš¡ Utilidade PrÃ¡tica', color='#4ecdc4')\n",
        "\n",
        "# Destacar o mÃ³dulo atual\n",
        "ax.axvline(x=2, color='gold', linestyle='--', alpha=0.7, linewidth=2)\n",
        "ax.text(2, 8.5, 'â† VOCÃŠ ESTÃ AQUI!', fontsize=12, fontweight='bold', \n",
        "        ha='center', bbox=dict(boxstyle='round,pad=0.3', facecolor='gold', alpha=0.7))\n",
        "\n",
        "# Customizar o grÃ¡fico\n",
        "ax.set_xlabel('MÃ³dulos do Curso', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('NÃ­vel (1-10)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('ğŸš€ EvoluÃ§Ã£o do Conhecimento em RAG/LangChain', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(modulos)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(0, 11)\n",
        "\n",
        "# Adicionar anotaÃ§Ãµes\n",
        "ax.annotate('Fundamentos', xy=(1, 3), xytext=(0.5, 5),\n",
        "            arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "ax.annotate('RAG Completo!', xy=(2, 7), xytext=(2, 5),\n",
        "            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7),\n",
        "            fontsize=10, ha='center', color='green', fontweight='bold')\n",
        "\n",
        "ax.annotate('ProduÃ§Ã£o', xy=(5, 10), xytext=(4.5, 8.5),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“ˆ AnÃ¡lise da Jornada:\")\n",
        "print(f\"   â€¢ Complexidade atual: {complexidade[2]}/10 - Moderada\")\n",
        "print(f\"   â€¢ Utilidade atual: {utilidade[2]}/10 - Alta!\")\n",
        "print(\"   â€¢ PrÃ³ximos passos: Agents (complexidade +2, utilidade +2)\")\n",
        "print(\"   â€¢ Objetivo: ProduÃ§Ã£o (complexidade +4, utilidade +3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“‹ Resumo do MÃ³dulo: O que Aprendemos?\n\nCaramba, que mÃ³dulo intenso! ğŸ¤¯ Vamos recapitular tudo que rolou:\n\n### âœ… **Conceitos Dominados:**\n\n1. **RAG Architecture** ğŸ—ï¸\n   - Retrieval-Augmented Generation\n   - Fase de indexaÃ§Ã£o vs. fase de consulta\n   - Como juntar retrieval + generation\n\n2. **ImplementaÃ§Ã£o PrÃ¡tica** ğŸ’»\n   - `create_retrieval_chain()` e `create_stuff_documents_chain()`\n   - Integration com componentes dos mÃ³dulos anteriores\n   - Prompt engineering para RAG\n\n3. **TÃ©cnicas AvanÃ§adas** ğŸš€\n   - Filtros por metadata\n   - Diferentes estratÃ©gias de retrieval\n   - Context enrichment\n   - Performance monitoring\n\n### ğŸ“Š **FÃ³rmulas e Conceitos TÃ©cnicos:**\n\n$$RAG(query) = LLM(prompt + context + query)$$\n$$context = Retriever(embed(query), VectorStore)$$\n$$similarity(a,b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}$$\n\n### ğŸ¯ **AplicaÃ§Ãµes Reais:**\n- Chatbots corporativos\n- Sistemas de suporte tÃ©cnico\n- Assistentes de pesquisa\n- Q&A sobre documentaÃ§Ã£o\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-08_img_05.png)\n\n**Dica do Pedro Final**: RAG mudou o jogo da IA! VocÃªs agora sabem como fazer LLMs conversarem com qualquer base de dados. Isso vale ouro no mercado! ğŸ’°âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ParabÃ©ns! Vamos celebrar com algumas estatÃ­sticas finais\n",
        "import datetime\n",
        "\n",
        "print(\"ğŸ‰ PARABÃ‰NS! VocÃª completou o MÃ³dulo 8 - RAG Implementation!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# EstatÃ­sticas do que foi construÃ­do\n",
        "stats = {\n",
        "    \"Documentos processados\": len(documents),\n",
        "    \"Chunks criados\": len(chunks),\n",
        "    \"Embeddings gerados\": vector_store.index.ntotal if 'vector_store' in locals() else 0,\n",
        "    \"DimensÃ£o dos vetores\": vector_store.index.d if 'vector_store' in locals() else 0,\n",
        "    \"Consultas RAG testadas\": len(resultados) if 'resultados' in locals() else 0,\n",
        "    \"Chains criadas\": 2,  # document_chain + rag_chain\n",
        "    \"Componentes integrados\": 4  # LLM, Embeddings, VectorStore, Retriever\n",
        "}\n",
        "\n",
        "print(\"ğŸ“Š ESTATÃSTICAS DA SUA IMPLEMENTAÃ‡ÃƒO RAG:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"   â€¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\nğŸš€ PRÃ“XIMOS PASSOS:\")\n",
        "print(\"   â€¢ MÃ³dulo 9: Agents e Tools - RAG com superpoderes!\")\n",
        "print(\"   â€¢ MÃ³dulo 10-11: Projetos finais - RAG em aplicaÃ§Ãµes reais\")\n",
        "print(\"   â€¢ MÃ³dulo 12: Deploy - RAG em produÃ§Ã£o com Streamlit\")\n",
        "\n",
        "print(\"\\nğŸ’¡ DICAS PARA CONTINUAR ESTUDANDO:\")\n",
        "print(\"   â€¢ Teste com seus prÃ³prios documentos\")\n",
        "print(\"   â€¢ Experimente diferentes embeddings\")\n",
        "print(\"   â€¢ Implemente os exercÃ­cios propostos\")\n",
        "print(\"   â€¢ Leia sobre RAG avanÃ§ado (GraphRAG, MultiVector RAG, etc.)\")\n",
        "\n",
        "print(f\"\\nğŸ“… ConcluÃ­do em: {datetime.datetime.now().strftime('%d/%m/%Y Ã s %H:%M')}\")\n",
        "print(\"\\nğŸ† VOCÃŠ AGORA Ã‰ UM RAG MASTER! Liiindo! ğŸ‡§ğŸ‡·ğŸš€\")"
      ]
    }
  ]
}