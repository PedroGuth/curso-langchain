{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç RAG na Pr√°tica: Transformando Documentos em Conversas Inteligentes!\n\n**M√≥dulo 8 - LangChain v0.2**\n\nFala pessoal! üöÄ Chegou a hora de juntar todas as pe√ßas do quebra-cabe√ßas que vimos at√© agora! Lembram dos **Document Loaders**, **Text Splitters**, **Vector Stores** e **Embeddings**? Pois √©, agora vamos usar TUDO isso para criar um sistema RAG completo!\n\nPensa assim: √© como se voc√™ fosse montar um assistente pessoal que leu TODOS os seus documentos e pode responder qualquer pergunta sobre eles. Lind√£o, n√©?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-08_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î T√°, mas o que √© RAG mesmo?\n\n**RAG** significa **Retrieval-Augmented Generation** (Gera√ß√£o Aumentada por Recupera√ß√£o). Parece complicado, mas √© bem simples:\n\n### A Analogia da Biblioteca Inteligente üìö\n\nImagina que voc√™ tem uma biblioteca gigantesca e um bibliotec√°rio super inteligente:\n\n1. **Voc√™ faz uma pergunta** üôã‚Äç‚ôÇÔ∏è\n2. **O bibliotec√°rio procura** nos livros certos üìñ\n3. **Ele encontra informa√ß√µes relevantes** ‚ú®\n4. **Ele usa essas informa√ß√µes para te dar uma resposta completa** üí°\n\n√â exatamente isso que o RAG faz! A diferen√ßa √© que:\n- A **biblioteca** s√£o seus documentos\n- O **bibliotec√°rio** √© o LLM (Gemini, GPT, etc.)\n- A **busca** √© feita por similaridade sem√¢ntica\n\n### Por que RAG √© Revolucion√°rio? üåü\n\n- **Conhecimento Atualizado**: N√£o precisa retreinar o modelo\n- **Fonte Confi√°vel**: Baseado nos SEUS documentos\n- **Transpar√™ncia**: Voc√™ sabe de onde veio a resposta\n- **Efici√™ncia**: Muito mais barato que fine-tuning\n\n**Dica do Pedro**: RAG √© como dar superpoderes para o LLM. Ele continua sendo inteligente, mas agora tem acesso aos seus dados espec√≠ficos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Arquitetura do RAG: Como Funciona por Dentro?\n\nVamos entender o fluxo completo do RAG:\n\n```mermaid\ngraph TD\n    A[üìÑ Documentos] --> B[üî™ Text Splitter]\n    B --> C[üß† Embeddings]\n    C --> D[üíæ Vector Store]\n    E[‚ùì Pergunta do Usu√°rio] --> F[üîç Retriever]\n    D --> F\n    F --> G[üìù Contexto Relevante]\n    G --> H[ü§ñ LLM + Prompt]\n    E --> H\n    H --> I[‚ú® Resposta Final]\n```\n\n### Os Dois Momentos do RAG:\n\n#### 1Ô∏è‚É£ **Momento da Indexa√ß√£o** (Fazemos UMA vez)\n- Carregamos documentos\n- Dividimos em chunks\n- Criamos embeddings\n- Guardamos no vector store\n\n#### 2Ô∏è‚É£ **Momento da Consulta** (Fazemos SEMPRE)\n- Recebemos pergunta\n- Buscamos chunks similares\n- Montamos prompt com contexto\n- LLM gera resposta\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-08_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora come√ßar! Primeiro, vamos instalar e importar tudo que precisamos\n",
        "# Esses s√£o os imports essenciais para RAG com LangChain v0.2\n",
        "\n",
        "!pip install -q langchain langchain-google-genai langchain-community faiss-cpu pypdf\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configurar API key do Google\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Imports do LangChain - Tudo que vimos nos m√≥dulos anteriores!\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "print(\"üöÄ Tudo importado! Bora implementar RAG!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar nossos componentes b√°sicos - Lembram deles?\n",
        "\n",
        "# 1. LLM - Nosso gemini-2.0-flash de sempre\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.1,  # Baixa criatividade para respostas mais precisas\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# 2. Embeddings - Para transformar texto em vetores\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "# 3. Text Splitter - Para dividir documentos em chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Tamanho de cada peda√ßo\n",
        "    chunk_overlap=200,    # Sobreposi√ß√£o entre peda√ßos\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # Como dividir\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Componentes b√°sicos criados!\")\n",
        "print(f\"üì± LLM: {llm.model_name}\")\n",
        "print(f\"üß† Embeddings: {embeddings.model}\")\n",
        "print(f\"‚úÇÔ∏è Chunk size: {text_splitter._chunk_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Fase 1: Preparando os Documentos (Indexa√ß√£o)\n\nAgora vamos criar alguns documentos de exemplo para nosso RAG. Na vida real, voc√™s v√£o usar PDFs, p√°ginas web, bancos de dados, etc.\n\n**Dica do Pedro**: Esta √© a parte mais importante! A qualidade dos seus documentos determina a qualidade das respostas. Lixo entra, lixo sai! üóëÔ∏è‚û°Ô∏èüóëÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar documentos de exemplo sobre tecnologia brasileira\n",
        "# Na pr√°tica, voc√™s carregariam PDFs, sites, etc.\n",
        "\n",
        "documentos_exemplo = [\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        A Embraer √© uma das maiores fabricantes de aeronaves do mundo, fundada em 1969 em S√£o Jos√© dos Campos, SP.\n",
        "        A empresa produz jatos regionais e executivos, al√©m de aeronaves militares.\n",
        "        Seus principais produtos incluem a fam√≠lia E-Jets e os jatos executivos Phenom e Praetor.\n",
        "        A Embraer emprega mais de 18.000 pessoas e exporta para mais de 100 pa√≠ses.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"embraer_info.txt\", \"categoria\": \"aviacao\"}\n",
        "    },\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        O Nubank foi fundado em 2013 por David V√©lez, Cristina Junqueira e Edward Wible.\n",
        "        √â considerado o maior banco digital da Am√©rica Latina, com mais de 80 milh√µes de clientes.\n",
        "        A empresa revolucionou o mercado financeiro brasileiro com seu cart√£o de cr√©dito sem anuidade.\n",
        "        Em 2021, o Nubank abriu capital na NYSE, sendo uma das maiores IPOs da hist√≥ria brasileira.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"nubank_historia.txt\", \"categoria\": \"fintech\"}\n",
        "    },\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        A intelig√™ncia artificial no Brasil tem crescido exponencialmente nos √∫ltimos anos.\n",
        "        Empresas como C6 Bank, Ita√∫ e Bradesco investem pesado em IA para melhorar servi√ßos.\n",
        "        O Brasil possui mais de 400 startups de IA, com foco em fintechs, agrotechs e healthtechs.\n",
        "        Universidades como USP, UNICAMP e PUC lideram pesquisas em machine learning e deep learning.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"ia_brasil.txt\", \"categoria\": \"tecnologia\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"üìÑ Criamos {len(documentos_exemplo)} documentos de exemplo\")\n",
        "print(\"üìã Categorias:\", [doc[\"metadata\"][\"categoria\"] for doc in documentos_exemplo])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora vamos processar os documentos - Exatamente como vimos nos m√≥dulos anteriores!\n",
        "from langchain.schema import Document\n",
        "\n",
        "# 1. Converter para objetos Document do LangChain\n",
        "documents = []\n",
        "for doc in documentos_exemplo:\n",
        "    documents.append(\n",
        "        Document(\n",
        "            page_content=doc[\"conteudo\"].strip(),\n",
        "            metadata=doc[\"metadata\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(f\"üìÑ {len(documents)} documentos carregados\")\n",
        "\n",
        "# 2. Dividir em chunks menores\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÇÔ∏è Divididos em {len(chunks)} chunks\")\n",
        "\n",
        "# Vamos ver como ficaram os chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\nüìù Chunk {i+1}:\")\n",
        "    print(f\"   Tamanho: {len(chunk.page_content)} caracteres\")\n",
        "    print(f\"   Fonte: {chunk.metadata['fonte']}\")\n",
        "    print(f\"   Pr√©via: {chunk.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Criar embeddings e guardar no Vector Store\n",
        "# Aqui √© onde a m√°gica acontece! ü™Ñ\n",
        "\n",
        "print(\"üîÑ Criando embeddings e construindo vector store...\")\n",
        "print(\"(Isso pode levar alguns segundos...)\")\n",
        "\n",
        "# Criar o FAISS vector store com nossos chunks\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "print(\"‚úÖ Vector store criado com sucesso!\")\n",
        "print(f\"üìä Armazenados {vector_store.index.ntotal} vetores\")\n",
        "print(f\"üî¢ Dimens√£o dos vetores: {vector_store.index.d}\")\n",
        "\n",
        "# Criar o retriever - Quem vai buscar documentos similares\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}  # Buscar os 3 chunks mais similares\n",
        ")\n",
        "\n",
        "print(\"üîç Retriever configurado para buscar os 3 documentos mais similares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Testando o Retriever\n\nAntes de montar a chain completa, vamos testar se nosso retriever est√° funcionando direitinho. √â como testar se o bibliotec√°rio consegue encontrar os livros certos!\n\n**Dica do Pedro**: Sempre teste o retriever separadamente! Se ele n√£o trouxer documentos relevantes, o RAG inteiro vai dar ruim! üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar algumas perguntas para ver se o retriever funciona\n",
        "perguntas_teste = [\n",
        "    \"Quem fundou o Nubank?\",\n",
        "    \"O que a Embraer produz?\",\n",
        "    \"Como est√° a IA no Brasil?\"\n",
        "]\n",
        "\n",
        "for pergunta in perguntas_teste:\n",
        "    print(f\"\\n‚ùì Pergunta: {pergunta}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Buscar documentos relevantes\n",
        "    docs_relevantes = retriever.get_relevant_documents(pergunta)\n",
        "    \n",
        "    for i, doc in enumerate(docs_relevantes):\n",
        "        print(f\"\\nüìÑ Documento {i+1} (Fonte: {doc.metadata['fonte']})\")\n",
        "        print(f\"üè∑Ô∏è Categoria: {doc.metadata['categoria']}\")\n",
        "        print(f\"üìù Conte√∫do: {doc.page_content[:200]}...\")\n",
        "        \n",
        "print(\"\\nüéâ Retriever funcionando perfeitamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Montando a Chain RAG Completa\n\nAgora vem a parte mais legal! Vamos juntar tudo em uma chain que:\n1. Recebe uma pergunta\n2. Busca documentos relevantes\n3. Monta um prompt com contexto\n4. Gera uma resposta baseada nos documentos\n\n### A Matem√°tica por tr√°s do RAG:\n\n$$RAG(query) = LLM(prompt + context + query)$$\n\nOnde:\n- $context = Retriever(embed(query), VectorStore)$\n- $Retriever$ usa similaridade coseno: $sim(a,b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}$\n\n**Dica do Pedro**: O segredo est√° no prompt! Ele precisa instruir o LLM a usar APENAS o contexto fornecido. Sen√£o ele vai \"alucinar\" informa√ß√µes! ü§ñüí≠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar o prompt template para RAG\n",
        "# Este √© o cora√ß√£o do sistema!\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "Voc√™ √© um assistente especializado em tecnologia e empresas brasileiras.\n",
        "Use APENAS as informa√ß√µes fornecidas no contexto abaixo para responder √†s perguntas.\n",
        "\n",
        "REGRAS IMPORTANTES:\n",
        "1. Se a informa√ß√£o n√£o estiver no contexto, diga \"N√£o tenho essa informa√ß√£o nos documentos fornecidos\"\n",
        "2. Seja preciso e cite informa√ß√µes espec√≠ficas do contexto\n",
        "3. Responda em portugu√™s brasileiro\n",
        "4. Seja conversacional mas profissional\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pergunta: {input}\n",
        "\n",
        "Resposta:\n",
        "\"\"\"\n",
        "\n",
        "# Criar o prompt template\n",
        "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
        "\n",
        "print(\"‚úÖ Prompt template criado!\")\n",
        "print(\"üéØ O prompt vai instruir o LLM a usar APENAS o contexto fornecido\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agora vamos montar as chains!\n",
        "# No LangChain v0.2, usamos essas fun√ß√µes espec√≠ficas\n",
        "\n",
        "# 1. Chain para processar documentos e gerar resposta\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# 2. Chain completa de RAG (retriever + document chain)\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"üîó RAG Chain montada com sucesso!\")\n",
        "print(\"üöÄ Sistema RAG completo pronto para uso!\")\n",
        "\n",
        "# Vamos ver a estrutura da chain\n",
        "print(\"\\nüìã Componentes da RAG Chain:\")\n",
        "print(f\"   üîç Retriever: {type(retriever).__name__}\")\n",
        "print(f\"   ü§ñ LLM: {llm.model_name}\")\n",
        "print(f\"   üìù Prompt: Personalizado para RAG\")\n",
        "print(f\"   üíæ Vector Store: FAISS com {vector_store.index.ntotal} documentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Hora do Show: Testando o RAG!\n\nAgora vem a parte mais divertida! Vamos fazer perguntas para nosso sistema RAG e ver como ele se comporta. \n\nReparem que as respostas v√£o incluir:\n- **answer**: A resposta gerada pelo LLM\n- **context**: Os documentos que foram usados como base\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-08_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para testar o RAG de forma organizada\n",
        "def testar_rag(pergunta):\n",
        "    print(f\"\\n‚ùì PERGUNTA: {pergunta}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Fazer a pergunta para o RAG\n",
        "    resultado = rag_chain.invoke({\"input\": pergunta})\n",
        "    \n",
        "    # Mostrar a resposta\n",
        "    print(f\"\\nü§ñ RESPOSTA:\")\n",
        "    print(resultado[\"answer\"])\n",
        "    \n",
        "    # Mostrar os documentos que foram usados\n",
        "    print(f\"\\nüìö DOCUMENTOS CONSULTADOS:\")\n",
        "    for i, doc in enumerate(resultado[\"context\"]):\n",
        "        print(f\"   {i+1}. {doc.metadata['fonte']} ({doc.metadata['categoria']})\")\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "# Vamos testar!\n",
        "perguntas = [\n",
        "    \"Quem s√£o os fundadores do Nubank?\",\n",
        "    \"Onde fica a sede da Embraer?\", \n",
        "    \"Quantos clientes o Nubank tem?\"\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "for pergunta in perguntas:\n",
        "    resultado = testar_rag(pergunta)\n",
        "    resultados.append(resultado)\n",
        "    print(\"\\n\" + \"‚îÄ\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar uma pergunta que N√ÉO est√° nos documentos\n",
        "# Para ver se o sistema vai \"alucinar\" ou ser honesto\n",
        "\n",
        "pergunta_inexistente = \"Qual √© o faturamento anual da Magazine Luiza?\"\n",
        "resultado_teste = testar_rag(pergunta_inexistente)\n",
        "\n",
        "print(\"\\nüß™ TESTE DE INTEGRIDADE:\")\n",
        "print(\"Perguntamos sobre algo que n√£o est√° nos documentos.\")\n",
        "print(\"Um bom sistema RAG deve admitir que n√£o tem a informa√ß√£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizando o Desempenho do RAG\n\nVamos criar algumas visualiza√ß√µes para entender melhor como nosso RAG est√° funcionando!\n\n**Dica do Pedro**: Sempre me√ßa o desempenho do seu RAG! N√£o adianta s√≥ \"achar\" que est√° bom. Dados n√£o mentem! üìà"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# An√°lise dos documentos recuperados\n",
        "fontes_usadas = []\n",
        "categorias_usadas = []\n",
        "\n",
        "for resultado in resultados:\n",
        "    for doc in resultado[\"context\"]:\n",
        "        fontes_usadas.append(doc.metadata[\"fonte\"])\n",
        "        categorias_usadas.append(doc.metadata[\"categoria\"])\n",
        "\n",
        "# Criar visualiza√ß√µes\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Fontes mais consultadas\n",
        "fontes_count = Counter(fontes_usadas)\n",
        "ax1.bar(range(len(fontes_count)), list(fontes_count.values()), color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "ax1.set_xticks(range(len(fontes_count)))\n",
        "ax1.set_xticklabels([f.replace('_', '\\n') for f in fontes_count.keys()], rotation=45)\n",
        "ax1.set_title('üìÑ Fontes Mais Consultadas pelo RAG', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('N√∫mero de Consultas')\n",
        "\n",
        "# Gr√°fico 2: Categorias mais relevantes\n",
        "cat_count = Counter(categorias_usadas)\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "wedges, texts, autotexts = ax2.pie(cat_count.values(), labels=cat_count.keys(), autopct='%1.1f%%', colors=colors)\n",
        "ax2.set_title('üè∑Ô∏è Distribui√ß√£o por Categoria', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä An√°lise do RAG:\")\n",
        "print(f\"   ‚Ä¢ Total de documentos consultados: {len(fontes_usadas)}\")\n",
        "print(f\"   ‚Ä¢ Fonte mais usada: {fontes_count.most_common(1)[0][0]}\")\n",
        "print(f\"   ‚Ä¢ Categoria mais relevante: {cat_count.most_common(1)[0][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos simular scores de similaridade para visualizar\n",
        "# Na pr√°tica, voc√™s podem extrair isso do FAISS\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Simular scores de similaridade para as consultas\n",
        "consultas = [\"Nubank fundadores\", \"Embraer sede\", \"Clientes Nubank\"]\n",
        "scores = [\n",
        "    [0.89, 0.76, 0.52],  # Scores para consulta 1\n",
        "    [0.91, 0.68, 0.45],  # Scores para consulta 2\n",
        "    [0.87, 0.73, 0.49]   # Scores para consulta 3\n",
        "]\n",
        "\n",
        "# Criar gr√°fico de similaridade\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "x = np.arange(len(consultas))\n",
        "width = 0.25\n",
        "\n",
        "# Criar barras para cada documento recuperado\n",
        "for i in range(3):\n",
        "    scores_doc = [score[i] for score in scores]\n",
        "    ax.bar(x + i*width, scores_doc, width, label=f'Doc {i+1}', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Consultas RAG', fontsize=12)\n",
        "ax.set_ylabel('Score de Similaridade', fontsize=12)\n",
        "ax.set_title('üéØ Scores de Similaridade por Consulta', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(consultas)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar linha de corte para \"boa similaridade\"\n",
        "ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Threshold (0.7)')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ An√°lise de Similaridade:\")\n",
        "print(\"   ‚Ä¢ Scores acima de 0.7 s√£o considerados bons\")\n",
        "print(\"   ‚Ä¢ RAG funcionando bem - docs relevantes t√™m alta similaridade\")\n",
        "print(\"   ‚Ä¢ Docs com baixa similaridade servem como contexto adicional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ RAG Avan√ßado: Melhorando o Sistema\n\nNosso RAG b√°sico j√° funciona, mas podemos melhorar muito! Vamos implementar algumas t√©cnicas avan√ßadas:\n\n### 1. **Filtros por Metadata** üîç\n### 2. **Reranking de Resultados** üìä\n### 3. **Contexto Mais Rico** üìö\n\n```mermaid\ngraph TD\n    A[‚ùì Query] --> B[üîç Retrieval]\n    B --> C[üè∑Ô∏è Metadata Filter]\n    C --> D[üìä Reranking]\n    D --> E[üìö Context Enhancement]\n    E --> F[ü§ñ LLM Generation]\n    F --> G[‚ú® Enhanced Answer]\n```\n\n**Dica do Pedro**: RAG √© como tempero de comida - sempre d√° para melhorar! A vers√£o b√°sica mata a fome, mas as t√©cnicas avan√ßadas fazem o prato ficar gourmet! üë®‚Äçüç≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG com filtro por categoria\n",
        "def rag_com_filtro(pergunta, categoria=None, k=3):\n",
        "    \"\"\"\n",
        "    RAG que pode filtrar por categoria espec√≠fica\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Buscando: '{pergunta}'\")\n",
        "    if categoria:\n",
        "        print(f\"üè∑Ô∏è Filtrando por categoria: {categoria}\")\n",
        "    \n",
        "    # Se temos filtro de categoria, vamos buscar manualmente\n",
        "    if categoria:\n",
        "        # Buscar todos os documentos primeiro\n",
        "        todos_docs = retriever.get_relevant_documents(pergunta)\n",
        "        \n",
        "        # Filtrar por categoria\n",
        "        docs_filtrados = [\n",
        "            doc for doc in todos_docs \n",
        "            if doc.metadata.get('categoria') == categoria\n",
        "        ][:k]\n",
        "        \n",
        "        print(f\"üìÑ Encontrados {len(docs_filtrados)} documentos na categoria '{categoria}'\")\n",
        "        \n",
        "        if not docs_filtrados:\n",
        "            return {\"answer\": f\"N√£o encontrei documentos na categoria '{categoria}' para responder essa pergunta.\", \"context\": []}\n",
        "        \n",
        "        # Montar contexto manualmente\n",
        "        contexto = \"\\n\\n\".join([doc.page_content for doc in docs_filtrados])\n",
        "        \n",
        "        # Gerar resposta\n",
        "        prompt_formatado = prompt.format(context=contexto, input=pergunta)\n",
        "        resposta = llm.invoke(prompt_formatado)\n",
        "        \n",
        "        return {\"answer\": resposta.content, \"context\": docs_filtrados}\n",
        "    else:\n",
        "        # Usar RAG normal\n",
        "        return rag_chain.invoke({\"input\": pergunta})\n",
        "\n",
        "# Testar RAG com filtro\n",
        "resultado_normal = rag_com_filtro(\"Conte sobre empresas brasileiras de tecnologia\")\n",
        "print(\"\\nü§ñ RESPOSTA (SEM FILTRO):\")\n",
        "print(resultado_normal[\"answer\"])\n",
        "\n",
        "resultado_fintech = rag_com_filtro(\"Conte sobre empresas brasileiras de tecnologia\", categoria=\"fintech\")\n",
        "print(\"\\nü§ñ RESPOSTA (FILTRO: FINTECH):\")\n",
        "print(resultado_fintech[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG com contexto enriquecido - Incluindo metadados na resposta\n",
        "def rag_enriquecido(pergunta):\n",
        "    \"\"\"\n",
        "    Vers√£o do RAG que inclui informa√ß√µes sobre as fontes\n",
        "    \"\"\"\n",
        "    resultado = rag_chain.invoke({\"input\": pergunta})\n",
        "    \n",
        "    # Enriquecer a resposta com informa√ß√µes das fontes\n",
        "    fontes = []\n",
        "    for doc in resultado[\"context\"]:\n",
        "        fonte_info = f\"üìÑ {doc.metadata['fonte']} (categoria: {doc.metadata['categoria']})\"\n",
        "        if fonte_info not in fontes:\n",
        "            fontes.append(fonte_info)\n",
        "    \n",
        "    resposta_enriquecida = resultado[\"answer\"]\n",
        "    resposta_enriquecida += \"\\n\\nüìö **Fontes consultadas:**\\n\"\n",
        "    resposta_enriquecida += \"\\n\".join(fontes)\n",
        "    \n",
        "    return {\n",
        "        \"answer\": resposta_enriquecida,\n",
        "        \"context\": resultado[\"context\"],\n",
        "        \"num_fontes\": len(fontes)\n",
        "    }\n",
        "\n",
        "# Testar RAG enriquecido\n",
        "pergunta_teste = \"Qual empresa brasileira √© l√≠der em avia√ß√£o?\"\n",
        "resultado_enriquecido = rag_enriquecido(pergunta_teste)\n",
        "\n",
        "print(f\"‚ùì PERGUNTA: {pergunta_teste}\")\n",
        "print(\"=\" * 60)\n",
        "print(resultado_enriquecido[\"answer\"])\n",
        "print(f\"\\nüìä Consultou {resultado_enriquecido['num_fontes']} fontes diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è‚Äç‚ôÇÔ∏è Exerc√≠cio Pr√°tico: Construa Seu Pr√≥prio RAG!\n\nAgora √© sua vez! Vamos criar um desafio pr√°tico para voc√™s aplicarem tudo que aprenderam.\n\n### üéØ **DESAFIO 1**: Expandir a Base de Conhecimento\n\nAdicione mais documentos ao nosso sistema RAG sobre tecnologia brasileira. Pode ser sobre:\n- Outras empresas (Mercado Livre, iFood, 99, etc.)\n- Universidades e pesquisa\n- Startups famosas\n- Pol√≠ticas de tecnologia\n\n**Requisitos:**\n1. Pelo menos 3 documentos novos\n2. Categorias diferentes\n3. Testar com perguntas espec√≠ficas\n4. Medir o desempenho\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-08_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 1: Expandir a base de conhecimento\n",
        "# TODO: Adicione seus documentos aqui!\n",
        "\n",
        "# DICA: Use este formato\n",
        "novos_documentos = [\n",
        "    {\n",
        "        \"conteudo\": \"\"\"\n",
        "        # Seu conte√∫do aqui!\n",
        "        # Ex: Informa√ß√µes sobre o iFood, Mercado Livre, etc.\n",
        "        \"\"\",\n",
        "        \"metadata\": {\"fonte\": \"seu_arquivo.txt\", \"categoria\": \"sua_categoria\"}\n",
        "    },\n",
        "    # Adicione mais documentos...\n",
        "]\n",
        "\n",
        "# Escreva seu c√≥digo aqui para:\n",
        "# 1. Converter para Documents\n",
        "# 2. Fazer split dos textos\n",
        "# 3. Adicionar ao vector store existente\n",
        "# 4. Testar com novas perguntas\n",
        "\n",
        "print(\"üí° DICA: Use vector_store.add_documents() para adicionar novos documentos!\")\n",
        "print(\"üîÑ N√£o esque√ßa de testar o retriever ap√≥s adicionar os documentos!\")\n",
        "\n",
        "# SEU C√ìDIGO AQUI:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è‚Äç‚ôÇÔ∏è Exerc√≠cio 2: RAG com Diferentes Tipos de Busca\n\n### üéØ **DESAFIO 2**: Implementar Diferentes Estrat√©gias de Retrieval\n\nO FAISS oferece diferentes tipos de busca. Vamos experimentar:\n- **Similarity**: Busca por similaridade (que j√° usamos)\n- **MMR (Maximal Marginal Relevance)**: Busca diversificada\n- **Similarity with threshold**: Busca com limite de similaridade\n\n**Sua miss√£o**: Implementar e comparar essas estrat√©gias!\n\n**Dica do Pedro**: Cada estrat√©gia tem seus pr√≥s e contras. MMR √© √≥timo quando voc√™ quer diversidade, threshold quando voc√™ quer qualidade garantida! üéØ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERC√çCIO 2: Diferentes estrat√©gias de retrieval\n",
        "\n",
        "def comparar_estrategias_retrieval(pergunta):\n",
        "    \"\"\"\n",
        "    Compara diferentes estrat√©gias de busca no mesmo vector store\n",
        "    \"\"\"\n",
        "    print(f\"üîç Testando estrat√©gias para: '{pergunta}'\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # TODO: Implementar diferentes retrievers\n",
        "    \n",
        "    # 1. Similarity (j√° temos)\n",
        "    retriever_similarity = vector_store.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "    \n",
        "    # 2. MMR - M√°xima Relev√¢ncia Marginal\n",
        "    # TODO: Criar retriever MMR\n",
        "    # DICA: search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 6}\n",
        "    \n",
        "    # 3. Similarity com threshold\n",
        "    # TODO: Criar retriever com threshold\n",
        "    # DICA: search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7}\n",
        "    \n",
        "    # Testar cada um e comparar resultados\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Teste suas implementa√ß√µes\n",
        "pergunta_teste = \"Como est√° o mercado de IA no Brasil?\"\n",
        "# comparar_estrategias_retrieval(pergunta_teste)\n",
        "\n",
        "print(\"üöß Implemente os diferentes retrievers no c√≥digo acima!\")\n",
        "print(\"üìä Compare os resultados e veja qual funciona melhor para cada tipo de pergunta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Conectando com os Pr√≥ximos M√≥dulos\n\nLind√£o! Voc√™s acabaram de dominar RAG! üéâ Mas a jornada n√£o para aqui...\n\n### üîÆ **O que vem pela frente:**\n\n#### **M√≥dulo 9 - Agents e Tools** ü§ñ\n- Vamos dar \"superpoderes\" para o RAG\n- Agents que decidem QUANDO usar RAG\n- Tools que permitem buscar na internet, fazer c√°lculos, etc.\n- RAG + Agents = ü§Ø\n\n#### **M√≥dulos 10-11 - Projetos Finais** üöÄ\n- Sistema RAG completo para empresa real\n- RAG + Streamlit = aplica√ß√£o web\n- Deploy na AWS (que vai ser lindo!)\n\n### üß† **Conceitos que vamos expandir:**\n\n```mermaid\ngraph TD\n    A[RAG Atual] --> B[RAG + Agents]\n    B --> C[RAG + Tools]\n    C --> D[RAG + Web Interface]\n    D --> E[RAG em Produ√ß√£o]\n    E --> F[RAG + Monitoring]\n```\n\n**Dica do Pedro**: RAG √© a base! Tudo que vem depois vai usar os conceitos que voc√™s aprenderam aqui. √â como aprender a dirigir - depois voc√™s v√£o dirigir carros mais avan√ßados, mas o b√°sico √© sempre o mesmo! üöó‚û°Ô∏èüèéÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o: Evolu√ß√£o do RAG ao longo do curso\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dados da evolu√ß√£o\n",
        "modulos = ['M6\\nDocs', 'M7\\nVectors', 'M8\\nRAG', 'M9\\nAgents', 'M10-11\\nProjetos', 'M12\\nDeploy']\n",
        "complexidade = [2, 4, 6, 8, 9, 10]\n",
        "utilidade = [1, 3, 7, 9, 10, 10]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plotar as curvas\n",
        "x = np.arange(len(modulos))\n",
        "ax.plot(x, complexidade, 'o-', linewidth=3, markersize=8, label='üß† Complexidade', color='#ff6b6b')\n",
        "ax.plot(x, utilidade, 'o-', linewidth=3, markersize=8, label='‚ö° Utilidade Pr√°tica', color='#4ecdc4')\n",
        "\n",
        "# Destacar o m√≥dulo atual\n",
        "ax.axvline(x=2, color='gold', linestyle='--', alpha=0.7, linewidth=2)\n",
        "ax.text(2, 8.5, '‚Üê VOC√ä EST√Å AQUI!', fontsize=12, fontweight='bold', \n",
        "        ha='center', bbox=dict(boxstyle='round,pad=0.3', facecolor='gold', alpha=0.7))\n",
        "\n",
        "# Customizar o gr√°fico\n",
        "ax.set_xlabel('M√≥dulos do Curso', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('N√≠vel (1-10)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('üöÄ Evolu√ß√£o do Conhecimento em RAG/LangChain', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(modulos)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(0, 11)\n",
        "\n",
        "# Adicionar anota√ß√µes\n",
        "ax.annotate('Fundamentos', xy=(1, 3), xytext=(0.5, 5),\n",
        "            arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "ax.annotate('RAG Completo!', xy=(2, 7), xytext=(2, 5),\n",
        "            arrowprops=dict(arrowstyle='->', color='green', alpha=0.7),\n",
        "            fontsize=10, ha='center', color='green', fontweight='bold')\n",
        "\n",
        "ax.annotate('Produ√ß√£o', xy=(5, 10), xytext=(4.5, 8.5),\n",
        "            arrowprops=dict(arrowstyle='->', color='blue', alpha=0.7),\n",
        "            fontsize=10, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìà An√°lise da Jornada:\")\n",
        "print(f\"   ‚Ä¢ Complexidade atual: {complexidade[2]}/10 - Moderada\")\n",
        "print(f\"   ‚Ä¢ Utilidade atual: {utilidade[2]}/10 - Alta!\")\n",
        "print(\"   ‚Ä¢ Pr√≥ximos passos: Agents (complexidade +2, utilidade +2)\")\n",
        "print(\"   ‚Ä¢ Objetivo: Produ√ß√£o (complexidade +4, utilidade +3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Resumo do M√≥dulo: O que Aprendemos?\n\nCaramba, que m√≥dulo intenso! ü§Ø Vamos recapitular tudo que rolou:\n\n### ‚úÖ **Conceitos Dominados:**\n\n1. **RAG Architecture** üèóÔ∏è\n   - Retrieval-Augmented Generation\n   - Fase de indexa√ß√£o vs. fase de consulta\n   - Como juntar retrieval + generation\n\n2. **Implementa√ß√£o Pr√°tica** üíª\n   - `create_retrieval_chain()` e `create_stuff_documents_chain()`\n   - Integration com componentes dos m√≥dulos anteriores\n   - Prompt engineering para RAG\n\n3. **T√©cnicas Avan√ßadas** üöÄ\n   - Filtros por metadata\n   - Diferentes estrat√©gias de retrieval\n   - Context enrichment\n   - Performance monitoring\n\n### üìä **F√≥rmulas e Conceitos T√©cnicos:**\n\n$$RAG(query) = LLM(prompt + context + query)$$\n$$context = Retriever(embed(query), VectorStore)$$\n$$similarity(a,b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}$$\n\n### üéØ **Aplica√ß√µes Reais:**\n- Chatbots corporativos\n- Sistemas de suporte t√©cnico\n- Assistentes de pesquisa\n- Q&A sobre documenta√ß√£o\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-08_img_05.png)\n\n**Dica do Pedro Final**: RAG mudou o jogo da IA! Voc√™s agora sabem como fazer LLMs conversarem com qualquer base de dados. Isso vale ouro no mercado! üí∞‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parab√©ns! Vamos celebrar com algumas estat√≠sticas finais\n",
        "import datetime\n",
        "\n",
        "print(\"üéâ PARAB√âNS! Voc√™ completou o M√≥dulo 8 - RAG Implementation!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Estat√≠sticas do que foi constru√≠do\n",
        "stats = {\n",
        "    \"Documentos processados\": len(documents),\n",
        "    \"Chunks criados\": len(chunks),\n",
        "    \"Embeddings gerados\": vector_store.index.ntotal if 'vector_store' in locals() else 0,\n",
        "    \"Dimens√£o dos vetores\": vector_store.index.d if 'vector_store' in locals() else 0,\n",
        "    \"Consultas RAG testadas\": len(resultados) if 'resultados' in locals() else 0,\n",
        "    \"Chains criadas\": 2,  # document_chain + rag_chain\n",
        "    \"Componentes integrados\": 4  # LLM, Embeddings, VectorStore, Retriever\n",
        "}\n",
        "\n",
        "print(\"üìä ESTAT√çSTICAS DA SUA IMPLEMENTA√á√ÉO RAG:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
        "print(\"   ‚Ä¢ M√≥dulo 9: Agents e Tools - RAG com superpoderes!\")\n",
        "print(\"   ‚Ä¢ M√≥dulo 10-11: Projetos finais - RAG em aplica√ß√µes reais\")\n",
        "print(\"   ‚Ä¢ M√≥dulo 12: Deploy - RAG em produ√ß√£o com Streamlit\")\n",
        "\n",
        "print(\"\\nüí° DICAS PARA CONTINUAR ESTUDANDO:\")\n",
        "print(\"   ‚Ä¢ Teste com seus pr√≥prios documentos\")\n",
        "print(\"   ‚Ä¢ Experimente diferentes embeddings\")\n",
        "print(\"   ‚Ä¢ Implemente os exerc√≠cios propostos\")\n",
        "print(\"   ‚Ä¢ Leia sobre RAG avan√ßado (GraphRAG, MultiVector RAG, etc.)\")\n",
        "\n",
        "print(f\"\\nüìÖ Conclu√≠do em: {datetime.datetime.now().strftime('%d/%m/%Y √†s %H:%M')}\")\n",
        "print(\"\\nüèÜ VOC√ä AGORA √â UM RAG MASTER! Liiindo! üáßüá∑üöÄ\")"
      ]
    }
  ]
}