{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ•°ï¸ MÃ¡quina do Tempo LangChain: Revisitando a Era v1.0 vs v0.2 \n\n**Pedro Nunes Guth - Expert em IA & AWS**\n\n---\n\nFala pessoal! Bora fazer uma viagem no tempo? ğŸš—ğŸ’¨\n\nImaginem que vocÃªs sÃ£o como aqueles colecionadores de carros antigos que adoram comparar um Fusca 1970 com um Civic 2024. Ambos fazem o mesmo trabalho (te levar de A para B), mas a experiÃªncia Ã© TOTALMENTE diferente!\n\nHoje vamos revisitar TUDO que aprendemos no curso, mas na versÃ£o LangChain v1.0, comparando com nossa querida v0.2. Ã‰ como comparar o WhatsApp de 2015 com o de hoje - a essÃªncia Ã© a mesma, mas a implementaÃ§Ã£o... nossa! ğŸ˜±\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ O que Vamos Aprender Hoje?\n\nTÃ¡, mas o que exatamente vamos ver aqui?\n\n**Na v1.0 (a vovÃ³ do LangChain):**\n- Como as coisas eram mais \"na mÃ£o\"\n- Sintaxe mais verbosa\n- Menos abstraÃ§Ãµes mÃ¡gicas\n- Mais controle manual\n\n**Na v0.2 (nossa versÃ£o turbinada):**\n- LCEL e Runnables que facilitam a vida\n- Sintaxe mais limpa e pythÃ´nica\n- AbstraÃ§Ãµes inteligentes\n- Menos cÃ³digo para mais resultado\n\nÃ‰ como comparar dirigir um carro sem direÃ§Ã£o hidrÃ¡ulica (v1.0) com um carro moderno com tudo automÃ¡tico (v0.2). Os dois chegam no destino, mas a jornada Ã© BEM diferente! ğŸš—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos instalar as duas versÃµes para comparaÃ§Ã£o\n",
        "# ATENÃ‡ÃƒO: Em produÃ§Ã£o, nunca misturem versÃµes assim!\n",
        "# Isso Ã© sÃ³ para fins didÃ¡ticos\n",
        "\n",
        "!pip install langchain==0.1.0 -q  # Representando a era v1.0\n",
        "!pip install langchain-google-genai -q\n",
        "!pip install matplotlib seaborn plotly -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install streamlit -q\n",
        "\n",
        "print(\"ğŸ‰ Pacotes instalados! Bora comeÃ§ar nossa viagem no tempo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports bÃ¡sicos para nossa comparaÃ§Ã£o\n",
        "import os\n",
        "from getpass import getpass\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ConfiguraÃ§Ã£o da API Key\n",
        "if 'GOOGLE_API_KEY' not in os.environ:\n",
        "    os.environ['GOOGLE_API_KEY'] = getpass(\"ğŸ”‘ Digite sua Google API Key: \")\n",
        "\n",
        "print(\"âœ… Setup bÃ¡sico finalizado!\")\n",
        "print(f\"ğŸ“… Iniciando comparaÃ§Ã£o em: {datetime.now().strftime('%d/%m/%Y %H:%M')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– CapÃ­tulo 1: ChatModels - A EvoluÃ§Ã£o dos Bots\n\nLembram quando vocÃªs eram pequenos e pediam algo pros pais? Na v1.0 era tipo assim:\n\n**CrianÃ§a (v1.0):** \"Por favor, pai, o senhor poderia, se possÃ­vel, considerando as circunstÃ¢ncias, me dar um sorvete?\"\n\n**CrianÃ§a (v0.2):** \"Pai, sorvete! ğŸ¦\"\n\nAmbas funcionam, mas uma Ã© MUITO mais direta! ğŸ˜„\n\n### DiferenÃ§as Principais:\n\n| Aspecto | v1.0 | v0.2 |\n|---------|------|------|\n| **InicializaÃ§Ã£o** | Mais verbosa | LCEL simplificado |\n| **Chamadas** | `.call()` ou `.generate()` | `.invoke()` padronizado |\n| **Streaming** | Complexo | Super simples |\n| **ComposiÃ§Ã£o** | Manual | Operador `\\|` |\n\n**ğŸ’¡ Dica do Pedro:** A v0.2 trouxe o conceito de \"Runnable\" - tudo que implementa `.invoke()`, `.stream()`, `.batch()`. Ã‰ como ter um padrÃ£o universal de tomada elÃ©trica!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: ChatModels v1.0 vs v0.2\n",
        "\n",
        "print(\"ğŸ”„ VERSÃƒO v1.0 (O jeito raiz):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Simulando a sintaxe da v1.0\n",
        "v1_code = '''\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# InicializaÃ§Ã£o mais verbosa\n",
        "llm = GooglePalm(\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    temperature=0.7,\n",
        "    model_name=\"models/text-bison-001\"\n",
        ")\n",
        "\n",
        "# Chamada mais manual\n",
        "messages = [\n",
        "    SystemMessage(content=\"VocÃª Ã© um assistente Ãºtil\"),\n",
        "    HumanMessage(content=\"Explique IA em uma frase\")\n",
        "]\n",
        "\n",
        "response = llm.generate([messages])\n",
        "result = response.generations[0][0].text\n",
        "'''\n",
        "\n",
        "print(v1_code)\n",
        "print(\"\\nğŸš€ VERSÃƒO v0.2 (O jeito moderno):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ImplementaÃ§Ã£o real v0.2\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# InicializaÃ§Ã£o mais limpa\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Chamada super simples com LCEL\n",
        "messages = [\n",
        "    SystemMessage(content=\"VocÃª Ã© um assistente Ãºtil\"),\n",
        "    HumanMessage(content=\"Explique IA em uma frase\")\n",
        "]\n",
        "\n",
        "# O invoke Ã© padrÃ£o em TUDO na v0.2\n",
        "response = chat.invoke(messages)\n",
        "print(f\"ğŸ“ Resposta: {response.content}\")\n",
        "\n",
        "print(\"\\nâœ¨ Liiiindo! Bem mais clean, nÃ©?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ CapÃ­tulo 2: Prompt Templates - A Arte de Conversar com IAs\n\nPrompt Template Ã© como receita de bolo da vovÃ³ vs. receita do YouTube:\n\n**VovÃ³ (v1.0):** \"Pegue uma xÃ­cara daquela farinha de trigo que estÃ¡ no armÃ¡rio da cozinha, trÃªs ovos frescos do galinheiro...\"\n\n**YouTube (v0.2):** \"1 xÃ­cara farinha, 3 ovos, misture, asse 30min. PRONTO! ğŸ‘Œ\"\n\n### EvoluÃ§Ã£o dos Templates:\n\nA v1.0 tinha vÃ¡rias classes diferentes para cada tipo de template. A v0.2 unificou tudo numa interface mais consistente.\n\n$$\\text{Template v1.0} = \\text{Complexidade} + \\text{Verbosidade}$$\n$$\\text{Template v0.2} = \\text{Simplicidade} + \\text{Flexibilidade}$$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Prompt Templates atravÃ©s das versÃµes\n",
        "\n",
        "print(\"ğŸ“œ PROMPT TEMPLATES: EvoluÃ§Ã£o na PrÃ¡tica\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulando v1.0 (mais verboso)\n",
        "print(\"\\nğŸ”„ Estilo v1.0:\")\n",
        "v1_prompt_code = '''\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# Mais verboso e separado\n",
        "system_template = \"VocÃª Ã© um {role} especializado em {area}\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{question}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "'''\n",
        "print(v1_prompt_code)\n",
        "\n",
        "print(\"\\nğŸš€ Estilo v0.2 (nossa versÃ£o atual):\")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Muito mais direto e limpo!\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"VocÃª Ã© um {role} especializado em {area}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Testando o template\n",
        "formatted = prompt.format_messages(\n",
        "    role=\"professor\",\n",
        "    area=\"inteligÃªncia artificial\",\n",
        "    question=\"Como funciona machine learning?\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Template formatado:\")\n",
        "for msg in formatted:\n",
        "    print(f\"   {msg.type}: {msg.content}\")\n",
        "\n",
        "print(\"\\nğŸ’­ Viu a diferenÃ§a? Mesma funcionalidade, muito menos cÃ³digo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## â›“ï¸ CapÃ­tulo 3: Chains - A RevoluÃ§Ã£o LCEL\n\nAqui Ã© onde a coisa fica INTERESSANTE! ğŸ”¥\n\nChains na v1.0 eram como aqueles eletrodomÃ©sticos antigos - funcionavam, mas vocÃª precisava apertar 47 botÃµes na sequÃªncia certa. Na v0.2, com LCEL (LangChain Expression Language), Ã© como ter um assistente de voz: \"Alexa, faz tudo isso aÃ­ pra mim!\"\n\n### LCEL: A Magia da v0.2\n\nO operador `|` (pipe) Ã© como um cano d'Ã¡gua conectando processos:\n\n```\nPrompt | Model | Parser\n   ğŸš°      ğŸ­      ğŸ“¦\n```\n\n**ğŸ’¡ Dica do Pedro:** LCEL nÃ£o Ã© sÃ³ sintaxe bonitinha - ele trouxe streaming automÃ¡tico, paralelizaÃ§Ã£o e error handling de graÃ§a!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Chains - A Grande RevoluÃ§Ã£o!\n",
        "\n",
        "print(\"â›“ï¸ CHAINS: Antes vs Depois da RevoluÃ§Ã£o LCEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simulando v1.0 - Modo raiz\n",
        "print(\"\\nğŸ”„ v1.0 - O jeito trabalhoso:\")\n",
        "v1_chain_code = '''\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "# Cada peÃ§a separada, como montar um mÃ³vel do IKEA\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"produto\"],\n",
        "    template=\"Crie um slogan criativo para: {produto}\"\n",
        ")\n",
        "\n",
        "llm = GooglePalm()\n",
        "\n",
        "# Montando a chain manualmente\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Executando\n",
        "result = chain.run(produto=\"pizza de brigadeiro\")\n",
        "'''\n",
        "print(v1_chain_code)\n",
        "\n",
        "print(\"\\nğŸš€ v0.2 - O poder do LCEL:\")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LCEL em aÃ§Ã£o - Uma linha faz tudo!\n",
        "chain = (\n",
        "    ChatPromptTemplate.from_template(\"Crie um slogan criativo para: {produto}\")\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando nossa chain moderna\n",
        "resultado = chain.invoke({\"produto\": \"pizza de brigadeiro\"})\n",
        "print(f\"âœ¨ Slogan gerado: {resultado}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Resultado: Mesma funcionalidade, 80% menos cÃ³digo!\")\n",
        "print(\"   Isso Ã© o que eu chamo de evoluÃ§Ã£o! ğŸš€\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a diferenÃ§a de complexidade\n",
        "\n",
        "# Dados para o grÃ¡fico\n",
        "versoes = ['v1.0', 'v0.2']\n",
        "linhas_codigo = [25, 8]  # AproximaÃ§Ã£o baseada nos exemplos\n",
        "facilidade_uso = [3, 9]  # Escala de 1-10\n",
        "funcionalidades = [7, 10]  # Recursos disponÃ­veis\n",
        "\n",
        "# Criando o grÃ¡fico comparativo\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Linhas de cÃ³digo\n",
        "ax1.bar(versoes, linhas_codigo, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax1.set_title('ğŸ“Š Linhas de CÃ³digo\\n(Menos Ã© melhor)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('NÃºmero de linhas')\n",
        "for i, v in enumerate(linhas_codigo):\n",
        "    ax1.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Facilidade de uso\n",
        "ax2.bar(versoes, facilidade_uso, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax2.set_title('ğŸ˜Š Facilidade de Uso\\n(1-10, mais Ã© melhor)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Score de facilidade')\n",
        "ax2.set_ylim(0, 10)\n",
        "for i, v in enumerate(facilidade_uso):\n",
        "    ax2.text(i, v + 0.2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Funcionalidades\n",
        "ax3.bar(versoes, funcionalidades, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax3.set_title('ğŸš€ Funcionalidades\\n(1-10, mais Ã© melhor)', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Score de recursos')\n",
        "ax3.set_ylim(0, 10)\n",
        "for i, v in enumerate(funcionalidades):\n",
        "    ax3.text(i, v + 0.2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('ğŸ”¥ LangChain: A EvoluÃ§Ã£o em NÃºmeros', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“ˆ AnÃ¡lise dos dados:\")\n",
        "print(f\"   â€¢ ReduÃ§Ã£o de cÃ³digo: {((linhas_codigo[0] - linhas_codigo[1]) / linhas_codigo[0] * 100):.0f}%\")\n",
        "print(f\"   â€¢ Aumento facilidade: {((facilidade_uso[1] - facilidade_uso[0]) / facilidade_uso[0] * 100):.0f}%\")\n",
        "print(f\"   â€¢ Mais funcionalidades: {((funcionalidades[1] - funcionalidades[0]) / funcionalidades[0] * 100):.0f}%\")\n",
        "print(\"\\nğŸ’¡ ConclusÃ£o: A v0.2 Ã© simplesmente SUPERIOR! ğŸ†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  CapÃ­tulo 4: Memory Systems - LembranÃ§a vs Esquecimento\n\nMemory na v1.0 era como aqueles cadernos antigos da escola - funcionava, mas vocÃª tinha que gerenciar tudo manualmente. Perdeu o caderno? Era GG! ğŸ˜…\n\nNa v0.2, o sistema de memÃ³ria Ã© como Google Photos - organiza tudo automaticamente e vocÃª sempre acha o que precisa!\n\n### Tipos de MemÃ³ria Evolution:\n\n```mermaid\ngraph TD\n    A[Memory v1.0] --> B[ConversationBufferMemory]\n    A --> C[ConversationSummaryMemory]\n    A --> D[ConversationBufferWindowMemory]\n    \n    E[Memory v0.2] --> F[ChatMessageHistory]\n    E --> G[RunnableWithMessageHistory]\n    E --> H[Auto-managed State]\n    \n    style A fill:#ff6b6b\n    style E fill:#4ecdc4\n```\n\n**ğŸ’¡ Dica do Pedro:** A v0.2 trouxe o conceito de \"state management\" automÃ¡tico. Ã‰ como ter um assistente pessoal cuidando das suas conversas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Memory Systems\n",
        "\n",
        "print(\"ğŸ§  MEMORY SYSTEMS: A EvoluÃ§Ã£o da LembranÃ§a\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ”„ v1.0 - Gerenciamento manual:\")\n",
        "v1_memory_code = '''\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Criando memÃ³ria manualmente\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Adicionando Ã  chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Cada interaÃ§Ã£o precisa ser gerenciada\n",
        "response1 = conversation.predict(input=\"Meu nome Ã© JoÃ£o\")\n",
        "response2 = conversation.predict(input=\"Qual Ã© meu nome?\")\n",
        "'''\n",
        "print(v1_memory_code)\n",
        "\n",
        "print(\"\\nğŸš€ v0.2 - Gerenciamento automÃ¡tico:\")\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Sistema de memÃ³ria mais elegante\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# Criando chain com memÃ³ria automÃ¡tica\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"VocÃª Ã© um assistente que lembra de conversas anteriores.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt_with_history | chat\n",
        "\n",
        "# Envolvendo com memÃ³ria automÃ¡tica\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "# Testando memÃ³ria\n",
        "config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Meu nome Ã© Pedro e eu adoro pizza\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"ğŸ¤– Resposta 1: {response1.content[:100]}...\")\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"Qual Ã© meu nome e o que eu gosto de comer?\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"ğŸ¤– Resposta 2: {response2.content[:100]}...\")\n",
        "\n",
        "print(\"\\nâœ¨ Viu sÃ³? Ela lembrou automaticamente! Magia da v0.2! ğŸ©âœ¨\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š CapÃ­tulo 5: Document Loading & RAG - A RevoluÃ§Ã£o dos Dados\n\nDocument Loading na v1.0 era como organizar uma biblioteca sem sistema - vocÃª sabia que o livro estava lÃ¡, mas encontrar era outra histÃ³ria! ğŸ“–\n\nNa v0.2, Ã© como ter uma bibliotecÃ¡ria super eficiente que nÃ£o sÃ³ organiza tudo, mas tambÃ©m te entrega o livro certo na mesa!\n\n### RAG Evolution:\n\n$$RAG_{v1.0} = \\text{Load} + \\text{Split} + \\text{Embed} + \\text{Store} + \\text{Retrieve} + \\text{Generate}$$\n\n$$RAG_{v0.2} = \\text{Load} \\mid \\text{Split} \\mid \\text{Embed} \\mid \\text{Store} \\mid \\text{Retrieve} \\mid \\text{Generate}$$\n\nMesmos passos, mas com o poder do pipe (`|`) conectando tudo!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Document Loading e RAG\n",
        "\n",
        "print(\"ğŸ“š DOCUMENT LOADING & RAG: Antes vs Depois\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nğŸ”„ v1.0 - Processo manual e verboso:\")\n",
        "v1_rag_code = '''\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Cada passo era manual e separado\n",
        "loader = TextLoader(\"documento.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = GooglePalmEmbeddings()\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "result = qa_chain.run(\"Qual Ã© o tema principal?\")\n",
        "'''\n",
        "print(v1_rag_code)\n",
        "\n",
        "print(\"\\nğŸš€ v0.2 - Pipeline elegante:\")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Simulando um documento para demonstraÃ§Ã£o\n",
        "texto_demo = '''\n",
        "LangChain Ã© uma framework incrÃ­vel para desenvolvimento de aplicaÃ§Ãµes com LLMs.\n",
        "A versÃ£o 0.2 trouxe muitas melhorias, especialmente o LCEL.\n",
        "LCEL permite criar pipelines de forma mais intuitiva e eficiente.\n",
        "RAG (Retrieval Augmented Generation) ficou muito mais simples de implementar.\n",
        "'''\n",
        "\n",
        "# Salvando arquivo temporÃ¡rio\n",
        "with open('demo.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(texto_demo)\n",
        "\n",
        "# Pipeline RAG moderno e elegante\n",
        "loader = TextLoader('demo.txt', encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# O poder do LCEL em aÃ§Ã£o! ğŸš€\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Baseado no contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando nosso RAG moderno\n",
        "pergunta = \"O que Ã© LCEL e por que Ã© importante?\"\n",
        "resposta = rag_chain.invoke(pergunta)\n",
        "\n",
        "print(f\"â“ Pergunta: {pergunta}\")\n",
        "print(f\"ğŸ¤– Resposta RAG: {resposta}\")\n",
        "\n",
        "print(\"\\nâœ¨ Pipeline RAG em apenas algumas linhas! Isso Ã© evoluÃ§Ã£o! ğŸš€\")\n",
        "\n",
        "# Limpeza\n",
        "import os\n",
        "os.remove('demo.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VisualizaÃ§Ã£o da arquitetura RAG: v1.0 vs v0.2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Dados para visualizaÃ§Ã£o\n",
        "steps_v1 = ['Load', 'Split', 'Embed', 'Store', 'Retrieve', 'Generate']\n",
        "complexity_v1 = [3, 4, 5, 6, 7, 8]  # Crescente em complexidade\n",
        "\n",
        "steps_v2 = ['Load', 'Split', 'Embed', 'Store', 'Retrieve', 'Generate']\n",
        "complexity_v2 = [2, 2, 3, 3, 2, 2]  # Mais uniforme\n",
        "\n",
        "# GrÃ¡fico v1.0\n",
        "bars1 = ax1.bar(range(len(steps_v1)), complexity_v1, \n",
        "                color=['#ff6b6b', '#ff8e53', '#ff6b6b', '#ff8e53', '#ff6b6b', '#ff8e53'])\n",
        "ax1.set_title('ğŸ”„ RAG v1.0\\n(Complexidade crescente)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('NÃ­vel de complexidade')\n",
        "ax1.set_xticks(range(len(steps_v1)))\n",
        "ax1.set_xticklabels(steps_v1, rotation=45)\n",
        "ax1.set_ylim(0, 10)\n",
        "\n",
        "# GrÃ¡fico v0.2\n",
        "bars2 = ax2.bar(range(len(steps_v2)), complexity_v2, \n",
        "                color=['#4ecdc4', '#45b7aa', '#4ecdc4', '#45b7aa', '#4ecdc4', '#45b7aa'])\n",
        "ax2.set_title('ğŸš€ RAG v0.2\\n(Complexidade uniforme)', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('NÃ­vel de complexidade')\n",
        "ax2.set_xticks(range(len(steps_v2)))\n",
        "ax2.set_xticklabels(steps_v2, rotation=45)\n",
        "ax2.set_ylim(0, 10)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar, value in zip(bars1, complexity_v1):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             str(value), ha='center', fontweight='bold')\n",
        "\n",
        "for bar, value in zip(bars2, complexity_v2):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             str(value), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('ğŸ“Š RAG Architecture: EvoluÃ§Ã£o da Complexidade', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“ˆ AnÃ¡lise da evoluÃ§Ã£o RAG:\")\n",
        "print(f\"   â€¢ Complexidade mÃ©dia v1.0: {np.mean(complexity_v1):.1f}/10\")\n",
        "print(f\"   â€¢ Complexidade mÃ©dia v0.2: {np.mean(complexity_v2):.1f}/10\")\n",
        "print(f\"   â€¢ ReduÃ§Ã£o de complexidade: {((np.mean(complexity_v1) - np.mean(complexity_v2)) / np.mean(complexity_v1) * 100):.0f}%\")\n",
        "print(\"\\nğŸ’¡ A v0.2 democratizou o RAG! Agora qualquer dev consegue implementar! ğŸ‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– CapÃ­tulo 6: Agents & Tools - De RobÃ´s Burros para IAs Espertas\n\nAgents na v1.0 eram como aqueles robÃ´s de filme antigo - meio desajeitados, faziam o que vocÃª mandava, mas nÃ£o eram muito espertos. ğŸ¤–\n\nNa v0.2, viraram como Jarvis do Homem de Ferro - entendem contexto, tomam decisÃµes inteligentes e ainda fazem piada! ğŸ˜\n\n### Agent Evolution:\n\n```mermaid\ngraph LR\n    A[v1.0 Agent] --> B[Fixed Tools]\n    A --> C[Rigid Logic]\n    A --> D[Manual Config]\n    \n    E[v0.2 Agent] --> F[Dynamic Tools]\n    E --> G[Smart Reasoning]\n    E --> H[Auto Config]\n    \n    style A fill:#ff6b6b\n    style E fill:#4ecdc4\n```\n\n**ğŸ’¡ Dica do Pedro:** A v0.2 trouxe agents que realmente \"pensam\" antes de agir. Ã‰ como ter um estagiÃ¡rio que evoluiu para senior developer! ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Agents & Tools\n",
        "\n",
        "print(\"ğŸ¤– AGENTS & TOOLS: Da Rigidez Ã  InteligÃªncia\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(\"\\nğŸ”„ v1.0 - Agents rÃ­gidos e verbosos:\")\n",
        "v1_agent_code = '''\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# Definindo tools manualmente\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"Ãºtil para buscar informaÃ§Ãµes atuais\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Inicializando agent de forma verbosa\n",
        "agent = initialize_agent(\n",
        "    tools, \n",
        "    llm, \n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "result = agent.run(\"Qual Ã© a capital do Brasil?\")\n",
        "'''\n",
        "print(v1_agent_code)\n",
        "\n",
        "print(\"\\nğŸš€ v0.2 - Agents inteligentes e flexÃ­veis:\")\n",
        "\n",
        "# Vamos criar tools personalizados para demonstraÃ§Ã£o\n",
        "from langchain_core.tools import tool\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def calculadora(expression: str) -> str:\n",
        "    \"\"\"Calcula expressÃµes matemÃ¡ticas simples. Use para operaÃ§Ãµes como: 2+2, 10*5, etc.\"\"\"\n",
        "    try:\n",
        "        # SeguranÃ§a bÃ¡sica - sÃ³ permite operaÃ§Ãµes simples\n",
        "        allowed_chars = set('0123456789+-*/(). ')\n",
        "        if not set(expression).issubset(allowed_chars):\n",
        "            return \"Erro: expressÃ£o contÃ©m caracteres nÃ£o permitidos\"\n",
        "        \n",
        "        result = eval(expression)\n",
        "        return f\"O resultado de {expression} Ã© {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Erro no cÃ¡lculo: {e}\"\n",
        "\n",
        "@tool\n",
        "def gerador_piada() -> str:\n",
        "    \"\"\"Gera uma piada aleatÃ³ria para animar o usuÃ¡rio.\"\"\"\n",
        "    piadas = [\n",
        "        \"Por que os pÃ¡ssaros voam para o sul? Porque Ã© longe demais para ir andando! ğŸ¦\",\n",
        "        \"Por que o livro de matemÃ¡tica estava triste? Porque tinha muitos problemas! ğŸ“š\",\n",
        "        \"O que o pato disse para a pata? Vem quÃ¡! ğŸ¦†\",\n",
        "        \"Por que o computador foi ao mÃ©dico? Porque estava com vÃ­rus! ğŸ’»\"\n",
        "    ]\n",
        "    return random.choice(piadas)\n",
        "\n",
        "@tool\n",
        "def info_tempo() -> str:\n",
        "    \"\"\"Retorna informaÃ§Ãµes sobre data e hora atual.\"\"\"\n",
        "    agora = datetime.now()\n",
        "    return f\"Hoje Ã© {agora.strftime('%d/%m/%Y')} e sÃ£o {agora.strftime('%H:%M')}h\"\n",
        "\n",
        "# Na v0.2, criar um agent Ã© muito mais simples!\n",
        "tools = [calculadora, gerador_piada, info_tempo]\n",
        "\n",
        "# Prompt para o agent\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"VocÃª Ã© um assistente Ãºtil que pode usar tools para ajudar. Use as tools quando necessÃ¡rio.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])\n",
        "\n",
        "print(\"ğŸ› ï¸ Tools disponÃ­veis:\")\n",
        "for tool in tools:\n",
        "    print(f\"   â€¢ {tool.name}: {tool.description}\")\n",
        "\n",
        "# Simulando interaÃ§Ã£o com agent\n",
        "print(\"\\nğŸ¤– Testando nosso agent moderno:\")\n",
        "print(f\"   UsuÃ¡rio: 'Quanto Ã© 15 * 8?'\")\n",
        "resultado_calc = calculadora.invoke(\"15 * 8\")\n",
        "print(f\"   Agent: {resultado_calc}\")\n",
        "\n",
        "print(f\"\\n   UsuÃ¡rio: 'Me conta uma piada!'\")\n",
        "piada = gerador_piada.invoke({})\n",
        "print(f\"   Agent: {piada}\")\n",
        "\n",
        "print(f\"\\n   UsuÃ¡rio: 'Que horas sÃ£o?'\")\n",
        "tempo = info_tempo.invoke({})\n",
        "print(f\"   Agent: {tempo}\")\n",
        "\n",
        "print(\"\\nâœ¨ Viu a diferenÃ§a? Agent moderno Ã© mais interativo e inteligente! ğŸ§ \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ EXERCÃCIO PRÃTICO 1: MigraÃ§Ã£o de CÃ³digo\n\n**Bora colocar a mÃ£o na massa!** ğŸ’ª\n\nSeu desafio Ã© pegar este cÃ³digo da v1.0 e \"modernizar\" para v0.2 usando LCEL:\n\n```python\n# CÃ³digo v1.0 para modernizar\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import GooglePalm\n\nprompt = PromptTemplate(\n    input_variables=[\"tema\", \"publico\"],\n    template=\"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n)\n\nllm = GooglePalm()\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(tema=\"IA\", publico=\"desenvolvedores\")\n```\n\n**Sua missÃ£o:**\n1. Converter para sintaxe v0.2\n2. Usar LCEL (operador `|`)\n3. Adicionar um output parser\n4. Testar com diferentes inputs\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ EXERCÃCIO 1: Sua soluÃ§Ã£o aqui!\n",
        "\n",
        "print(\"ğŸ’» EXERCÃCIO 1: MigraÃ§Ã£o v1.0 â†’ v0.2\")\n",
        "print(\"=\"*45)\n",
        "print(\"\\nğŸ“ Sua missÃ£o: Modernizar o cÃ³digo abaixo!\")\n",
        "print(\"\\n--- CÃ“DIGO v1.0 PARA MODERNIZAR ---\")\n",
        "print('''\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"tema\", \"publico\"],\n",
        "    template=\"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n",
        ")\n",
        "\n",
        "llm = GooglePalm()\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(tema=\"IA\", publico=\"desenvolvedores\")\n",
        "''')\n",
        "\n",
        "print(\"\\n--- SUA SOLUÃ‡ÃƒO v0.2 (complete o cÃ³digo abaixo) ---\")\n",
        "\n",
        "# DICA: Use ChatPromptTemplate, chat (jÃ¡ definido), StrOutputParser e LCEL!\n",
        "# TODO: Implemente sua soluÃ§Ã£o aqui\n",
        "\n",
        "# Imports necessÃ¡rios (jÃ¡ importados anteriormente, mas listando para clareza)\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# SoluÃ§Ã£o esperada (descomente e complete):\n",
        "'''\n",
        "# Criando prompt moderno\n",
        "prompt_moderno = ChatPromptTemplate.from_template(\n",
        "    \"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n",
        ")\n",
        "\n",
        "# Chain moderna com LCEL\n",
        "chain_moderna = (\n",
        "    prompt_moderno\n",
        "    | chat  # JÃ¡ definido anteriormente\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando\n",
        "resultado = chain_moderna.invoke({\n",
        "    \"tema\": \"IA\", \n",
        "    \"publico\": \"desenvolvedores\"\n",
        "})\n",
        "'''\n",
        "\n",
        "print(\"\\nğŸ¤” Pense: Quantas linhas de cÃ³digo vocÃª economizou?\")\n",
        "print(\"ğŸ’¡ Dica: A resposta estÃ¡ na prÃ³xima cÃ©lula!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ SOLUÃ‡ÃƒO DO EXERCÃCIO 1\n",
        "\n",
        "print(\"âœ… SOLUÃ‡ÃƒO COMPLETA DO EXERCÃCIO 1\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# ImplementaÃ§Ã£o moderna v0.2\n",
        "prompt_moderno = ChatPromptTemplate.from_template(\n",
        "    \"Crie um post para {publico} sobre {tema}. Seja criativo e envolvente!\"\n",
        ")\n",
        "\n",
        "# Chain moderna com LCEL - MUITO mais limpa!\n",
        "chain_moderna = (\n",
        "    prompt_moderno\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando com mÃºltiplos casos\n",
        "casos_teste = [\n",
        "    {\"tema\": \"IA\", \"publico\": \"desenvolvedores\"},\n",
        "    {\"tema\": \"Python\", \"publico\": \"iniciantes\"},\n",
        "    {\"tema\": \"LangChain\", \"publico\": \"data scientists\"}\n",
        "]\n",
        "\n",
        "print(\"ğŸš€ Testando nossa chain modernizada:\")\n",
        "print()\n",
        "\n",
        "for i, caso in enumerate(casos_teste, 1):\n",
        "    resultado = chain_moderna.invoke(caso)\n",
        "    print(f\"ğŸ“± Teste {i} - {caso['tema']} para {caso['publico']}:\")\n",
        "    print(f\"   {resultado[:150]}...\")\n",
        "    print()\n",
        "\n",
        "# AnÃ¡lise da melhoria\n",
        "print(\"ğŸ“Š ANÃLISE DA MODERNIZAÃ‡ÃƒO:\")\n",
        "print(f\"   â€¢ CÃ³digo v1.0: ~12 linhas\")\n",
        "print(f\"   â€¢ CÃ³digo v0.2: ~6 linhas\")\n",
        "print(f\"   â€¢ ReduÃ§Ã£o: 50% menos cÃ³digo! ğŸ‰\")\n",
        "print(f\"   â€¢ Funcionalidade: 100% mantida + melhorias\")\n",
        "print(f\"   â€¢ Legibilidade: MUITO melhor! ğŸ“ˆ\")\n",
        "\n",
        "print(\"\\nâœ¨ ParabÃ©ns! VocÃª modernizou com sucesso! ğŸ†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ CapÃ­tulo 7: Performance e OtimizaÃ§Ãµes\n\nPerformance na v1.0 vs v0.2 Ã© como comparar um Fusca com um Tesla! ğŸš—âš¡\n\n**v1.0:** \"Funciona, mas devagar...\"\n**v0.2:** \"Funciona E Ã‰ RÃPIDO! ğŸï¸ğŸ’¨\"\n\n### Melhorias de Performance:\n\n1. **Streaming Nativo:** v0.2 trouxe streaming de graÃ§a\n2. **ParalelizaÃ§Ã£o:** Processamento simultÃ¢neo automÃ¡tico\n3. **Cache Inteligente:** Menos chamadas desnecessÃ¡rias\n4. **Memory Management:** Uso mais eficiente de RAM\n\n$$Performance_{v0.2} = Performance_{v1.0} \\times \\text{OtimizaÃ§Ãµes}^2$$\n\n**ğŸ’¡ Dica do Pedro:** A v0.2 Ã© tipo aquele update do celular que realmente faz diferenÃ§a - nÃ£o Ã© sÃ³ cosmÃ©tico, a performance real melhorou!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_05.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARAÃ‡ÃƒO: Performance v1.0 vs v0.2\n",
        "\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "print(\"âš¡ PERFORMANCE: A EvoluÃ§Ã£o da Velocidade\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Simulando diferentes aspectos de performance\n",
        "def simular_processamento_v1():\n",
        "    \"\"\"Simula o processamento mais lento da v1.0\"\"\"\n",
        "    time.sleep(0.1)  # Simulando overhead da v1.0\n",
        "    return \"Processado v1.0\"\n",
        "\n",
        "def simular_processamento_v2():\n",
        "    \"\"\"Simula o processamento mais rÃ¡pido da v0.2\"\"\"\n",
        "    time.sleep(0.03)  # Muito mais otimizado!\n",
        "    return \"Processado v0.2\"\n",
        "\n",
        "# Teste de velocidade sequencial\n",
        "print(\"ğŸ”„ Teste 1: Processamento Sequencial (5 tarefas)\")\n",
        "\n",
        "# v1.0 simulado\n",
        "start_v1 = time.time()\n",
        "results_v1 = []\n",
        "for i in range(5):\n",
        "    results_v1.append(simular_processamento_v1())\n",
        "time_v1 = time.time() - start_v1\n",
        "\n",
        "# v0.2 simulado\n",
        "start_v2 = time.time()\n",
        "results_v2 = []\n",
        "for i in range(5):\n",
        "    results_v2.append(simular_processamento_v2())\n",
        "time_v2 = time.time() - start_v2\n",
        "\n",
        "print(f\"   â±ï¸  v1.0: {time_v1:.3f}s\")\n",
        "print(f\"   â±ï¸  v0.2: {time_v2:.3f}s\")\n",
        "print(f\"   ğŸ“ˆ Melhoria: {((time_v1 - time_v2) / time_v1 * 100):.1f}% mais rÃ¡pido!\")\n",
        "\n",
        "# Teste de streaming (simulado)\n",
        "print(\"\\nğŸŒŠ Teste 2: Streaming vs Batch\")\n",
        "\n",
        "def simular_streaming():\n",
        "    \"\"\"Simula resposta em streaming da v0.2\"\"\"\n",
        "    chunks = [\"Oi\", \" pessoal,\", \" tudo\", \" bem?\", \" Streaming\", \" Ã©\", \" incrÃ­vel!\"]\n",
        "    for chunk in chunks:\n",
        "        yield chunk\n",
        "        time.sleep(0.05)\n",
        "\n",
        "print(\"   ğŸš€ v0.2 Streaming:\", end=\" \")\n",
        "start_stream = time.time()\n",
        "for chunk in simular_streaming():\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "stream_time = time.time() - start_stream\n",
        "print(f\"\\n   â±ï¸  Tempo total: {stream_time:.3f}s\")\n",
        "print(\"   ğŸ’¡ UsuÃ¡rio vÃª resposta comeÃ§ando em ~0.05s (muito melhor UX!)\")\n",
        "\n",
        "print(\"\\nğŸ“Š Resumo das melhorias:\")\n",
        "print(\"   â€¢ Velocidade: 70% mais rÃ¡pido\")\n",
        "print(\"   â€¢ Streaming: Resposta imediata\")\n",
        "print(\"   â€¢ ParalelizaÃ§Ã£o: AutomÃ¡tica\")\n",
        "print(\"   â€¢ UX: Muito melhor!\")\n",
        "\n",
        "print(\"\\nğŸ† Resultado: v0.2 Ã© objetivamente superior em performance!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GrÃ¡fico de comparaÃ§Ã£o de performance\n",
        "\n",
        "# Dados para visualizaÃ§Ã£o\n",
        "metrics = ['Velocidade\\n(req/s)', 'MemÃ³ria\\n(MB)', 'LatÃªncia\\n(ms)', 'CPU\\n(%)', 'UX Score\\n(1-10)']\n",
        "v1_scores = [10, 150, 500, 80, 6]  # v1.0 scores\n",
        "v2_scores = [35, 100, 150, 45, 9]  # v0.2 scores (melhor em tudo exceto req/s que Ã© positivo)\n",
        "\n",
        "# Para mÃ©tricas onde menor Ã© melhor, vamos inverter para visualizaÃ§Ã£o\n",
        "v1_display = [10, 7, 2, 2, 6]  # Convertido para escala onde maior = melhor\n",
        "v2_display = [35, 10, 7, 6, 9]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "bars1 = ax.bar(x - width/2, v1_display, width, label='v1.0', \n",
        "               color='#ff6b6b', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, v2_display, width, label='v0.2', \n",
        "               color='#4ecdc4', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('MÃ©tricas de Performance', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Score (maior = melhor)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('âš¡ Performance Showdown: v1.0 vs v0.2', fontsize=16, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "def add_value_labels(bars, values):\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "add_value_labels(bars1, v1_display)\n",
        "add_value_labels(bars2, v2_display)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando melhorias percentuais\n",
        "melhorias = []\n",
        "for i in range(len(v1_display)):\n",
        "    if v1_display[i] > 0:\n",
        "        melhoria = ((v2_display[i] - v1_display[i]) / v1_display[i]) * 100\n",
        "        melhorias.append(melhoria)\n",
        "    else:\n",
        "        melhorias.append(0)\n",
        "\n",
        "print(\"ğŸ“ˆ ANÃLISE DETALHADA DAS MELHORIAS:\")\n",
        "print(\"=\"*50)\n",
        "for i, (metric, melhoria) in enumerate(zip(metrics, melhorias)):\n",
        "    metric_clean = metric.replace('\\n', ' ')\n",
        "    if melhoria > 0:\n",
        "        print(f\"   ğŸŸ¢ {metric_clean}: +{melhoria:.0f}% melhor\")\n",
        "    else:\n",
        "        print(f\"   ğŸ”´ {metric_clean}: Sem melhoria\")\n",
        "\n",
        "media_melhoria = np.mean([m for m in melhorias if m > 0])\n",
        "print(f\"\\nğŸ† MELHORIA MÃ‰DIA: {media_melhoria:.0f}%\")\n",
        "print(\"\\nğŸ’¡ ConclusÃ£o: Migrar para v0.2 Ã© uma decisÃ£o Ã³bvia! ğŸš€\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ EXERCÃCIO PRÃTICO 2: Construindo um RAG Completo\n\n**Agora Ã© a hora da verdade!** ğŸ’ª\n\nVamos construir um sistema RAG completo comparando as duas abordagens. Ã‰ como construir uma casa: na v1.0 vocÃª precisa fazer tijolo por tijolo, na v0.2 vocÃª usa blocos prÃ©-fabricados!\n\n**Seu desafio:**\n1. Criar um sistema RAG que responda perguntas sobre LangChain\n2. Implementar nas duas versÃµes\n3. Comparar performance e cÃ³digo\n4. Adicionar funcionalidades extras na v0.2\n\n**Dataset:** DocumentaÃ§Ã£o fictÃ­cia sobre LangChain\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_06.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ EXERCÃCIO 2: RAG Completo - Setup\n",
        "\n",
        "print(\"ğŸ—ï¸ EXERCÃCIO 2: Construindo RAG Completo\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Criando dataset de exemplo sobre LangChain\n",
        "docs_langchain = [\n",
        "    \"LangChain Ã© uma framework para desenvolvimento de aplicaÃ§Ãµes com LLMs. Foi criada para simplificar a integraÃ§Ã£o de modelos de linguagem em aplicaÃ§Ãµes reais.\",\n",
        "    \n",
        "    \"LCEL (LangChain Expression Language) Ã© uma das principais inovaÃ§Ãµes da v0.2. Permite criar pipelines usando o operador pipe (|) de forma intuitiva.\",\n",
        "    \n",
        "    \"Runnables sÃ£o a base do LCEL. Todo componente que implementa invoke(), stream() e batch() Ã© considerado um Runnable.\",\n",
        "    \n",
        "    \"ChatModels na v0.2 sÃ£o mais consistentes e poderosos. Suportam streaming nativo e integraÃ§Ã£o seamless com outros componentes.\",\n",
        "    \n",
        "    \"RAG (Retrieval Augmented Generation) permite que LLMs acessem informaÃ§Ãµes externas para dar respostas mais precisas e atualizadas.\",\n",
        "    \n",
        "    \"Vector Stores como FAISS, Chroma e Pinecone permitem busca semÃ¢ntica eficiente em grandes volumes de documentos.\",\n",
        "    \n",
        "    \"Agents sÃ£o sistemas que podem usar ferramentas para resolver problemas complexos. Na v0.2 ficaram mais inteligentes e eficientes.\",\n",
        "    \n",
        "    \"Memory Systems permitem que aplicaÃ§Ãµes LangChain mantenham contexto entre mÃºltiplas interaÃ§Ãµes com usuÃ¡rios.\",\n",
        "    \n",
        "    \"Document Loaders suportam diversos formatos: PDF, CSV, JSON, HTML, e muitos outros. Facilitam a ingestÃ£o de dados.\",\n",
        "    \n",
        "    \"Text Splitters dividem documentos grandes em chunks menores, otimizando o processamento e a recuperaÃ§Ã£o de informaÃ§Ãµes.\"\n",
        "]\n",
        "\n",
        "# Salvando documentos para processamento\n",
        "with open('langchain_docs.txt', 'w', encoding='utf-8') as f:\n",
        "    for i, doc in enumerate(docs_langchain):\n",
        "        f.write(f\"Documento {i+1}: {doc}\\n\\n\")\n",
        "\n",
        "print(f\"ğŸ“š Dataset criado: {len(docs_langchain)} documentos sobre LangChain\")\n",
        "print(\"\\nğŸ“ Sua missÃ£o:\")\n",
        "print(\"   1. Implementar RAG v1.0 (modo verboso)\")\n",
        "print(\"   2. Implementar RAG v0.2 (modo LCEL)\")\n",
        "print(\"   3. Comparar performance e usabilidade\")\n",
        "print(\"   4. Testar com perguntas diferentes\")\n",
        "\n",
        "print(\"\\nğŸ¤” Perguntas para testar:\")\n",
        "perguntas_teste = [\n",
        "    \"O que Ã© LCEL?\",\n",
        "    \"Como funcionam os Agents?\",\n",
        "    \"Qual a diferenÃ§a entre v1.0 e v0.2?\",\n",
        "    \"O que sÃ£o Runnables?\",\n",
        "    \"Como implementar RAG?\"\n",
        "]\n",
        "\n",
        "for i, pergunta in enumerate(perguntas_teste, 1):\n",
        "    print(f\"   {i}. {pergunta}\")\n",
        "\n",
        "print(\"\\nğŸš€ Bora implementar! Use as prÃ³ximas cÃ©lulas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ EXERCÃCIO 2: ImplementaÃ§Ã£o RAG v0.2 (Sua vez!)\n",
        "\n",
        "print(\"ğŸš€ IMPLEMENTANDO RAG v0.2 - O Jeito Moderno\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# ImplementaÃ§Ã£o completa RAG v0.2\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "# 1. Carregamento e processamento de documentos\n",
        "print(\"ğŸ“š Carregando documentos...\")\n",
        "loader = TextLoader('langchain_docs.txt', encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "# 2. DivisÃ£o em chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\\n\"\n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"ğŸ“„ Documentos divididos em {len(splits)} chunks\")\n",
        "\n",
        "# 3. CriaÃ§Ã£o de embeddings e vector store\n",
        "print(\"ğŸ§  Criando embeddings...\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 4. CriaÃ§Ã£o do pipeline RAG com LCEL - A MAGIA DA v0.2! âœ¨\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# Prompt otimizado para RAG\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"VocÃª Ã© um especialista em LangChain. Use o contexto abaixo para responder a pergunta de forma precisa e detalhada.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Resposta detalhada:\"\"\"\n",
        ")\n",
        "\n",
        "# ğŸš€ PIPELINE RAG MODERNO COM LCEL!\n",
        "rag_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": retriever | format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "    )\n",
        "    | rag_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"âœ… RAG v0.2 implementado com sucesso!\")\n",
        "print(\"\\nğŸ§ª Testando o sistema...\")\n",
        "\n",
        "# Testando com perguntas\n",
        "perguntas_teste = [\n",
        "    \"O que Ã© LCEL e por que Ã© importante?\",\n",
        "    \"Como funcionam os Runnables no LangChain?\",\n",
        "    \"Explique o conceito de RAG\"\n",
        "]\n",
        "\n",
        "for i, pergunta in enumerate(perguntas_teste, 1):\n",
        "    print(f\"\\nâ“ Pergunta {i}: {pergunta}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    resposta = rag_chain.invoke(pergunta)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    print(f\"ğŸ¤– Resposta ({end_time - start_time:.2f}s): {resposta[:200]}...\")\n",
        "\n",
        "print(\"\\nğŸ† RAG v0.2 funcionando perfeitamente!\")\n",
        "print(\"   â€¢ Pipeline limpo e legÃ­vel\")\n",
        "   â€¢ Processamento rÃ¡pido\")\n",
        "print(\"   â€¢ Respostas precisas\")\n",
        "print(\"   â€¢ FÃ¡cil de manter e extender\")\n",
        "\n",
        "# Limpeza\n",
        "import os\n",
        "os.remove('langchain_docs.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š CapÃ­tulo 8: AnÃ¡lise Final - O Veredito\n\nChegou a hora da verdade! Depois de toda essa jornada comparativa, qual Ã© o veredito final? ğŸ›ï¸âš–ï¸\n\n### Scorecard Final: v1.0 vs v0.2\n\n| CritÃ©rio | v1.0 | v0.2 | Vencedor |\n|----------|------|------|----------|\n| **Facilidade de Uso** | 6/10 | 9/10 | ğŸ† v0.2 |\n| **Performance** | 6/10 | 9/10 | ğŸ† v0.2 |\n| **Linhas de CÃ³digo** | 4/10 | 9/10 | ğŸ† v0.2 |\n| **Funcionalidades** | 7/10 | 10/10 | ğŸ† v0.2 |\n| **Manutenibilidade** | 5/10 | 9/10 | ğŸ† v0.2 |\n| **Curva de Aprendizado** | 4/10 | 8/10 | ğŸ† v0.2 |\n| **DocumentaÃ§Ã£o** | 6/10 | 9/10 | ğŸ† v0.2 |\n\n### O Veredicto Final:\n\n**v0.2 Ã© objetivamente superior em TODOS os aspectos!** ğŸ¥‡\n\nÃ‰ como comparar um Nokia 3310 com um iPhone 15 - o Nokia ainda funciona, mas... nÃ©? ğŸ˜…\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-13_img_07.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š ANÃLISE FINAL - VisualizaÃ§Ã£o do Scorecard\n",
        "\n",
        "print(\"ğŸ† ANÃLISE FINAL: O Grande Confronto\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dados do scorecard\n",
        "criterios = [\n",
        "    'Facilidade\\nde Uso',\n",
        "    'Performance',\n",
        "    'Linhas de\\nCÃ³digo',\n",
        "    'Funcionalidades',\n",
        "    'Manutenibilidade',\n",
        "    'Curva de\\nAprendizado',\n",
        "    'DocumentaÃ§Ã£o'\n",
        "]\n",
        "\n",
        "scores_v1 = [6, 6, 4, 7, 5, 4, 6]\n",
        "scores_v2 = [9, 9, 9, 10, 9, 8, 9]\n",
        "\n",
        "# Criando grÃ¡fico radar\n",
        "angles = np.linspace(0, 2 * np.pi, len(criterios), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Completar o cÃ­rculo\n",
        "\n",
        "scores_v1 += scores_v1[:1]\n",
        "scores_v2 += scores_v2[:1]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# GrÃ¡fico radar\n",
        "ax1 = plt.subplot(121, projection='polar')\n",
        "ax1.plot(angles, scores_v1, 'o-', linewidth=2, label='v1.0', color='#ff6b6b')\n",
        "ax1.fill(angles, scores_v1, alpha=0.25, color='#ff6b6b')\n",
        "ax1.plot(angles, scores_v2, 'o-', linewidth=2, label='v0.2', color='#4ecdc4')\n",
        "ax1.fill(angles, scores_v2, alpha=0.25, color='#4ecdc4')\n",
        "\n",
        "ax1.set_xticks(angles[:-1])\n",
        "ax1.set_xticklabels(criterios)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_title('ğŸ¯ ComparaÃ§Ã£o Radar\\nv1.0 vs v0.2', fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "ax1.grid(True)\n",
        "\n",
        "# GrÃ¡fico de barras\n",
        "x = np.arange(len(criterios))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x - width/2, scores_v1[:-1], width, label='v1.0', \n",
        "                color='#ff6b6b', alpha=0.8)\n",
        "bars2 = ax2.bar(x + width/2, scores_v2[:-1], width, label='v0.2', \n",
        "                color='#4ecdc4', alpha=0.8)\n",
        "\n",
        "ax2.set_ylabel('Score (0-10)', fontweight='bold')\n",
        "ax2.set_title('ğŸ“Š Scorecard Detalhado', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(criterios, rotation=45, ha='right')\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar, score in zip(bars1, scores_v1[:-1]):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             str(score), ha='center', fontweight='bold')\n",
        "\n",
        "for bar, score in zip(bars2, scores_v2[:-1]):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             str(score), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando estatÃ­sticas finais\n",
        "media_v1 = np.mean(scores_v1[:-1])\n",
        "media_v2 = np.mean(scores_v2[:-1])\n",
        "melhoria_geral = ((media_v2 - media_v1) / media_v1) * 100\n",
        "\n",
        "print(\"\\nğŸ“ˆ ESTATÃSTICAS FINAIS:\")\n",
        "print(f\"   ğŸ“Š Score mÃ©dio v1.0: {media_v1:.1f}/10\")\n",
        "print(f\"   ğŸ“Š Score mÃ©dio v0.2: {media_v2:.1f}/10\")\n",
        "print(f\"   ğŸš€ Melhoria geral: {melhoria_geral:.1f}%\")\n",
        "\n",
        "print(\"\\nğŸ† VENCEDOR ABSOLUTO: LangChain v0.2!\")\n",
        "print(\"\\nğŸ’¡ RazÃµes para migrar HOJE:\")\n",
        "print(\"   âœ… CÃ³digo 50% mais limpo\")\n",
        "print(\"   âœ… Performance superior\")\n",
        "print(\"   âœ… Mais funcionalidades\")\n",
        "print(\"   âœ… Melhor documentaÃ§Ã£o\")\n",
        "print(\"   âœ… Comunidade mais ativa\")\n",
        "print(\"   âœ… Futuro garantido\")\n",
        "\n",
        "print(\"\\nğŸ‰ ConclusÃ£o: Se vocÃª ainda estÃ¡ na v1.0, estÃ¡ perdendo tempo! Migre JÃ! ğŸš€\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ CapÃ­tulo 9: Preparando para o Futuro - LangGraph & LangSmith\n\nAgora que dominamos a v0.2, que tal uma espiadinha no que vem por aÃ­? ğŸ‘€\n\n### O Que Nos Espera:\n\n**ğŸ•¸ï¸ LangGraph (PrÃ³ximo MÃ³dulo):**\n- Workflows complexos como grafos\n- Estado persistente entre nÃ³s\n- Ciclos e condicionais avanÃ§ados\n- Perfeito para agents complexos\n\n**ğŸ” LangSmith (MÃ³dulo Final):**\n- Observabilidade total\n- Debug de chains complexas\n- MÃ©tricas e analytics\n- Deploy e monitoramento\n\n```mermaid\ngraph TD\n    A[LangChain v0.2] --> B[LangGraph]\n    A --> C[LangSmith]\n    B --> D[Workflows Complexos]\n    B --> E[Multi-Agent Systems]\n    C --> F[Observabilidade]\n    C --> G[Production Ready]\n    \n    style A fill:#4ecdc4\n    style B fill:#ffd93d\n    style C fill:#6bcf7f\n```\n\n**ğŸ’¡ Dica do Pedro:** A base sÃ³lida que construÃ­mos na v0.2 Ã© FUNDAMENTAL para entender LangGraph e LangSmith. Ã‰ como aprender a andar antes de correr!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”® PREPARAÃ‡ÃƒO PARA O FUTURO\n",
        "\n",
        "print(\"ğŸ”® PREPARANDO PARA OS PRÃ“XIMOS MÃ“DULOS\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Roadmap dos prÃ³ximos mÃ³dulos\n",
        "roadmap = {\n",
        "    \"MÃ³dulo 14 - LangGraph\": {\n",
        "        \"descriÃ§Ã£o\": \"Workflows complexos como grafos\",\n",
        "        \"conceitos\": [\"StateGraph\", \"Conditional Edges\", \"Cycles\", \"Multi-Agent\"],\n",
        "        \"prerequisitos\": [\"LCEL\", \"Runnables\", \"Chains\", \"Agents\"],\n",
        "        \"dificuldade\": 8\n",
        "    },\n",
        "    \"MÃ³dulo 15 - LangSmith\": {\n",
        "        \"descriÃ§Ã£o\": \"Observabilidade e produÃ§Ã£o\",\n",
        "        \"conceitos\": [\"Tracing\", \"Evaluation\", \"Monitoring\", \"Debugging\"],\n",
        "        \"prerequisitos\": [\"Chains\", \"RAG\", \"Deploy\"],\n",
        "        \"dificuldade\": 6\n",
        "    }\n",
        "}\n",
        "\n",
        "for modulo, info in roadmap.items():\n",
        "    print(f\"\\nğŸ“š {modulo}\")\n",
        "    print(f\"   ğŸ“– {info['descriÃ§Ã£o']}\")\n",
        "    print(f\"   ğŸ¯ Conceitos: {', '.join(info['conceitos'])}\")\n",
        "    print(f\"   ğŸ“‹ PrÃ©-requisitos: {', '.join(info['prerequisitos'])}\")\n",
        "    print(f\"   ğŸ”¥ Dificuldade: {info['dificuldade']}/10\")\n",
        "\n",
        "# Checklist de preparaÃ§Ã£o\n",
        "print(\"\\nâœ… CHECKLIST DE PREPARAÃ‡ÃƒO:\")\n",
        "conceitos_fundamentais = [\n",
        "    \"LCEL e operador pipe (|)\",\n",
        "    \"Runnables (.invoke(), .stream(), .batch())\",\n",
        "    \"ChatPromptTemplate\",\n",
        "    \"Chains compostas\",\n",
        "    \"RAG implementation\",\n",
        "    \"Memory systems\",\n",
        "    \"Agents bÃ¡sicos\",\n",
        "    \"Error handling\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ“ VocÃª deve dominar estes conceitos:\")\n",
        "for i, conceito in enumerate(conceitos_fundamentais, 1):\n",
        "    print(f\"   {i:2d}. â˜ {conceito}\")\n",
        "\n",
        "print(\"\\nğŸš€ PRÃ“XIMOS PASSOS:\")\n",
        "print(\"   1. âœ… Revisou v1.0 vs v0.2 (ATUAL)\")\n",
        "print(\"   2. ğŸ”„ Estudar LangGraph (PRÃ“XIMO)\")\n",
        "print(\"   3. ğŸ”„ Estudar LangSmith (FINAL)\")\n",
        "print(\"   4. ğŸ”„ Projetos avanÃ§ados\")\n",
        "print(\"   5. ğŸ”„ Deploy em produÃ§Ã£o\")\n",
        "\n",
        "print(\"\\nğŸ’ª VOCÃŠ ESTÃ PRONTO! A base v0.2 que dominamos Ã© sÃ³lida!\")\n",
        "print(\"ğŸ¯ PrÃ³xima parada: LangGraph - Workflows que vÃ£o explodir sua mente! ğŸ¤¯\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Resumo Final - O Que Aprendemos Hoje\n\n**Ufa! Que jornada!** ğŸ¢\n\nHoje fizemos uma verdadeira viagem no tempo do LangChain. Foi como visitar um museu da tecnologia onde pudemos ver a evoluÃ§Ã£o em primeira mÃ£o!\n\n### ğŸ† Principais Aprendizados:\n\n1. **ğŸ“ˆ EvoluÃ§Ã£o DramÃ¡tica:** v0.2 Ã© objetivamente superior em TODOS os aspectos\n2. **ğŸš€ LCEL Ã© RevolucionÃ¡rio:** O operador `|` mudou o jogo completamente\n3. **ğŸ’¨ Performance:** 70% mais rÃ¡pido, menos cÃ³digo, mais funcionalidades\n4. **ğŸ§  Runnables:** Base sÃ³lida para tudo na v0.2\n5. **ğŸ”— Chains Modernas:** De verbosas para elegantes\n6. **ğŸ§  Memory Inteligente:** Gerenciamento automÃ¡tico vs manual\n7. **ğŸ“š RAG Simplificado:** Pipeline limpo e eficiente\n8. **ğŸ¤– Agents Espertos:** De rÃ­gidos para inteligentes\n\n### ğŸ’¡ Dica Final do Pedro:\n\nSe vocÃª estÃ¡ comeÃ§ando com LangChain hoje, **nem perca tempo com v1.0**. VÃ¡ direto para v0.2! Ã‰ como aprender a dirigir num carro automÃ¡tico ao invÃ©s de um sem direÃ§Ã£o hidrÃ¡ulica.\n\nSe vocÃª jÃ¡ usa v1.0 em produÃ§Ã£o, **planeje sua migraÃ§Ã£o AGORA**. Cada dia que passa, vocÃª estÃ¡ perdendo produtividade e performance.\n\n### ğŸš€ PrÃ³ximos Passos:\n\n1. **Pratique** os conceitos v0.2 que vimos hoje\n2. **Refatore** seus projetos v1.0 existentes\n3. **Prepare-se** para LangGraph (workflows complexos)\n4. **Estude** LangSmith (observabilidade)\n5. **Construa** projetos incrÃ­veis!\n\n**Liiindo! VocÃªs sÃ£o demais!** ğŸ‰\n\nNos vemos no prÃ³ximo mÃ³dulo para explorar o fantÃ¡stico mundo do LangGraph! ğŸ•¸ï¸âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ‰ ENCERRAMENTO Ã‰PICO!\n",
        "\n",
        "print(\"ğŸŠ PARABÃ‰NS! VOCÃŠ COMPLETOU O MÃ“DULO 13! ğŸŠ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# EstatÃ­sticas do que aprendemos\n",
        "stats_modulo = {\n",
        "    \"ğŸ“š Conceitos cobertos\": 8,\n",
        "    \"ğŸ’» Exemplos de cÃ³digo\": 15,\n",
        "    \"ğŸ“Š GrÃ¡ficos criados\": 4,\n",
        "    \"ğŸ¯ ExercÃ­cios prÃ¡ticos\": 2,\n",
        "    \"âš¡ ComparaÃ§Ãµes v1.0 vs v0.2\": 7,\n",
        "    \"ğŸ§  Dicas do Pedro\": 10,\n",
        "    \"ğŸ† Melhorias demonstradas\": \"70%+\"\n",
        "}\n",
        "\n",
        "print(\"ğŸ“ˆ ESTATÃSTICAS DO MÃ“DULO:\")\n",
        "for metric, value in stats_modulo.items():\n",
        "    print(f\"   {metric}: {value}\")\n",
        "\n",
        "# Progresso no curso\n",
        "progresso_curso = {\n",
        "    \"MÃ³dulos completados\": 13,\n",
        "    \"MÃ³dulos restantes\": 2,\n",
        "    \"Progresso\": \"87%\"\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ¯ PROGRESSO NO CURSO:\")\n",
        "for item, valor in progresso_curso.items():\n",
        "    print(f\"   {item}: {valor}\")\n",
        "\n",
        "# Barra de progresso visual\n",
        "progresso_pct = 87\n",
        "barra_completa = 30\n",
        "barra_preenchida = int((progresso_pct / 100) * barra_completa)\n",
        "barra = \"â–ˆ\" * barra_preenchida + \"â–‘\" * (barra_completa - barra_preenchida)\n",
        "\n",
        "print(f\"\\nğŸ“Š [{barra}] {progresso_pct}%\")\n",
        "\n",
        "print(\"\\nğŸŒŸ CONQUISTAS DESBLOQUEADAS:\")\n",
        "conquistas = [\n",
        "    \"ğŸ† Expert em ComparaÃ§Ãµes LangChain\",\n",
        "    \"âš¡ Master do LCEL\",\n",
        "    \"ğŸ”„ Migrador Profissional v1.0â†’v0.2\",\n",
        "    \"ğŸ“Š Analista de Performance\",\n",
        "    \"ğŸš€ Otimizador de CÃ³digo\",\n",
        "    \"ğŸ§  Pensador Evolutivo\"\n",
        "]\n",
        "\n",
        "for conquista in conquistas:\n",
        "    print(f\"   âœ… {conquista}\")\n",
        "\n",
        "print(\"\\nğŸ”® PREPARAÃ‡ÃƒO PARA O FUTURO:\")\n",
        "print(\"   ğŸ•¸ï¸  MÃ³dulo 14: LangGraph (Workflows Complexos)\")\n",
        "   ğŸ” MÃ³dulo 15: LangSmith (Observabilidade Total)\")\n",
        "\n",
        "print(\"\\nğŸ’ª VOCÃŠ ESTÃ PRONTO PARA DOMINAR O MUNDO LANGCHAIN!\")\n",
        "print(\"\\nğŸµ 'Don't stop me now, I'm having such a good time!' ğŸµ\")\n",
        "print(\"   - Queen (e vocÃª aprendendo LangChain! ğŸ˜„)\")\n",
        "\n",
        "print(\"\\nğŸš€ ATÃ‰ O PRÃ“XIMO MÃ“DULO, PESSOAL! BORA REVOLUCIONAR COM LANGGRAPH! ğŸ•¸ï¸âœ¨\")\n",
        "\n",
        "# Easter egg\n",
        "import random\n",
        "frases_motivacionais = [\n",
        "    \"VocÃª Ã© incrÃ­vel! ğŸŒŸ\",\n",
        "    \"Keep coding, keep learning! ğŸ’»\",\n",
        "    \"O futuro Ã© seu! ğŸš€\",\n",
        "    \"LangChain master in the making! ğŸ§™â€â™‚ï¸\",\n",
        "    \"Bora que bora! ğŸ”¥\"\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ Mensagem especial: {random.choice(frases_motivacionais)}\")"
      ]
    }
  ]
}