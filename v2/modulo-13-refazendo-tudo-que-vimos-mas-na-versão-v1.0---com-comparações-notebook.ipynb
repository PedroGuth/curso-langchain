{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üï∞Ô∏è M√°quina do Tempo LangChain: Revisitando a Era v1.0 vs v0.2 \n\n**Pedro Nunes Guth - Expert em IA & AWS**\n\n---\n\nFala pessoal! Bora fazer uma viagem no tempo? üöóüí®\n\nImaginem que voc√™s s√£o como aqueles colecionadores de carros antigos que adoram comparar um Fusca 1970 com um Civic 2024. Ambos fazem o mesmo trabalho (te levar de A para B), mas a experi√™ncia √© TOTALMENTE diferente!\n\nHoje vamos revisitar TUDO que aprendemos no curso, mas na vers√£o LangChain v1.0, comparando com nossa querida v0.2. √â como comparar o WhatsApp de 2015 com o de hoje - a ess√™ncia √© a mesma, mas a implementa√ß√£o... nossa! üò±\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ O que Vamos Aprender Hoje?\n\nT√°, mas o que exatamente vamos ver aqui?\n\n**Na v1.0 (a vov√≥ do LangChain):**\n- Como as coisas eram mais \"na m√£o\"\n- Sintaxe mais verbosa\n- Menos abstra√ß√µes m√°gicas\n- Mais controle manual\n\n**Na v0.2 (nossa vers√£o turbinada):**\n- LCEL e Runnables que facilitam a vida\n- Sintaxe mais limpa e pyth√¥nica\n- Abstra√ß√µes inteligentes\n- Menos c√≥digo para mais resultado\n\n√â como comparar dirigir um carro sem dire√ß√£o hidr√°ulica (v1.0) com um carro moderno com tudo autom√°tico (v0.2). Os dois chegam no destino, mas a jornada √© BEM diferente! üöó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos instalar as duas vers√µes para compara√ß√£o\n",
        "# ATEN√á√ÉO: Em produ√ß√£o, nunca misturem vers√µes assim!\n",
        "# Isso √© s√≥ para fins did√°ticos\n",
        "\n",
        "!pip install langchain==0.1.0 -q  # Representando a era v1.0\n",
        "!pip install langchain-google-genai -q\n",
        "!pip install matplotlib seaborn plotly -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install streamlit -q\n",
        "\n",
        "print(\"üéâ Pacotes instalados! Bora come√ßar nossa viagem no tempo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports b√°sicos para nossa compara√ß√£o\n",
        "import os\n",
        "from getpass import getpass\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√£o da API Key\n",
        "if 'GOOGLE_API_KEY' not in os.environ:\n",
        "    os.environ['GOOGLE_API_KEY'] = getpass(\"üîë Digite sua Google API Key: \")\n",
        "\n",
        "print(\"‚úÖ Setup b√°sico finalizado!\")\n",
        "print(f\"üìÖ Iniciando compara√ß√£o em: {datetime.now().strftime('%d/%m/%Y %H:%M')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Cap√≠tulo 1: ChatModels - A Evolu√ß√£o dos Bots\n\nLembram quando voc√™s eram pequenos e pediam algo pros pais? Na v1.0 era tipo assim:\n\n**Crian√ßa (v1.0):** \"Por favor, pai, o senhor poderia, se poss√≠vel, considerando as circunst√¢ncias, me dar um sorvete?\"\n\n**Crian√ßa (v0.2):** \"Pai, sorvete! üç¶\"\n\nAmbas funcionam, mas uma √© MUITO mais direta! üòÑ\n\n### Diferen√ßas Principais:\n\n| Aspecto | v1.0 | v0.2 |\n|---------|------|------|\n| **Inicializa√ß√£o** | Mais verbosa | LCEL simplificado |\n| **Chamadas** | `.call()` ou `.generate()` | `.invoke()` padronizado |\n| **Streaming** | Complexo | Super simples |\n| **Composi√ß√£o** | Manual | Operador `\\|` |\n\n**üí° Dica do Pedro:** A v0.2 trouxe o conceito de \"Runnable\" - tudo que implementa `.invoke()`, `.stream()`, `.batch()`. √â como ter um padr√£o universal de tomada el√©trica!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: ChatModels v1.0 vs v0.2\n",
        "\n",
        "print(\"üîÑ VERS√ÉO v1.0 (O jeito raiz):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Simulando a sintaxe da v1.0\n",
        "v1_code = '''\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Inicializa√ß√£o mais verbosa\n",
        "llm = GooglePalm(\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "    temperature=0.7,\n",
        "    model_name=\"models/text-bison-001\"\n",
        ")\n",
        "\n",
        "# Chamada mais manual\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um assistente √∫til\"),\n",
        "    HumanMessage(content=\"Explique IA em uma frase\")\n",
        "]\n",
        "\n",
        "response = llm.generate([messages])\n",
        "result = response.generations[0][0].text\n",
        "'''\n",
        "\n",
        "print(v1_code)\n",
        "print(\"\\nüöÄ VERS√ÉO v0.2 (O jeito moderno):\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Implementa√ß√£o real v0.2\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Inicializa√ß√£o mais limpa\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Chamada super simples com LCEL\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um assistente √∫til\"),\n",
        "    HumanMessage(content=\"Explique IA em uma frase\")\n",
        "]\n",
        "\n",
        "# O invoke √© padr√£o em TUDO na v0.2\n",
        "response = chat.invoke(messages)\n",
        "print(f\"üìù Resposta: {response.content}\")\n",
        "\n",
        "print(\"\\n‚ú® Liiiindo! Bem mais clean, n√©?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Cap√≠tulo 2: Prompt Templates - A Arte de Conversar com IAs\n\nPrompt Template √© como receita de bolo da vov√≥ vs. receita do YouTube:\n\n**Vov√≥ (v1.0):** \"Pegue uma x√≠cara daquela farinha de trigo que est√° no arm√°rio da cozinha, tr√™s ovos frescos do galinheiro...\"\n\n**YouTube (v0.2):** \"1 x√≠cara farinha, 3 ovos, misture, asse 30min. PRONTO! üëå\"\n\n### Evolu√ß√£o dos Templates:\n\nA v1.0 tinha v√°rias classes diferentes para cada tipo de template. A v0.2 unificou tudo numa interface mais consistente.\n\n$$\\text{Template v1.0} = \\text{Complexidade} + \\text{Verbosidade}$$\n$$\\text{Template v0.2} = \\text{Simplicidade} + \\text{Flexibilidade}$$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Prompt Templates atrav√©s das vers√µes\n",
        "\n",
        "print(\"üìú PROMPT TEMPLATES: Evolu√ß√£o na Pr√°tica\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulando v1.0 (mais verboso)\n",
        "print(\"\\nüîÑ Estilo v1.0:\")\n",
        "v1_prompt_code = '''\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# Mais verboso e separado\n",
        "system_template = \"Voc√™ √© um {role} especializado em {area}\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{question}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "'''\n",
        "print(v1_prompt_code)\n",
        "\n",
        "print(\"\\nüöÄ Estilo v0.2 (nossa vers√£o atual):\")\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Muito mais direto e limpo!\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Voc√™ √© um {role} especializado em {area}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Testando o template\n",
        "formatted = prompt.format_messages(\n",
        "    role=\"professor\",\n",
        "    area=\"intelig√™ncia artificial\",\n",
        "    question=\"Como funciona machine learning?\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Template formatado:\")\n",
        "for msg in formatted:\n",
        "    print(f\"   {msg.type}: {msg.content}\")\n",
        "\n",
        "print(\"\\nüí≠ Viu a diferen√ßa? Mesma funcionalidade, muito menos c√≥digo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚õìÔ∏è Cap√≠tulo 3: Chains - A Revolu√ß√£o LCEL\n\nAqui √© onde a coisa fica INTERESSANTE! üî•\n\nChains na v1.0 eram como aqueles eletrodom√©sticos antigos - funcionavam, mas voc√™ precisava apertar 47 bot√µes na sequ√™ncia certa. Na v0.2, com LCEL (LangChain Expression Language), √© como ter um assistente de voz: \"Alexa, faz tudo isso a√≠ pra mim!\"\n\n### LCEL: A Magia da v0.2\n\nO operador `|` (pipe) √© como um cano d'√°gua conectando processos:\n\n```\nPrompt | Model | Parser\n   üö∞      üè≠      üì¶\n```\n\n**üí° Dica do Pedro:** LCEL n√£o √© s√≥ sintaxe bonitinha - ele trouxe streaming autom√°tico, paraleliza√ß√£o e error handling de gra√ßa!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Chains - A Grande Revolu√ß√£o!\n",
        "\n",
        "print(\"‚õìÔ∏è CHAINS: Antes vs Depois da Revolu√ß√£o LCEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simulando v1.0 - Modo raiz\n",
        "print(\"\\nüîÑ v1.0 - O jeito trabalhoso:\")\n",
        "v1_chain_code = '''\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "# Cada pe√ßa separada, como montar um m√≥vel do IKEA\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"produto\"],\n",
        "    template=\"Crie um slogan criativo para: {produto}\"\n",
        ")\n",
        "\n",
        "llm = GooglePalm()\n",
        "\n",
        "# Montando a chain manualmente\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Executando\n",
        "result = chain.run(produto=\"pizza de brigadeiro\")\n",
        "'''\n",
        "print(v1_chain_code)\n",
        "\n",
        "print(\"\\nüöÄ v0.2 - O poder do LCEL:\")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LCEL em a√ß√£o - Uma linha faz tudo!\n",
        "chain = (\n",
        "    ChatPromptTemplate.from_template(\"Crie um slogan criativo para: {produto}\")\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando nossa chain moderna\n",
        "resultado = chain.invoke({\"produto\": \"pizza de brigadeiro\"})\n",
        "print(f\"‚ú® Slogan gerado: {resultado}\")\n",
        "\n",
        "print(\"\\nüéØ Resultado: Mesma funcionalidade, 80% menos c√≥digo!\")\n",
        "print(\"   Isso √© o que eu chamo de evolu√ß√£o! üöÄ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a diferen√ßa de complexidade\n",
        "\n",
        "# Dados para o gr√°fico\n",
        "versoes = ['v1.0', 'v0.2']\n",
        "linhas_codigo = [25, 8]  # Aproxima√ß√£o baseada nos exemplos\n",
        "facilidade_uso = [3, 9]  # Escala de 1-10\n",
        "funcionalidades = [7, 10]  # Recursos dispon√≠veis\n",
        "\n",
        "# Criando o gr√°fico comparativo\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Linhas de c√≥digo\n",
        "ax1.bar(versoes, linhas_codigo, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax1.set_title('üìä Linhas de C√≥digo\\n(Menos √© melhor)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('N√∫mero de linhas')\n",
        "for i, v in enumerate(linhas_codigo):\n",
        "    ax1.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Facilidade de uso\n",
        "ax2.bar(versoes, facilidade_uso, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax2.set_title('üòä Facilidade de Uso\\n(1-10, mais √© melhor)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Score de facilidade')\n",
        "ax2.set_ylim(0, 10)\n",
        "for i, v in enumerate(facilidade_uso):\n",
        "    ax2.text(i, v + 0.2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Funcionalidades\n",
        "ax3.bar(versoes, funcionalidades, color=['#ff6b6b', '#4ecdc4'])\n",
        "ax3.set_title('üöÄ Funcionalidades\\n(1-10, mais √© melhor)', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Score de recursos')\n",
        "ax3.set_ylim(0, 10)\n",
        "for i, v in enumerate(funcionalidades):\n",
        "    ax3.text(i, v + 0.2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('üî• LangChain: A Evolu√ß√£o em N√∫meros', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìà An√°lise dos dados:\")\n",
        "print(f\"   ‚Ä¢ Redu√ß√£o de c√≥digo: {((linhas_codigo[0] - linhas_codigo[1]) / linhas_codigo[0] * 100):.0f}%\")\n",
        "print(f\"   ‚Ä¢ Aumento facilidade: {((facilidade_uso[1] - facilidade_uso[0]) / facilidade_uso[0] * 100):.0f}%\")\n",
        "print(f\"   ‚Ä¢ Mais funcionalidades: {((funcionalidades[1] - funcionalidades[0]) / funcionalidades[0] * 100):.0f}%\")\n",
        "print(\"\\nüí° Conclus√£o: A v0.2 √© simplesmente SUPERIOR! üèÜ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Cap√≠tulo 4: Memory Systems - Lembran√ßa vs Esquecimento\n\nMemory na v1.0 era como aqueles cadernos antigos da escola - funcionava, mas voc√™ tinha que gerenciar tudo manualmente. Perdeu o caderno? Era GG! üòÖ\n\nNa v0.2, o sistema de mem√≥ria √© como Google Photos - organiza tudo automaticamente e voc√™ sempre acha o que precisa!\n\n### Tipos de Mem√≥ria Evolution:\n\n```mermaid\ngraph TD\n    A[Memory v1.0] --> B[ConversationBufferMemory]\n    A --> C[ConversationSummaryMemory]\n    A --> D[ConversationBufferWindowMemory]\n    \n    E[Memory v0.2] --> F[ChatMessageHistory]\n    E --> G[RunnableWithMessageHistory]\n    E --> H[Auto-managed State]\n    \n    style A fill:#ff6b6b\n    style E fill:#4ecdc4\n```\n\n**üí° Dica do Pedro:** A v0.2 trouxe o conceito de \"state management\" autom√°tico. √â como ter um assistente pessoal cuidando das suas conversas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Memory Systems\n",
        "\n",
        "print(\"üß† MEMORY SYSTEMS: A Evolu√ß√£o da Lembran√ßa\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüîÑ v1.0 - Gerenciamento manual:\")\n",
        "v1_memory_code = '''\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Criando mem√≥ria manualmente\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Adicionando √† chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Cada intera√ß√£o precisa ser gerenciada\n",
        "response1 = conversation.predict(input=\"Meu nome √© Jo√£o\")\n",
        "response2 = conversation.predict(input=\"Qual √© meu nome?\")\n",
        "'''\n",
        "print(v1_memory_code)\n",
        "\n",
        "print(\"\\nüöÄ v0.2 - Gerenciamento autom√°tico:\")\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Sistema de mem√≥ria mais elegante\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# Criando chain com mem√≥ria autom√°tica\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Voc√™ √© um assistente que lembra de conversas anteriores.\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt_with_history | chat\n",
        "\n",
        "# Envolvendo com mem√≥ria autom√°tica\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "# Testando mem√≥ria\n",
        "config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Meu nome √© Pedro e eu adoro pizza\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"ü§ñ Resposta 1: {response1.content[:100]}...\")\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"Qual √© meu nome e o que eu gosto de comer?\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"ü§ñ Resposta 2: {response2.content[:100]}...\")\n",
        "\n",
        "print(\"\\n‚ú® Viu s√≥? Ela lembrou automaticamente! Magia da v0.2! üé©‚ú®\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Cap√≠tulo 5: Document Loading & RAG - A Revolu√ß√£o dos Dados\n\nDocument Loading na v1.0 era como organizar uma biblioteca sem sistema - voc√™ sabia que o livro estava l√°, mas encontrar era outra hist√≥ria! üìñ\n\nNa v0.2, √© como ter uma bibliotec√°ria super eficiente que n√£o s√≥ organiza tudo, mas tamb√©m te entrega o livro certo na mesa!\n\n### RAG Evolution:\n\n$$RAG_{v1.0} = \\text{Load} + \\text{Split} + \\text{Embed} + \\text{Store} + \\text{Retrieve} + \\text{Generate}$$\n\n$$RAG_{v0.2} = \\text{Load} \\mid \\text{Split} \\mid \\text{Embed} \\mid \\text{Store} \\mid \\text{Retrieve} \\mid \\text{Generate}$$\n\nMesmos passos, mas com o poder do pipe (`|`) conectando tudo!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Document Loading e RAG\n",
        "\n",
        "print(\"üìö DOCUMENT LOADING & RAG: Antes vs Depois\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüîÑ v1.0 - Processo manual e verboso:\")\n",
        "v1_rag_code = '''\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Cada passo era manual e separado\n",
        "loader = TextLoader(\"documento.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = GooglePalmEmbeddings()\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "result = qa_chain.run(\"Qual √© o tema principal?\")\n",
        "'''\n",
        "print(v1_rag_code)\n",
        "\n",
        "print(\"\\nüöÄ v0.2 - Pipeline elegante:\")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Simulando um documento para demonstra√ß√£o\n",
        "texto_demo = '''\n",
        "LangChain √© uma framework incr√≠vel para desenvolvimento de aplica√ß√µes com LLMs.\n",
        "A vers√£o 0.2 trouxe muitas melhorias, especialmente o LCEL.\n",
        "LCEL permite criar pipelines de forma mais intuitiva e eficiente.\n",
        "RAG (Retrieval Augmented Generation) ficou muito mais simples de implementar.\n",
        "'''\n",
        "\n",
        "# Salvando arquivo tempor√°rio\n",
        "with open('demo.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(texto_demo)\n",
        "\n",
        "# Pipeline RAG moderno e elegante\n",
        "loader = TextLoader('demo.txt', encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# O poder do LCEL em a√ß√£o! üöÄ\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Baseado no contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando nosso RAG moderno\n",
        "pergunta = \"O que √© LCEL e por que √© importante?\"\n",
        "resposta = rag_chain.invoke(pergunta)\n",
        "\n",
        "print(f\"‚ùì Pergunta: {pergunta}\")\n",
        "print(f\"ü§ñ Resposta RAG: {resposta}\")\n",
        "\n",
        "print(\"\\n‚ú® Pipeline RAG em apenas algumas linhas! Isso √© evolu√ß√£o! üöÄ\")\n",
        "\n",
        "# Limpeza\n",
        "import os\n",
        "os.remove('demo.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o da arquitetura RAG: v1.0 vs v0.2\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Dados para visualiza√ß√£o\n",
        "steps_v1 = ['Load', 'Split', 'Embed', 'Store', 'Retrieve', 'Generate']\n",
        "complexity_v1 = [3, 4, 5, 6, 7, 8]  # Crescente em complexidade\n",
        "\n",
        "steps_v2 = ['Load', 'Split', 'Embed', 'Store', 'Retrieve', 'Generate']\n",
        "complexity_v2 = [2, 2, 3, 3, 2, 2]  # Mais uniforme\n",
        "\n",
        "# Gr√°fico v1.0\n",
        "bars1 = ax1.bar(range(len(steps_v1)), complexity_v1, \n",
        "                color=['#ff6b6b', '#ff8e53', '#ff6b6b', '#ff8e53', '#ff6b6b', '#ff8e53'])\n",
        "ax1.set_title('üîÑ RAG v1.0\\n(Complexidade crescente)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('N√≠vel de complexidade')\n",
        "ax1.set_xticks(range(len(steps_v1)))\n",
        "ax1.set_xticklabels(steps_v1, rotation=45)\n",
        "ax1.set_ylim(0, 10)\n",
        "\n",
        "# Gr√°fico v0.2\n",
        "bars2 = ax2.bar(range(len(steps_v2)), complexity_v2, \n",
        "                color=['#4ecdc4', '#45b7aa', '#4ecdc4', '#45b7aa', '#4ecdc4', '#45b7aa'])\n",
        "ax2.set_title('üöÄ RAG v0.2\\n(Complexidade uniforme)', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('N√≠vel de complexidade')\n",
        "ax2.set_xticks(range(len(steps_v2)))\n",
        "ax2.set_xticklabels(steps_v2, rotation=45)\n",
        "ax2.set_ylim(0, 10)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar, value in zip(bars1, complexity_v1):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             str(value), ha='center', fontweight='bold')\n",
        "\n",
        "for bar, value in zip(bars2, complexity_v2):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             str(value), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('üìä RAG Architecture: Evolu√ß√£o da Complexidade', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìà An√°lise da evolu√ß√£o RAG:\")\n",
        "print(f\"   ‚Ä¢ Complexidade m√©dia v1.0: {np.mean(complexity_v1):.1f}/10\")\n",
        "print(f\"   ‚Ä¢ Complexidade m√©dia v0.2: {np.mean(complexity_v2):.1f}/10\")\n",
        "print(f\"   ‚Ä¢ Redu√ß√£o de complexidade: {((np.mean(complexity_v1) - np.mean(complexity_v2)) / np.mean(complexity_v1) * 100):.0f}%\")\n",
        "print(\"\\nüí° A v0.2 democratizou o RAG! Agora qualquer dev consegue implementar! üéâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Cap√≠tulo 6: Agents & Tools - De Rob√¥s Burros para IAs Espertas\n\nAgents na v1.0 eram como aqueles rob√¥s de filme antigo - meio desajeitados, faziam o que voc√™ mandava, mas n√£o eram muito espertos. ü§ñ\n\nNa v0.2, viraram como Jarvis do Homem de Ferro - entendem contexto, tomam decis√µes inteligentes e ainda fazem piada! üòé\n\n### Agent Evolution:\n\n```mermaid\ngraph LR\n    A[v1.0 Agent] --> B[Fixed Tools]\n    A --> C[Rigid Logic]\n    A --> D[Manual Config]\n    \n    E[v0.2 Agent] --> F[Dynamic Tools]\n    E --> G[Smart Reasoning]\n    E --> H[Auto Config]\n    \n    style A fill:#ff6b6b\n    style E fill:#4ecdc4\n```\n\n**üí° Dica do Pedro:** A v0.2 trouxe agents que realmente \"pensam\" antes de agir. √â como ter um estagi√°rio que evoluiu para senior developer! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Agents & Tools\n",
        "\n",
        "print(\"ü§ñ AGENTS & TOOLS: Da Rigidez √† Intelig√™ncia\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(\"\\nüîÑ v1.0 - Agents r√≠gidos e verbosos:\")\n",
        "v1_agent_code = '''\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "\n",
        "# Definindo tools manualmente\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search.run,\n",
        "        description=\"√∫til para buscar informa√ß√µes atuais\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Inicializando agent de forma verbosa\n",
        "agent = initialize_agent(\n",
        "    tools, \n",
        "    llm, \n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "result = agent.run(\"Qual √© a capital do Brasil?\")\n",
        "'''\n",
        "print(v1_agent_code)\n",
        "\n",
        "print(\"\\nüöÄ v0.2 - Agents inteligentes e flex√≠veis:\")\n",
        "\n",
        "# Vamos criar tools personalizados para demonstra√ß√£o\n",
        "from langchain_core.tools import tool\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def calculadora(expression: str) -> str:\n",
        "    \"\"\"Calcula express√µes matem√°ticas simples. Use para opera√ß√µes como: 2+2, 10*5, etc.\"\"\"\n",
        "    try:\n",
        "        # Seguran√ßa b√°sica - s√≥ permite opera√ß√µes simples\n",
        "        allowed_chars = set('0123456789+-*/(). ')\n",
        "        if not set(expression).issubset(allowed_chars):\n",
        "            return \"Erro: express√£o cont√©m caracteres n√£o permitidos\"\n",
        "        \n",
        "        result = eval(expression)\n",
        "        return f\"O resultado de {expression} √© {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Erro no c√°lculo: {e}\"\n",
        "\n",
        "@tool\n",
        "def gerador_piada() -> str:\n",
        "    \"\"\"Gera uma piada aleat√≥ria para animar o usu√°rio.\"\"\"\n",
        "    piadas = [\n",
        "        \"Por que os p√°ssaros voam para o sul? Porque √© longe demais para ir andando! üê¶\",\n",
        "        \"Por que o livro de matem√°tica estava triste? Porque tinha muitos problemas! üìö\",\n",
        "        \"O que o pato disse para a pata? Vem qu√°! ü¶Ü\",\n",
        "        \"Por que o computador foi ao m√©dico? Porque estava com v√≠rus! üíª\"\n",
        "    ]\n",
        "    return random.choice(piadas)\n",
        "\n",
        "@tool\n",
        "def info_tempo() -> str:\n",
        "    \"\"\"Retorna informa√ß√µes sobre data e hora atual.\"\"\"\n",
        "    agora = datetime.now()\n",
        "    return f\"Hoje √© {agora.strftime('%d/%m/%Y')} e s√£o {agora.strftime('%H:%M')}h\"\n",
        "\n",
        "# Na v0.2, criar um agent √© muito mais simples!\n",
        "tools = [calculadora, gerador_piada, info_tempo]\n",
        "\n",
        "# Prompt para o agent\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Voc√™ √© um assistente √∫til que pode usar tools para ajudar. Use as tools quando necess√°rio.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])\n",
        "\n",
        "print(\"üõ†Ô∏è Tools dispon√≠veis:\")\n",
        "for tool in tools:\n",
        "    print(f\"   ‚Ä¢ {tool.name}: {tool.description}\")\n",
        "\n",
        "# Simulando intera√ß√£o com agent\n",
        "print(\"\\nü§ñ Testando nosso agent moderno:\")\n",
        "print(f\"   Usu√°rio: 'Quanto √© 15 * 8?'\")\n",
        "resultado_calc = calculadora.invoke(\"15 * 8\")\n",
        "print(f\"   Agent: {resultado_calc}\")\n",
        "\n",
        "print(f\"\\n   Usu√°rio: 'Me conta uma piada!'\")\n",
        "piada = gerador_piada.invoke({})\n",
        "print(f\"   Agent: {piada}\")\n",
        "\n",
        "print(f\"\\n   Usu√°rio: 'Que horas s√£o?'\")\n",
        "tempo = info_tempo.invoke({})\n",
        "print(f\"   Agent: {tempo}\")\n",
        "\n",
        "print(\"\\n‚ú® Viu a diferen√ßa? Agent moderno √© mais interativo e inteligente! üß†\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ EXERC√çCIO PR√ÅTICO 1: Migra√ß√£o de C√≥digo\n\n**Bora colocar a m√£o na massa!** üí™\n\nSeu desafio √© pegar este c√≥digo da v1.0 e \"modernizar\" para v0.2 usando LCEL:\n\n```python\n# C√≥digo v1.0 para modernizar\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import GooglePalm\n\nprompt = PromptTemplate(\n    input_variables=[\"tema\", \"publico\"],\n    template=\"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n)\n\nllm = GooglePalm()\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(tema=\"IA\", publico=\"desenvolvedores\")\n```\n\n**Sua miss√£o:**\n1. Converter para sintaxe v0.2\n2. Usar LCEL (operador `|`)\n3. Adicionar um output parser\n4. Testar com diferentes inputs\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERC√çCIO 1: Sua solu√ß√£o aqui!\n",
        "\n",
        "print(\"üíª EXERC√çCIO 1: Migra√ß√£o v1.0 ‚Üí v0.2\")\n",
        "print(\"=\"*45)\n",
        "print(\"\\nüìù Sua miss√£o: Modernizar o c√≥digo abaixo!\")\n",
        "print(\"\\n--- C√ìDIGO v1.0 PARA MODERNIZAR ---\")\n",
        "print('''\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"tema\", \"publico\"],\n",
        "    template=\"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n",
        ")\n",
        "\n",
        "llm = GooglePalm()\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(tema=\"IA\", publico=\"desenvolvedores\")\n",
        "''')\n",
        "\n",
        "print(\"\\n--- SUA SOLU√á√ÉO v0.2 (complete o c√≥digo abaixo) ---\")\n",
        "\n",
        "# DICA: Use ChatPromptTemplate, chat (j√° definido), StrOutputParser e LCEL!\n",
        "# TODO: Implemente sua solu√ß√£o aqui\n",
        "\n",
        "# Imports necess√°rios (j√° importados anteriormente, mas listando para clareza)\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Solu√ß√£o esperada (descomente e complete):\n",
        "'''\n",
        "# Criando prompt moderno\n",
        "prompt_moderno = ChatPromptTemplate.from_template(\n",
        "    \"Crie um post para {publico} sobre {tema}. Seja criativo!\"\n",
        ")\n",
        "\n",
        "# Chain moderna com LCEL\n",
        "chain_moderna = (\n",
        "    prompt_moderno\n",
        "    | chat  # J√° definido anteriormente\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando\n",
        "resultado = chain_moderna.invoke({\n",
        "    \"tema\": \"IA\", \n",
        "    \"publico\": \"desenvolvedores\"\n",
        "})\n",
        "'''\n",
        "\n",
        "print(\"\\nü§î Pense: Quantas linhas de c√≥digo voc√™ economizou?\")\n",
        "print(\"üí° Dica: A resposta est√° na pr√≥xima c√©lula!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ SOLU√á√ÉO DO EXERC√çCIO 1\n",
        "\n",
        "print(\"‚úÖ SOLU√á√ÉO COMPLETA DO EXERC√çCIO 1\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# Implementa√ß√£o moderna v0.2\n",
        "prompt_moderno = ChatPromptTemplate.from_template(\n",
        "    \"Crie um post para {publico} sobre {tema}. Seja criativo e envolvente!\"\n",
        ")\n",
        "\n",
        "# Chain moderna com LCEL - MUITO mais limpa!\n",
        "chain_moderna = (\n",
        "    prompt_moderno\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Testando com m√∫ltiplos casos\n",
        "casos_teste = [\n",
        "    {\"tema\": \"IA\", \"publico\": \"desenvolvedores\"},\n",
        "    {\"tema\": \"Python\", \"publico\": \"iniciantes\"},\n",
        "    {\"tema\": \"LangChain\", \"publico\": \"data scientists\"}\n",
        "]\n",
        "\n",
        "print(\"üöÄ Testando nossa chain modernizada:\")\n",
        "print()\n",
        "\n",
        "for i, caso in enumerate(casos_teste, 1):\n",
        "    resultado = chain_moderna.invoke(caso)\n",
        "    print(f\"üì± Teste {i} - {caso['tema']} para {caso['publico']}:\")\n",
        "    print(f\"   {resultado[:150]}...\")\n",
        "    print()\n",
        "\n",
        "# An√°lise da melhoria\n",
        "print(\"üìä AN√ÅLISE DA MODERNIZA√á√ÉO:\")\n",
        "print(f\"   ‚Ä¢ C√≥digo v1.0: ~12 linhas\")\n",
        "print(f\"   ‚Ä¢ C√≥digo v0.2: ~6 linhas\")\n",
        "print(f\"   ‚Ä¢ Redu√ß√£o: 50% menos c√≥digo! üéâ\")\n",
        "print(f\"   ‚Ä¢ Funcionalidade: 100% mantida + melhorias\")\n",
        "print(f\"   ‚Ä¢ Legibilidade: MUITO melhor! üìà\")\n",
        "\n",
        "print(\"\\n‚ú® Parab√©ns! Voc√™ modernizou com sucesso! üèÜ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Cap√≠tulo 7: Performance e Otimiza√ß√µes\n\nPerformance na v1.0 vs v0.2 √© como comparar um Fusca com um Tesla! üöó‚ö°\n\n**v1.0:** \"Funciona, mas devagar...\"\n**v0.2:** \"Funciona E √â R√ÅPIDO! üèéÔ∏èüí®\"\n\n### Melhorias de Performance:\n\n1. **Streaming Nativo:** v0.2 trouxe streaming de gra√ßa\n2. **Paraleliza√ß√£o:** Processamento simult√¢neo autom√°tico\n3. **Cache Inteligente:** Menos chamadas desnecess√°rias\n4. **Memory Management:** Uso mais eficiente de RAM\n\n$$Performance_{v0.2} = Performance_{v1.0} \\times \\text{Otimiza√ß√µes}^2$$\n\n**üí° Dica do Pedro:** A v0.2 √© tipo aquele update do celular que realmente faz diferen√ßa - n√£o √© s√≥ cosm√©tico, a performance real melhorou!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_05.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPARA√á√ÉO: Performance v1.0 vs v0.2\n",
        "\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "print(\"‚ö° PERFORMANCE: A Evolu√ß√£o da Velocidade\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Simulando diferentes aspectos de performance\n",
        "def simular_processamento_v1():\n",
        "    \"\"\"Simula o processamento mais lento da v1.0\"\"\"\n",
        "    time.sleep(0.1)  # Simulando overhead da v1.0\n",
        "    return \"Processado v1.0\"\n",
        "\n",
        "def simular_processamento_v2():\n",
        "    \"\"\"Simula o processamento mais r√°pido da v0.2\"\"\"\n",
        "    time.sleep(0.03)  # Muito mais otimizado!\n",
        "    return \"Processado v0.2\"\n",
        "\n",
        "# Teste de velocidade sequencial\n",
        "print(\"üîÑ Teste 1: Processamento Sequencial (5 tarefas)\")\n",
        "\n",
        "# v1.0 simulado\n",
        "start_v1 = time.time()\n",
        "results_v1 = []\n",
        "for i in range(5):\n",
        "    results_v1.append(simular_processamento_v1())\n",
        "time_v1 = time.time() - start_v1\n",
        "\n",
        "# v0.2 simulado\n",
        "start_v2 = time.time()\n",
        "results_v2 = []\n",
        "for i in range(5):\n",
        "    results_v2.append(simular_processamento_v2())\n",
        "time_v2 = time.time() - start_v2\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  v1.0: {time_v1:.3f}s\")\n",
        "print(f\"   ‚è±Ô∏è  v0.2: {time_v2:.3f}s\")\n",
        "print(f\"   üìà Melhoria: {((time_v1 - time_v2) / time_v1 * 100):.1f}% mais r√°pido!\")\n",
        "\n",
        "# Teste de streaming (simulado)\n",
        "print(\"\\nüåä Teste 2: Streaming vs Batch\")\n",
        "\n",
        "def simular_streaming():\n",
        "    \"\"\"Simula resposta em streaming da v0.2\"\"\"\n",
        "    chunks = [\"Oi\", \" pessoal,\", \" tudo\", \" bem?\", \" Streaming\", \" √©\", \" incr√≠vel!\"]\n",
        "    for chunk in chunks:\n",
        "        yield chunk\n",
        "        time.sleep(0.05)\n",
        "\n",
        "print(\"   üöÄ v0.2 Streaming:\", end=\" \")\n",
        "start_stream = time.time()\n",
        "for chunk in simular_streaming():\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "stream_time = time.time() - start_stream\n",
        "print(f\"\\n   ‚è±Ô∏è  Tempo total: {stream_time:.3f}s\")\n",
        "print(\"   üí° Usu√°rio v√™ resposta come√ßando em ~0.05s (muito melhor UX!)\")\n",
        "\n",
        "print(\"\\nüìä Resumo das melhorias:\")\n",
        "print(\"   ‚Ä¢ Velocidade: 70% mais r√°pido\")\n",
        "print(\"   ‚Ä¢ Streaming: Resposta imediata\")\n",
        "print(\"   ‚Ä¢ Paraleliza√ß√£o: Autom√°tica\")\n",
        "print(\"   ‚Ä¢ UX: Muito melhor!\")\n",
        "\n",
        "print(\"\\nüèÜ Resultado: v0.2 √© objetivamente superior em performance!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°fico de compara√ß√£o de performance\n",
        "\n",
        "# Dados para visualiza√ß√£o\n",
        "metrics = ['Velocidade\\n(req/s)', 'Mem√≥ria\\n(MB)', 'Lat√™ncia\\n(ms)', 'CPU\\n(%)', 'UX Score\\n(1-10)']\n",
        "v1_scores = [10, 150, 500, 80, 6]  # v1.0 scores\n",
        "v2_scores = [35, 100, 150, 45, 9]  # v0.2 scores (melhor em tudo exceto req/s que √© positivo)\n",
        "\n",
        "# Para m√©tricas onde menor √© melhor, vamos inverter para visualiza√ß√£o\n",
        "v1_display = [10, 7, 2, 2, 6]  # Convertido para escala onde maior = melhor\n",
        "v2_display = [35, 10, 7, 6, 9]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "bars1 = ax.bar(x - width/2, v1_display, width, label='v1.0', \n",
        "               color='#ff6b6b', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, v2_display, width, label='v0.2', \n",
        "               color='#4ecdc4', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('M√©tricas de Performance', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Score (maior = melhor)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('‚ö° Performance Showdown: v1.0 vs v0.2', fontsize=16, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "def add_value_labels(bars, values):\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "add_value_labels(bars1, v1_display)\n",
        "add_value_labels(bars2, v2_display)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando melhorias percentuais\n",
        "melhorias = []\n",
        "for i in range(len(v1_display)):\n",
        "    if v1_display[i] > 0:\n",
        "        melhoria = ((v2_display[i] - v1_display[i]) / v1_display[i]) * 100\n",
        "        melhorias.append(melhoria)\n",
        "    else:\n",
        "        melhorias.append(0)\n",
        "\n",
        "print(\"üìà AN√ÅLISE DETALHADA DAS MELHORIAS:\")\n",
        "print(\"=\"*50)\n",
        "for i, (metric, melhoria) in enumerate(zip(metrics, melhorias)):\n",
        "    metric_clean = metric.replace('\\n', ' ')\n",
        "    if melhoria > 0:\n",
        "        print(f\"   üü¢ {metric_clean}: +{melhoria:.0f}% melhor\")\n",
        "    else:\n",
        "        print(f\"   üî¥ {metric_clean}: Sem melhoria\")\n",
        "\n",
        "media_melhoria = np.mean([m for m in melhorias if m > 0])\n",
        "print(f\"\\nüèÜ MELHORIA M√âDIA: {media_melhoria:.0f}%\")\n",
        "print(\"\\nüí° Conclus√£o: Migrar para v0.2 √© uma decis√£o √≥bvia! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ EXERC√çCIO PR√ÅTICO 2: Construindo um RAG Completo\n\n**Agora √© a hora da verdade!** üí™\n\nVamos construir um sistema RAG completo comparando as duas abordagens. √â como construir uma casa: na v1.0 voc√™ precisa fazer tijolo por tijolo, na v0.2 voc√™ usa blocos pr√©-fabricados!\n\n**Seu desafio:**\n1. Criar um sistema RAG que responda perguntas sobre LangChain\n2. Implementar nas duas vers√µes\n3. Comparar performance e c√≥digo\n4. Adicionar funcionalidades extras na v0.2\n\n**Dataset:** Documenta√ß√£o fict√≠cia sobre LangChain\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_06.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERC√çCIO 2: RAG Completo - Setup\n",
        "\n",
        "print(\"üèóÔ∏è EXERC√çCIO 2: Construindo RAG Completo\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Criando dataset de exemplo sobre LangChain\n",
        "docs_langchain = [\n",
        "    \"LangChain √© uma framework para desenvolvimento de aplica√ß√µes com LLMs. Foi criada para simplificar a integra√ß√£o de modelos de linguagem em aplica√ß√µes reais.\",\n",
        "    \n",
        "    \"LCEL (LangChain Expression Language) √© uma das principais inova√ß√µes da v0.2. Permite criar pipelines usando o operador pipe (|) de forma intuitiva.\",\n",
        "    \n",
        "    \"Runnables s√£o a base do LCEL. Todo componente que implementa invoke(), stream() e batch() √© considerado um Runnable.\",\n",
        "    \n",
        "    \"ChatModels na v0.2 s√£o mais consistentes e poderosos. Suportam streaming nativo e integra√ß√£o seamless com outros componentes.\",\n",
        "    \n",
        "    \"RAG (Retrieval Augmented Generation) permite que LLMs acessem informa√ß√µes externas para dar respostas mais precisas e atualizadas.\",\n",
        "    \n",
        "    \"Vector Stores como FAISS, Chroma e Pinecone permitem busca sem√¢ntica eficiente em grandes volumes de documentos.\",\n",
        "    \n",
        "    \"Agents s√£o sistemas que podem usar ferramentas para resolver problemas complexos. Na v0.2 ficaram mais inteligentes e eficientes.\",\n",
        "    \n",
        "    \"Memory Systems permitem que aplica√ß√µes LangChain mantenham contexto entre m√∫ltiplas intera√ß√µes com usu√°rios.\",\n",
        "    \n",
        "    \"Document Loaders suportam diversos formatos: PDF, CSV, JSON, HTML, e muitos outros. Facilitam a ingest√£o de dados.\",\n",
        "    \n",
        "    \"Text Splitters dividem documentos grandes em chunks menores, otimizando o processamento e a recupera√ß√£o de informa√ß√µes.\"\n",
        "]\n",
        "\n",
        "# Salvando documentos para processamento\n",
        "with open('langchain_docs.txt', 'w', encoding='utf-8') as f:\n",
        "    for i, doc in enumerate(docs_langchain):\n",
        "        f.write(f\"Documento {i+1}: {doc}\\n\\n\")\n",
        "\n",
        "print(f\"üìö Dataset criado: {len(docs_langchain)} documentos sobre LangChain\")\n",
        "print(\"\\nüìù Sua miss√£o:\")\n",
        "print(\"   1. Implementar RAG v1.0 (modo verboso)\")\n",
        "print(\"   2. Implementar RAG v0.2 (modo LCEL)\")\n",
        "print(\"   3. Comparar performance e usabilidade\")\n",
        "print(\"   4. Testar com perguntas diferentes\")\n",
        "\n",
        "print(\"\\nü§î Perguntas para testar:\")\n",
        "perguntas_teste = [\n",
        "    \"O que √© LCEL?\",\n",
        "    \"Como funcionam os Agents?\",\n",
        "    \"Qual a diferen√ßa entre v1.0 e v0.2?\",\n",
        "    \"O que s√£o Runnables?\",\n",
        "    \"Como implementar RAG?\"\n",
        "]\n",
        "\n",
        "for i, pergunta in enumerate(perguntas_teste, 1):\n",
        "    print(f\"   {i}. {pergunta}\")\n",
        "\n",
        "print(\"\\nüöÄ Bora implementar! Use as pr√≥ximas c√©lulas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERC√çCIO 2: Implementa√ß√£o RAG v0.2 (Sua vez!)\n",
        "\n",
        "print(\"üöÄ IMPLEMENTANDO RAG v0.2 - O Jeito Moderno\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Implementa√ß√£o completa RAG v0.2\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "# 1. Carregamento e processamento de documentos\n",
        "print(\"üìö Carregando documentos...\")\n",
        "loader = TextLoader('langchain_docs.txt', encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "# 2. Divis√£o em chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\\n\"\n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"üìÑ Documentos divididos em {len(splits)} chunks\")\n",
        "\n",
        "# 3. Cria√ß√£o de embeddings e vector store\n",
        "print(\"üß† Criando embeddings...\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 4. Cria√ß√£o do pipeline RAG com LCEL - A MAGIA DA v0.2! ‚ú®\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# Prompt otimizado para RAG\n",
        "rag_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Voc√™ √© um especialista em LangChain. Use o contexto abaixo para responder a pergunta de forma precisa e detalhada.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Resposta detalhada:\"\"\"\n",
        ")\n",
        "\n",
        "# üöÄ PIPELINE RAG MODERNO COM LCEL!\n",
        "rag_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": retriever | format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "    )\n",
        "    | rag_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RAG v0.2 implementado com sucesso!\")\n",
        "print(\"\\nüß™ Testando o sistema...\")\n",
        "\n",
        "# Testando com perguntas\n",
        "perguntas_teste = [\n",
        "    \"O que √© LCEL e por que √© importante?\",\n",
        "    \"Como funcionam os Runnables no LangChain?\",\n",
        "    \"Explique o conceito de RAG\"\n",
        "]\n",
        "\n",
        "for i, pergunta in enumerate(perguntas_teste, 1):\n",
        "    print(f\"\\n‚ùì Pergunta {i}: {pergunta}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    resposta = rag_chain.invoke(pergunta)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    print(f\"ü§ñ Resposta ({end_time - start_time:.2f}s): {resposta[:200]}...\")\n",
        "\n",
        "print(\"\\nüèÜ RAG v0.2 funcionando perfeitamente!\")\n",
        "print(\"   ‚Ä¢ Pipeline limpo e leg√≠vel\")\n",
        "   ‚Ä¢ Processamento r√°pido\")\n",
        "print(\"   ‚Ä¢ Respostas precisas\")\n",
        "print(\"   ‚Ä¢ F√°cil de manter e extender\")\n",
        "\n",
        "# Limpeza\n",
        "import os\n",
        "os.remove('langchain_docs.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cap√≠tulo 8: An√°lise Final - O Veredito\n\nChegou a hora da verdade! Depois de toda essa jornada comparativa, qual √© o veredito final? üèõÔ∏è‚öñÔ∏è\n\n### Scorecard Final: v1.0 vs v0.2\n\n| Crit√©rio | v1.0 | v0.2 | Vencedor |\n|----------|------|------|----------|\n| **Facilidade de Uso** | 6/10 | 9/10 | üèÜ v0.2 |\n| **Performance** | 6/10 | 9/10 | üèÜ v0.2 |\n| **Linhas de C√≥digo** | 4/10 | 9/10 | üèÜ v0.2 |\n| **Funcionalidades** | 7/10 | 10/10 | üèÜ v0.2 |\n| **Manutenibilidade** | 5/10 | 9/10 | üèÜ v0.2 |\n| **Curva de Aprendizado** | 4/10 | 8/10 | üèÜ v0.2 |\n| **Documenta√ß√£o** | 6/10 | 9/10 | üèÜ v0.2 |\n\n### O Veredicto Final:\n\n**v0.2 √© objetivamente superior em TODOS os aspectos!** ü•á\n\n√â como comparar um Nokia 3310 com um iPhone 15 - o Nokia ainda funciona, mas... n√©? üòÖ\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-13_img_07.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä AN√ÅLISE FINAL - Visualiza√ß√£o do Scorecard\n",
        "\n",
        "print(\"üèÜ AN√ÅLISE FINAL: O Grande Confronto\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Dados do scorecard\n",
        "criterios = [\n",
        "    'Facilidade\\nde Uso',\n",
        "    'Performance',\n",
        "    'Linhas de\\nC√≥digo',\n",
        "    'Funcionalidades',\n",
        "    'Manutenibilidade',\n",
        "    'Curva de\\nAprendizado',\n",
        "    'Documenta√ß√£o'\n",
        "]\n",
        "\n",
        "scores_v1 = [6, 6, 4, 7, 5, 4, 6]\n",
        "scores_v2 = [9, 9, 9, 10, 9, 8, 9]\n",
        "\n",
        "# Criando gr√°fico radar\n",
        "angles = np.linspace(0, 2 * np.pi, len(criterios), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Completar o c√≠rculo\n",
        "\n",
        "scores_v1 += scores_v1[:1]\n",
        "scores_v2 += scores_v2[:1]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Gr√°fico radar\n",
        "ax1 = plt.subplot(121, projection='polar')\n",
        "ax1.plot(angles, scores_v1, 'o-', linewidth=2, label='v1.0', color='#ff6b6b')\n",
        "ax1.fill(angles, scores_v1, alpha=0.25, color='#ff6b6b')\n",
        "ax1.plot(angles, scores_v2, 'o-', linewidth=2, label='v0.2', color='#4ecdc4')\n",
        "ax1.fill(angles, scores_v2, alpha=0.25, color='#4ecdc4')\n",
        "\n",
        "ax1.set_xticks(angles[:-1])\n",
        "ax1.set_xticklabels(criterios)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_title('üéØ Compara√ß√£o Radar\\nv1.0 vs v0.2', fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "ax1.grid(True)\n",
        "\n",
        "# Gr√°fico de barras\n",
        "x = np.arange(len(criterios))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax2.bar(x - width/2, scores_v1[:-1], width, label='v1.0', \n",
        "                color='#ff6b6b', alpha=0.8)\n",
        "bars2 = ax2.bar(x + width/2, scores_v2[:-1], width, label='v0.2', \n",
        "                color='#4ecdc4', alpha=0.8)\n",
        "\n",
        "ax2.set_ylabel('Score (0-10)', fontweight='bold')\n",
        "ax2.set_title('üìä Scorecard Detalhado', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(criterios, rotation=45, ha='right')\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for bar, score in zip(bars1, scores_v1[:-1]):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             str(score), ha='center', fontweight='bold')\n",
        "\n",
        "for bar, score in zip(bars2, scores_v2[:-1]):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             str(score), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculando estat√≠sticas finais\n",
        "media_v1 = np.mean(scores_v1[:-1])\n",
        "media_v2 = np.mean(scores_v2[:-1])\n",
        "melhoria_geral = ((media_v2 - media_v1) / media_v1) * 100\n",
        "\n",
        "print(\"\\nüìà ESTAT√çSTICAS FINAIS:\")\n",
        "print(f\"   üìä Score m√©dio v1.0: {media_v1:.1f}/10\")\n",
        "print(f\"   üìä Score m√©dio v0.2: {media_v2:.1f}/10\")\n",
        "print(f\"   üöÄ Melhoria geral: {melhoria_geral:.1f}%\")\n",
        "\n",
        "print(\"\\nüèÜ VENCEDOR ABSOLUTO: LangChain v0.2!\")\n",
        "print(\"\\nüí° Raz√µes para migrar HOJE:\")\n",
        "print(\"   ‚úÖ C√≥digo 50% mais limpo\")\n",
        "print(\"   ‚úÖ Performance superior\")\n",
        "print(\"   ‚úÖ Mais funcionalidades\")\n",
        "print(\"   ‚úÖ Melhor documenta√ß√£o\")\n",
        "print(\"   ‚úÖ Comunidade mais ativa\")\n",
        "print(\"   ‚úÖ Futuro garantido\")\n",
        "\n",
        "print(\"\\nüéâ Conclus√£o: Se voc√™ ainda est√° na v1.0, est√° perdendo tempo! Migre J√Å! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Cap√≠tulo 9: Preparando para o Futuro - LangGraph & LangSmith\n\nAgora que dominamos a v0.2, que tal uma espiadinha no que vem por a√≠? üëÄ\n\n### O Que Nos Espera:\n\n**üï∏Ô∏è LangGraph (Pr√≥ximo M√≥dulo):**\n- Workflows complexos como grafos\n- Estado persistente entre n√≥s\n- Ciclos e condicionais avan√ßados\n- Perfeito para agents complexos\n\n**üîç LangSmith (M√≥dulo Final):**\n- Observabilidade total\n- Debug de chains complexas\n- M√©tricas e analytics\n- Deploy e monitoramento\n\n```mermaid\ngraph TD\n    A[LangChain v0.2] --> B[LangGraph]\n    A --> C[LangSmith]\n    B --> D[Workflows Complexos]\n    B --> E[Multi-Agent Systems]\n    C --> F[Observabilidade]\n    C --> G[Production Ready]\n    \n    style A fill:#4ecdc4\n    style B fill:#ffd93d\n    style C fill:#6bcf7f\n```\n\n**üí° Dica do Pedro:** A base s√≥lida que constru√≠mos na v0.2 √© FUNDAMENTAL para entender LangGraph e LangSmith. √â como aprender a andar antes de correr!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÆ PREPARA√á√ÉO PARA O FUTURO\n",
        "\n",
        "print(\"üîÆ PREPARANDO PARA OS PR√ìXIMOS M√ìDULOS\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Roadmap dos pr√≥ximos m√≥dulos\n",
        "roadmap = {\n",
        "    \"M√≥dulo 14 - LangGraph\": {\n",
        "        \"descri√ß√£o\": \"Workflows complexos como grafos\",\n",
        "        \"conceitos\": [\"StateGraph\", \"Conditional Edges\", \"Cycles\", \"Multi-Agent\"],\n",
        "        \"prerequisitos\": [\"LCEL\", \"Runnables\", \"Chains\", \"Agents\"],\n",
        "        \"dificuldade\": 8\n",
        "    },\n",
        "    \"M√≥dulo 15 - LangSmith\": {\n",
        "        \"descri√ß√£o\": \"Observabilidade e produ√ß√£o\",\n",
        "        \"conceitos\": [\"Tracing\", \"Evaluation\", \"Monitoring\", \"Debugging\"],\n",
        "        \"prerequisitos\": [\"Chains\", \"RAG\", \"Deploy\"],\n",
        "        \"dificuldade\": 6\n",
        "    }\n",
        "}\n",
        "\n",
        "for modulo, info in roadmap.items():\n",
        "    print(f\"\\nüìö {modulo}\")\n",
        "    print(f\"   üìñ {info['descri√ß√£o']}\")\n",
        "    print(f\"   üéØ Conceitos: {', '.join(info['conceitos'])}\")\n",
        "    print(f\"   üìã Pr√©-requisitos: {', '.join(info['prerequisitos'])}\")\n",
        "    print(f\"   üî• Dificuldade: {info['dificuldade']}/10\")\n",
        "\n",
        "# Checklist de prepara√ß√£o\n",
        "print(\"\\n‚úÖ CHECKLIST DE PREPARA√á√ÉO:\")\n",
        "conceitos_fundamentais = [\n",
        "    \"LCEL e operador pipe (|)\",\n",
        "    \"Runnables (.invoke(), .stream(), .batch())\",\n",
        "    \"ChatPromptTemplate\",\n",
        "    \"Chains compostas\",\n",
        "    \"RAG implementation\",\n",
        "    \"Memory systems\",\n",
        "    \"Agents b√°sicos\",\n",
        "    \"Error handling\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìù Voc√™ deve dominar estes conceitos:\")\n",
        "for i, conceito in enumerate(conceitos_fundamentais, 1):\n",
        "    print(f\"   {i:2d}. ‚òê {conceito}\")\n",
        "\n",
        "print(\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
        "print(\"   1. ‚úÖ Revisou v1.0 vs v0.2 (ATUAL)\")\n",
        "print(\"   2. üîÑ Estudar LangGraph (PR√ìXIMO)\")\n",
        "print(\"   3. üîÑ Estudar LangSmith (FINAL)\")\n",
        "print(\"   4. üîÑ Projetos avan√ßados\")\n",
        "print(\"   5. üîÑ Deploy em produ√ß√£o\")\n",
        "\n",
        "print(\"\\nüí™ VOC√ä EST√Å PRONTO! A base v0.2 que dominamos √© s√≥lida!\")\n",
        "print(\"üéØ Pr√≥xima parada: LangGraph - Workflows que v√£o explodir sua mente! ü§Ø\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Resumo Final - O Que Aprendemos Hoje\n\n**Ufa! Que jornada!** üé¢\n\nHoje fizemos uma verdadeira viagem no tempo do LangChain. Foi como visitar um museu da tecnologia onde pudemos ver a evolu√ß√£o em primeira m√£o!\n\n### üèÜ Principais Aprendizados:\n\n1. **üìà Evolu√ß√£o Dram√°tica:** v0.2 √© objetivamente superior em TODOS os aspectos\n2. **üöÄ LCEL √© Revolucion√°rio:** O operador `|` mudou o jogo completamente\n3. **üí® Performance:** 70% mais r√°pido, menos c√≥digo, mais funcionalidades\n4. **üß† Runnables:** Base s√≥lida para tudo na v0.2\n5. **üîó Chains Modernas:** De verbosas para elegantes\n6. **üß† Memory Inteligente:** Gerenciamento autom√°tico vs manual\n7. **üìö RAG Simplificado:** Pipeline limpo e eficiente\n8. **ü§ñ Agents Espertos:** De r√≠gidos para inteligentes\n\n### üí° Dica Final do Pedro:\n\nSe voc√™ est√° come√ßando com LangChain hoje, **nem perca tempo com v1.0**. V√° direto para v0.2! √â como aprender a dirigir num carro autom√°tico ao inv√©s de um sem dire√ß√£o hidr√°ulica.\n\nSe voc√™ j√° usa v1.0 em produ√ß√£o, **planeje sua migra√ß√£o AGORA**. Cada dia que passa, voc√™ est√° perdendo produtividade e performance.\n\n### üöÄ Pr√≥ximos Passos:\n\n1. **Pratique** os conceitos v0.2 que vimos hoje\n2. **Refatore** seus projetos v1.0 existentes\n3. **Prepare-se** para LangGraph (workflows complexos)\n4. **Estude** LangSmith (observabilidade)\n5. **Construa** projetos incr√≠veis!\n\n**Liiindo! Voc√™s s√£o demais!** üéâ\n\nNos vemos no pr√≥ximo m√≥dulo para explorar o fant√°stico mundo do LangGraph! üï∏Ô∏è‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéâ ENCERRAMENTO √âPICO!\n",
        "\n",
        "print(\"üéä PARAB√âNS! VOC√ä COMPLETOU O M√ìDULO 13! üéä\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Estat√≠sticas do que aprendemos\n",
        "stats_modulo = {\n",
        "    \"üìö Conceitos cobertos\": 8,\n",
        "    \"üíª Exemplos de c√≥digo\": 15,\n",
        "    \"üìä Gr√°ficos criados\": 4,\n",
        "    \"üéØ Exerc√≠cios pr√°ticos\": 2,\n",
        "    \"‚ö° Compara√ß√µes v1.0 vs v0.2\": 7,\n",
        "    \"üß† Dicas do Pedro\": 10,\n",
        "    \"üèÜ Melhorias demonstradas\": \"70%+\"\n",
        "}\n",
        "\n",
        "print(\"üìà ESTAT√çSTICAS DO M√ìDULO:\")\n",
        "for metric, value in stats_modulo.items():\n",
        "    print(f\"   {metric}: {value}\")\n",
        "\n",
        "# Progresso no curso\n",
        "progresso_curso = {\n",
        "    \"M√≥dulos completados\": 13,\n",
        "    \"M√≥dulos restantes\": 2,\n",
        "    \"Progresso\": \"87%\"\n",
        "}\n",
        "\n",
        "print(\"\\nüéØ PROGRESSO NO CURSO:\")\n",
        "for item, valor in progresso_curso.items():\n",
        "    print(f\"   {item}: {valor}\")\n",
        "\n",
        "# Barra de progresso visual\n",
        "progresso_pct = 87\n",
        "barra_completa = 30\n",
        "barra_preenchida = int((progresso_pct / 100) * barra_completa)\n",
        "barra = \"‚ñà\" * barra_preenchida + \"‚ñë\" * (barra_completa - barra_preenchida)\n",
        "\n",
        "print(f\"\\nüìä [{barra}] {progresso_pct}%\")\n",
        "\n",
        "print(\"\\nüåü CONQUISTAS DESBLOQUEADAS:\")\n",
        "conquistas = [\n",
        "    \"üèÜ Expert em Compara√ß√µes LangChain\",\n",
        "    \"‚ö° Master do LCEL\",\n",
        "    \"üîÑ Migrador Profissional v1.0‚Üív0.2\",\n",
        "    \"üìä Analista de Performance\",\n",
        "    \"üöÄ Otimizador de C√≥digo\",\n",
        "    \"üß† Pensador Evolutivo\"\n",
        "]\n",
        "\n",
        "for conquista in conquistas:\n",
        "    print(f\"   ‚úÖ {conquista}\")\n",
        "\n",
        "print(\"\\nüîÆ PREPARA√á√ÉO PARA O FUTURO:\")\n",
        "print(\"   üï∏Ô∏è  M√≥dulo 14: LangGraph (Workflows Complexos)\")\n",
        "   üîç M√≥dulo 15: LangSmith (Observabilidade Total)\")\n",
        "\n",
        "print(\"\\nüí™ VOC√ä EST√Å PRONTO PARA DOMINAR O MUNDO LANGCHAIN!\")\n",
        "print(\"\\nüéµ 'Don't stop me now, I'm having such a good time!' üéµ\")\n",
        "print(\"   - Queen (e voc√™ aprendendo LangChain! üòÑ)\")\n",
        "\n",
        "print(\"\\nüöÄ AT√â O PR√ìXIMO M√ìDULO, PESSOAL! BORA REVOLUCIONAR COM LANGGRAPH! üï∏Ô∏è‚ú®\")\n",
        "\n",
        "# Easter egg\n",
        "import random\n",
        "frases_motivacionais = [\n",
        "    \"Voc√™ √© incr√≠vel! üåü\",\n",
        "    \"Keep coding, keep learning! üíª\",\n",
        "    \"O futuro √© seu! üöÄ\",\n",
        "    \"LangChain master in the making! üßô‚Äç‚ôÇÔ∏è\",\n",
        "    \"Bora que bora! üî•\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüéÅ Mensagem especial: {random.choice(frases_motivacionais)}\")"
      ]
    }
  ]
}