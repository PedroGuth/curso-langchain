{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Vector Store e Embeddings: A Biblioteca da IA Moderna\n\n**MÃ³dulo 7 de 15 - LangChain v0.2**\n\n### Por Pedro Nunes Guth\n\n---\n\nTÃ¡, mas o que Ã© um Vector Store? Imagina que vocÃª tem uma biblioteca GIGANTE com milhÃµes de livros, mas nÃ£o tem um sistema de catalogaÃ§Ã£o. Como vocÃª acharia o livro que precisa? ImpossÃ­vel, nÃ©?\n\nÃ‰ exatamente isso que fazemos com informaÃ§Ãµes quando usamos Vector Stores! Transformamos textos em \"impressÃµes digitais matemÃ¡ticas\" (embeddings) e organizamos tudo de forma que a IA consiga achar rapidinho o que precisa.\n\nNos mÃ³dulos anteriores, vimos como carregar e dividir documentos. Agora vamos aprender como fazer a IA \"entender\" e organizar essas informaÃ§Ãµes para usar no RAG (que vem no prÃ³ximo mÃ³dulo)!\n\n**ğŸ¯ O que vamos ver:**\n- O que sÃ£o embeddings na prÃ¡tica\n- Como funciona um Vector Store\n- ImplementaÃ§Ã£o com LangChain v0.2\n- Similaridade semÃ¢ntica\n- PreparaÃ§Ã£o para RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” Mas Afinal, O Que SÃ£o Embeddings?\n\nTÃ¡, vou explicar com uma analogia que todo brasileiro entende: **DNA das palavras**!\n\nSabe como cada pessoa tem um DNA Ãºnico que define suas caracterÃ­sticas? Embeddings sÃ£o como o \"DNA semÃ¢ntico\" das palavras e frases!\n\n### ğŸ“Š Matematicamente Falando:\n\nUm embedding Ã© uma representaÃ§Ã£o vetorial densa de texto em um espaÃ§o multidimensional (geralmente 768, 1024 ou 1536 dimensÃµes).\n\n$$\\text{embedding}(\\text{\"cachorro\"}) = [0.2, -0.1, 0.8, ..., 0.3] \\in \\mathbb{R}^n$$\n\nonde $n$ Ã© o nÃºmero de dimensÃµes do modelo.\n\n### ğŸ§ª A MÃ¡gica da Similaridade:\n\nPalavras com significados similares tÃªm embeddings prÃ³ximos no espaÃ§o vetorial:\n\n$$\\text{similaridade}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{||v_1|| \\cdot ||v_2||}$$\n\nIsso significa que \"cachorro\" e \"cÃ£o\" terÃ£o vetores bem prÃ³ximos!\n\n**ğŸ’¡ Dica do Pedro:** Pensa assim: se as palavras fossem pessoas numa festa, as que tÃªm assuntos em comum ficariam conversando perto umas das outras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Bora instalar as dependÃªncias!\n",
        "!pip install langchain langchain-community langchain-google-genai\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "!pip install numpy matplotlib seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports essenciais para nosso laboratÃ³rio de embeddings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangChain imports - a nova geraÃ§Ã£o!\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"ğŸš€ Todas as bibliotecas carregadas! Bora para a aventura dos vetores!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configurando nossa chave da Google (vocÃª vai precisar da sua!)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Se estiver no Colab, use:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Se estiver local, descomente e coloque sua chave:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_aqui\"\n",
        "\n",
        "print(\"ğŸ” Chaves configuradas! Vamos embeddar tudo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Primeiro Contato: Criando Nossos Primeiros Embeddings\n\nVamos comeÃ§ar com o bÃ¡sico: transformar texto em vetores! Ã‰ como traduzir portuguÃªs para \"matematiquÃªs\" que a IA entende.\n\n### ğŸ”§ Tipos de Modelos de Embedding:\n\n1. **Google Embeddings**: Integrados com Gemini\n2. **Sentence Transformers**: Gratuitos e poderosos\n3. **OpenAI Embeddings**: Caros mas eficientes\n\n**ğŸ’¡ Dica do Pedro:** Para estudar, use Sentence Transformers. Para produÃ§Ã£o sÃ©ria, Google ou OpenAI!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar nosso primeiro modelo de embedding!\n",
        "# Usando um modelo gratuito para comeÃ§ar\n",
        "\n",
        "# Modelo brasileiro que funciona bem em portuguÃªs!\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'}  # Usa CPU mesmo\n",
        ")\n",
        "\n",
        "print(\"ğŸ‰ Modelo de embedding carregado!\")\n",
        "print(f\"ğŸ“Š DimensÃµes do vetor: Vamos descobrir...\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando nosso modelo com frases brasileiras!\n",
        "frases_teste = [\n",
        "    \"Eu amo programar em Python\",\n",
        "    \"Adoro desenvolver com Python\", \n",
        "    \"Python Ã© uma linguagem incrÃ­vel\",\n",
        "    \"Hoje estÃ¡ chovendo muito\",\n",
        "    \"O clima estÃ¡ chuvoso\",\n",
        "    \"Vou comer pizza hoje\"\n",
        "]\n",
        "\n",
        "# Gerando embeddings para cada frase\n",
        "print(\"ğŸ§  Gerando embeddings...\")\n",
        "embeddings_frases = []\n",
        "\n",
        "for i, frase in enumerate(frases_teste):\n",
        "    embedding = embeddings_model.embed_query(frase)\n",
        "    embeddings_frases.append(embedding)\n",
        "    print(f\"ğŸ“ Frase {i+1}: '{frase}'\")\n",
        "    print(f\"ğŸ”¢ DimensÃµes: {len(embedding)}\")\n",
        "    print(f\"ğŸ“Š Primeiros 5 valores: {embedding[:5]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"âœ… Todos os embeddings gerados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ˆ Visualizando a MÃ¡gica dos Embeddings\n\nTÃ¡, mas como visualizar 384 dimensÃµes? Ã‰ impossÃ­vel! Mas temos um truque: **PCA (AnÃ¡lise de Componentes Principais)**.\n\nÃ‰ como pegar uma foto 3D e fazer uma sombra 2D na parede - perdemos informaÃ§Ã£o, mas conseguimos \"ver\" as relaÃ§Ãµes!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-07_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos visualizar nossos embeddings em 2D!\n",
        "# Ã‰ como fazer uma \"radiografia\" dos vetores\n",
        "\n",
        "# Convertendo para numpy array para facilitar\n",
        "embeddings_array = np.array(embeddings_frases)\n",
        "print(f\"ğŸ“Š Shape dos embeddings: {embeddings_array.shape}\")\n",
        "\n",
        "# Aplicando PCA para reduzir para 2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d = pca.fit_transform(embeddings_array)\n",
        "\n",
        "print(f\"ğŸ“‰ VariÃ¢ncia explicada: {pca.explained_variance_ratio_}\")\n",
        "print(f\"ğŸ“Š Total de variÃ¢ncia capturada: {sum(pca.explained_variance_ratio_):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando o grÃ¡fico dos embeddings - a visualizaÃ§Ã£o da mÃ¡gica!\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Cores diferentes para temas diferentes\n",
        "cores = ['red', 'red', 'red', 'blue', 'blue', 'green']\n",
        "temas = ['Python', 'Python', 'Python', 'Clima', 'Clima', 'Comida']\n",
        "\n",
        "# Plotando os pontos\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    plt.scatter(x, y, c=cores[i], s=100, alpha=0.7)\n",
        "    plt.annotate(f\"{i+1}: {temas[i]}\", \n",
        "                xy=(x, y), \n",
        "                xytext=(5, 5), \n",
        "                textcoords='offset points',\n",
        "                fontsize=10,\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor=cores[i], alpha=0.3))\n",
        "\n",
        "plt.title('ğŸ¯ VisualizaÃ§Ã£o dos Embeddings - Frases Similares Ficam Juntas!', fontsize=14)\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Legenda\n",
        "plt.scatter([], [], c='red', label='ProgramaÃ§Ã£o Python', s=100)\n",
        "plt.scatter([], [], c='blue', label='Clima/Chuva', s=100)\n",
        "plt.scatter([], [], c='green', label='Comida', s=100)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¨ Liiindo! Viu como as frases similares ficam prÃ³ximas?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Calculando Similaridades: A MatemÃ¡tica do \"Parecido\"\n\nAgora vamos calcular exatamente o quÃ£o \"parecidas\" sÃ£o nossas frases usando **similaridade cosseno**.\n\n### ğŸ“ FÃ³rmula da Similaridade Cosseno:\n\n$$\\cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n\n**Onde:**\n- 1.0 = IdÃªnticos\n- 0.0 = Sem relaÃ§Ã£o  \n- -1.0 = Opostos\n\n**ğŸ’¡ Dica do Pedro:** Ã‰ como medir o Ã¢ngulo entre dois vetores. Quanto menor o Ã¢ngulo, mais parecidos!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Calculando a matriz de similaridade - quem Ã© parecido com quem?\n",
        "similaridade_matrix = cosine_similarity(embeddings_array)\n",
        "\n",
        "print(\"ğŸ§® Matriz de Similaridade Cosseno:\")\n",
        "print(\"(1.0 = idÃªnticos, 0.0 = sem relaÃ§Ã£o)\\n\")\n",
        "\n",
        "# Criando labels mais legÃ­veis\n",
        "labels = [\n",
        "    \"Amo Python\",\n",
        "    \"Adoro Python\", \n",
        "    \"Python incrÃ­vel\",\n",
        "    \"Chovendo muito\",\n",
        "    \"Clima chuvoso\",\n",
        "    \"Comer pizza\"\n",
        "]\n",
        "\n",
        "# Mostrando a matriz bonitinha\n",
        "for i, frase1 in enumerate(labels):\n",
        "    for j, frase2 in enumerate(labels):\n",
        "        if i <= j:  # SÃ³ mostra a parte superior da matriz\n",
        "            score = similaridade_matrix[i][j]\n",
        "            emoji = \"ğŸ”¥\" if score > 0.8 and i != j else \"âœ…\" if score > 0.6 and i != j else \"â¡ï¸\" if i == j else \"âŒ\"\n",
        "            print(f\"{emoji} '{frase1}' vs '{frase2}': {score:.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando um heatmap da similaridade - visual e bonito!\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Heatmap com seaborn\n",
        "sns.heatmap(similaridade_matrix, \n",
        "            xticklabels=labels,\n",
        "            yticklabels=labels,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='RdYlBu_r',\n",
        "            center=0.5,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Similaridade Cosseno'})\n",
        "\n",
        "plt.title('ğŸ”¥ Mapa de Calor da Similaridade\\nQuanto mais vermelho, mais parecido!', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¨ Agora dÃ¡ pra ver claramente quem Ã© parecido com quem!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—„ï¸ Vector Stores: A Biblioteca Organizada da IA\n\nTÃ¡, mas e se tivermos milhÃµes de documentos? NÃ£o dÃ¡ pra calcular similaridade com tudo na mÃ£o!\n\nEntra o **Vector Store** - Ã© como uma biblioteca super organizada onde:\n- ğŸ“š Cada livro (documento) tem sua \"impressÃ£o digital\" (embedding)\n- ğŸ” Quando vocÃª procura algo, ele acha os mais parecidos rapidinho\n- âš¡ Usa algoritmos especiais (como FAISS) para busca ultra-rÃ¡pida\n\n### ğŸ—ï¸ Como Funciona um Vector Store:\n\n```mermaid\ngraph TD\n    A[Documento Original] --> B[Gerar Embedding]\n    B --> C[Armazenar no Vector Store]\n    D[Query do UsuÃ¡rio] --> E[Gerar Embedding da Query]\n    E --> F[Buscar Similares no Vector Store]\n    F --> G[Retornar Documentos Mais Relevantes]\n    C --> F\n```\n\n**ğŸ’¡ Dica do Pedro:** Ã‰ como o Spotify que recomenda mÃºsicas parecidas com o que vocÃª gosta, mas para textos!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar nosso primeiro Vector Store!\n",
        "# Usando FAISS - Facebook AI Similarity Search (gratuito e rÃ¡pido)\n",
        "\n",
        "# Primeiro, criamos documentos estruturados (lembra do mÃ³dulo 6?)\n",
        "documentos_exemplo = [\n",
        "    Document(\n",
        "        page_content=\"Python Ã© uma linguagem de programaÃ§Ã£o interpretada de alto nÃ­vel.\",\n",
        "        metadata={\"fonte\": \"wikipedia\", \"categoria\": \"programaÃ§Ã£o\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"LangChain Ã© um framework para desenvolver aplicaÃ§Ãµes com modelos de linguagem.\",\n",
        "        metadata={\"fonte\": \"docs\", \"categoria\": \"programaÃ§Ã£o\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"InteligÃªncia artificial estÃ¡ revolucionando a tecnologia moderna.\",\n",
        "        metadata={\"fonte\": \"artigo\", \"categoria\": \"tecnologia\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Machine learning permite que computadores aprendam sem programaÃ§Ã£o explÃ­cita.\",\n",
        "        metadata={\"fonte\": \"livro\", \"categoria\": \"tecnologia\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"O futebol Ã© o esporte mais popular do Brasil.\",\n",
        "        metadata={\"fonte\": \"esporte\", \"categoria\": \"esporte\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A culinÃ¡ria brasileira Ã© muito diversificada e saborosa.\",\n",
        "        metadata={\"fonte\": \"gastronomia\", \"categoria\": \"cultura\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“š Criamos {len(documentos_exemplo)} documentos para nosso vector store!\")\n",
        "print(\"\\nğŸ“‹ Resumo dos documentos:\")\n",
        "for i, doc in enumerate(documentos_exemplo):\n",
        "    print(f\"{i+1}. {doc.page_content[:50]}... [{doc.metadata['categoria']}]\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando nosso Vector Store com FAISS\n",
        "print(\"ğŸ—ï¸ Construindo o Vector Store...\")\n",
        "print(\"ğŸ“Š Gerando embeddings para todos os documentos...\")\n",
        "\n",
        "# FAISS.from_documents faz toda a mÃ¡gica:\n",
        "# 1. Gera embeddings para cada documento\n",
        "# 2. Cria o Ã­ndice FAISS\n",
        "# 3. Armazena tudo organizadinho\n",
        "vector_store = FAISS.from_documents(\n",
        "    documents=documentos_exemplo,\n",
        "    embedding=embeddings_model\n",
        ")\n",
        "\n",
        "print(\"âœ… Vector Store criado com sucesso!\")\n",
        "print(f\"ğŸ“ˆ NÃºmero de documentos indexados: {vector_store.index.ntotal}\")\n",
        "print(\"ğŸš€ Agora podemos fazer buscas semÃ¢nticas ultra-rÃ¡pidas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Busca SemÃ¢ntica: A MÃ¡gica Acontece Aqui!\n\nAgora vem a parte mais **LINDA** do Vector Store: busca semÃ¢ntica!\n\nDiferente de busca tradicional (que procura palavras exatas), a busca semÃ¢ntica entende o **significado**!\n\n### ğŸ†š ComparaÃ§Ã£o:\n- **Busca Tradicional:** \"cachorro\" â‰  \"cÃ£o\" \n- **Busca SemÃ¢ntica:** \"cachorro\" â‰ˆ \"cÃ£o\" â‰ˆ \"pet\" â‰ˆ \"animal domÃ©stico\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-07_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos testar nossa busca semÃ¢ntica!\n",
        "def busca_semantica(query, k=3):\n",
        "    \"\"\"FunÃ§Ã£o para fazer busca semÃ¢ntica e mostrar resultados bonitinhos\"\"\"\n",
        "    print(f\"ğŸ” Buscando por: '{query}'\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Busca por similaridade\n",
        "    resultados = vector_store.similarity_search_with_score(query, k=k)\n",
        "    \n",
        "    for i, (doc, score) in enumerate(resultados, 1):\n",
        "        # Convertendo score do FAISS (distÃ¢ncia) para similaridade\n",
        "        similaridade = 1 / (1 + score)  # Quanto menor a distÃ¢ncia, maior a similaridade\n",
        "        \n",
        "        print(f\"ğŸ† Resultado {i} - Similaridade: {similaridade:.3f}\")\n",
        "        print(f\"ğŸ“ ConteÃºdo: {doc.page_content}\")\n",
        "        print(f\"ğŸ·ï¸ Categoria: {doc.metadata['categoria']}\")\n",
        "        print(f\"ğŸ“Š Score FAISS: {score:.3f}\")\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "print(\"ğŸ¯ FunÃ§Ã£o de busca semÃ¢ntica pronta! Vamos testar...\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 1: Buscando sobre programaÃ§Ã£o\n",
        "resultados1 = busca_semantica(\"Como programar em linguagens de cÃ³digo?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 2: Buscando sobre IA/tecnologia\n",
        "resultados2 = busca_semantica(\"Algoritmos que aprendem sozinhos\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 3: Buscando sobre cultura brasileira\n",
        "resultados3 = busca_semantica(\"TradiÃ§Ãµes e comida do Brasil\", k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas do Vector Store\n\nBora turbinar nosso Vector Store! Tem vÃ¡rias configuraÃ§Ãµes que fazem diferenÃ§a na qualidade da busca.\n\n### ğŸ›ï¸ ParÃ¢metros Importantes:\n\n1. **k**: Quantos resultados retornar\n2. **fetch_k**: Quantos candidatos buscar internamente (sempre >= k)\n3. **lambda_mult**: Para MMR (Maximum Marginal Relevance) - diversidade vs relevÃ¢ncia\n4. **score_threshold**: Filtro por score mÃ­nimo\n\n**ğŸ’¡ Dica do Pedro:** Fetch_k maior = busca mais precisa mas mais lenta. Ã‰ o trade-off clÃ¡ssico!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando diferentes tipos de busca\n",
        "query_teste = \"Desenvolvimento de software com IA\"\n",
        "\n",
        "print(\"ğŸ§ª Comparando diferentes tipos de busca:\\n\")\n",
        "\n",
        "# 1. Busca simples por similaridade\n",
        "print(\"1ï¸âƒ£ BUSCA POR SIMILARIDADE SIMPLES:\")\n",
        "docs_similaridade = vector_store.similarity_search(query_teste, k=2)\n",
        "for i, doc in enumerate(docs_similaridade, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# 2. Busca com score\n",
        "print(\"2ï¸âƒ£ BUSCA COM SCORE:\")\n",
        "docs_com_score = vector_store.similarity_search_with_score(query_teste, k=2)\n",
        "for i, (doc, score) in enumerate(docs_com_score, 1):\n",
        "    print(f\"   {i}. Score: {score:.3f} - {doc.page_content[:50]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# 3. MMR (Maximum Marginal Relevance) - busca diversificada\n",
        "print(\"3ï¸âƒ£ BUSCA COM MMR (diversificada):\")\n",
        "docs_mmr = vector_store.max_marginal_relevance_search(\n",
        "    query_teste, \n",
        "    k=2, \n",
        "    fetch_k=4,  # Busca 4 candidatos internamente\n",
        "    lambda_mult=0.7  # 0.7 = balanceado, 1.0 = sÃ³ relevÃ¢ncia, 0.0 = sÃ³ diversidade\n",
        ")\n",
        "for i, doc in enumerate(docs_mmr, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\nâœ… Viu a diferenÃ§a? MMR tenta trazer resultados relevantes MAS diversos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š AnÃ¡lise de Performance: Medindo a Qualidade\n\nComo saber se nosso Vector Store tÃ¡ funcionando bem? Vamos criar mÃ©tricas!\n\n### ğŸ“ˆ MÃ©tricas Importantes:\n\n1. **Tempo de busca**: Velocidade das consultas\n2. **PrecisÃ£o**: RelevÃ¢ncia dos resultados\n3. **Diversidade**: Variedade dos resultados\n4. **Cobertura**: Capacidade de achar informaÃ§Ã£o relevante\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-07_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos medir a performance do nosso Vector Store!\n",
        "import time\n",
        "\n",
        "def medir_performance_busca(queries, vector_store, k=3, num_testes=5):\n",
        "    \"\"\"Mede tempo e analisa qualidade das buscas\"\"\"\n",
        "    resultados_performance = []\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"â±ï¸ Testando query: '{query}'\")\n",
        "        tempos = []\n",
        "        \n",
        "        # MÃºltiplos testes para mÃ©dia confiÃ¡vel\n",
        "        for i in range(num_testes):\n",
        "            start_time = time.time()\n",
        "            docs = vector_store.similarity_search(query, k=k)\n",
        "            end_time = time.time()\n",
        "            tempos.append(end_time - start_time)\n",
        "        \n",
        "        tempo_medio = np.mean(tempos)\n",
        "        tempo_std = np.std(tempos)\n",
        "        \n",
        "        # AnÃ¡lise de diversidade (categorias diferentes nos resultados)\n",
        "        categorias = [doc.metadata['categoria'] for doc in docs]\n",
        "        diversidade = len(set(categorias)) / len(categorias)\n",
        "        \n",
        "        resultados_performance.append({\n",
        "            'query': query,\n",
        "            'tempo_medio': tempo_medio,\n",
        "            'tempo_std': tempo_std,\n",
        "            'diversidade': diversidade,\n",
        "            'categorias_encontradas': categorias\n",
        "        })\n",
        "        \n",
        "        print(f\"   âš¡ Tempo mÃ©dio: {tempo_medio*1000:.2f}ms (Â±{tempo_std*1000:.2f}ms)\")\n",
        "        print(f\"   ğŸ¯ Diversidade: {diversidade:.2f}\")\n",
        "        print(f\"   ğŸ“‹ Categorias: {categorias}\")\n",
        "        print()\n",
        "    \n",
        "    return resultados_performance\n",
        "\n",
        "# Queries de teste\n",
        "queries_teste = [\n",
        "    \"programaÃ§Ã£o e desenvolvimento\",\n",
        "    \"inteligÃªncia artificial\",\n",
        "    \"cultura brasileira\",\n",
        "    \"aprendizado de mÃ¡quina\"\n",
        "]\n",
        "\n",
        "print(\"ğŸš€ Iniciando testes de performance...\\n\")\n",
        "performance_results = medir_performance_busca(queries_teste, vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualizando os resultados de performance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico 1: Tempo de busca\n",
        "queries = [r['query'] for r in performance_results]\n",
        "tempos = [r['tempo_medio']*1000 for r in performance_results]  # em ms\n",
        "erros = [r['tempo_std']*1000 for r in performance_results]\n",
        "\n",
        "bars1 = ax1.bar(range(len(queries)), tempos, yerr=erros, \n",
        "                capsize=5, alpha=0.7, color='skyblue')\n",
        "ax1.set_title('âš¡ Tempo de Busca por Query', fontsize=12)\n",
        "ax1.set_ylabel('Tempo (ms)')\n",
        "ax1.set_xticks(range(len(queries)))\n",
        "ax1.set_xticklabels([q[:15] + '...' if len(q) > 15 else q for q in queries], \n",
        "                    rotation=45, ha='right')\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars1):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + erros[i]/2,\n",
        "             f'{height:.1f}ms', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# GrÃ¡fico 2: Diversidade dos resultados\n",
        "diversidades = [r['diversidade'] for r in performance_results]\n",
        "bars2 = ax2.bar(range(len(queries)), diversidades, \n",
        "                alpha=0.7, color='lightcoral')\n",
        "ax2.set_title('ğŸ¯ Diversidade dos Resultados', fontsize=12)\n",
        "ax2.set_ylabel('Score de Diversidade (0-1)')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_xticks(range(len(queries)))\n",
        "ax2.set_xticklabels([q[:15] + '...' if len(q) > 15 else q for q in queries], \n",
        "                    rotation=45, ha='right')\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars2):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "             f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# EstatÃ­sticas resumo\n",
        "tempo_total_medio = np.mean([r['tempo_medio'] for r in performance_results])\n",
        "diversidade_media = np.mean([r['diversidade'] for r in performance_results])\n",
        "\n",
        "print(f\"ğŸ“Š RESUMO DA PERFORMANCE:\")\n",
        "print(f\"âš¡ Tempo mÃ©dio de busca: {tempo_total_medio*1000:.2f}ms\")\n",
        "print(f\"ğŸ¯ Diversidade mÃ©dia: {diversidade_media:.2f}\")\n",
        "print(f\"ğŸ’¡ Vector Store funcionando {'ğŸš€ EXCELENTE!' if tempo_total_medio < 0.01 else 'âœ… BEM!'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¾ Salvando e Carregando Vector Stores\n\nE se quisermos salvar nosso Vector Store? NÃ£o queremos ficar recriando embeddings toda hora!\n\n### ğŸ”„ Persistence Strategies:\n\n1. **Local**: Salva em disco (FAISS suporta nativamente)\n2. **Cloud**: AWS S3, Google Cloud Storage\n3. **Database**: PostgreSQL com pgvector, Pinecone, Weaviate\n\n**ğŸ’¡ Dica do Pedro:** Para desenvolvimento, use local. Para produÃ§Ã£o, considere soluÃ§Ãµes cloud!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Salvando nosso Vector Store\n",
        "import os\n",
        "\n",
        "# Criando diretÃ³rio se nÃ£o existir\n",
        "vector_store_path = \"./meu_vector_store\"\n",
        "os.makedirs(vector_store_path, exist_ok=True)\n",
        "\n",
        "print(\"ğŸ’¾ Salvando Vector Store...\")\n",
        "\n",
        "# FAISS permite salvar o Ã­ndice\n",
        "vector_store.save_local(vector_store_path)\n",
        "\n",
        "print(f\"âœ… Vector Store salvo em: {vector_store_path}\")\n",
        "print(\"ğŸ“ Arquivos criados:\")\n",
        "for arquivo in os.listdir(vector_store_path):\n",
        "    tamanho = os.path.getsize(os.path.join(vector_store_path, arquivo))\n",
        "    print(f\"   ğŸ“„ {arquivo}: {tamanho} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Carregando Vector Store salvo\n",
        "print(\"ğŸ“‚ Carregando Vector Store do disco...\")\n",
        "\n",
        "# Carregando o vector store salvo\n",
        "vector_store_carregado = FAISS.load_local(\n",
        "    vector_store_path, \n",
        "    embeddings_model,\n",
        "    allow_dangerous_deserialization=True  # NecessÃ¡rio para arquivos locais\n",
        ")\n",
        "\n",
        "print(\"âœ… Vector Store carregado com sucesso!\")\n",
        "print(f\"ğŸ“Š Documentos no Ã­ndice: {vector_store_carregado.index.ntotal}\")\n",
        "\n",
        "# Testando se funciona\n",
        "print(\"\\nğŸ§ª Testando vector store carregado:\")\n",
        "teste_docs = vector_store_carregado.similarity_search(\"Python programming\", k=2)\n",
        "for i, doc in enumerate(teste_docs, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:50]}...\")\n",
        "\n",
        "print(\"\\nğŸ‰ Liiindo! Funciona perfeitamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‹ï¸â€â™€ï¸ ExercÃ­cio PrÃ¡tico 1: Construa Seu PrÃ³prio Vector Store\n\nAgora Ã© sua vez! Vamos criar um Vector Store com documentos sobre o Brasil.\n\n**ğŸ¯ Seu Desafio:**\n1. Criar 8 documentos sobre diferentes aspectos do Brasil\n2. Construir um Vector Store\n3. Fazer 3 buscas diferentes\n4. Analisar os resultados\n\n**ğŸ’¡ Dica do Pedro:** Pense em categorias como: geografia, cultura, economia, histÃ³ria, esporte, culinÃ¡ria!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# SEU CÃ“DIGO AQUI!\n",
        "# 1. Crie uma lista com 8 Document objects sobre o Brasil\n",
        "# Exemplo de estrutura:\n",
        "\n",
        "meus_documentos_brasil = [\n",
        "    # Document(\n",
        "    #     page_content=\"Sua informaÃ§Ã£o sobre o Brasil aqui...\",\n",
        "    #     metadata={\"categoria\": \"geografia\", \"fonte\": \"exemplo\"}\n",
        "    # ),\n",
        "    # ... adicione mais 7 documentos\n",
        "]\n",
        "\n",
        "# SEU CÃ“DIGO PARA:\n",
        "# 2. Criar o vector store\n",
        "# 3. Fazer 3 buscas diferentes\n",
        "# 4. Mostrar os resultados\n",
        "\n",
        "print(\"ğŸš§ Escreva seu cÃ³digo aqui!\")\n",
        "print(\"ğŸ’ª VocÃª consegue! Bora praticar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¬ ExercÃ­cio PrÃ¡tico 2: Comparando Modelos de Embedding\n\nDiferentes modelos de embedding podem dar resultados diferentes! Vamos comparar.\n\n**ğŸ¯ Seu Desafio:**\n1. Usar dois modelos diferentes (ex: multilingual vs portuguÃªs especÃ­fico)\n2. Fazer a mesma busca com ambos\n3. Comparar os resultados\n4. Analisar qual funciona melhor\n\n**ğŸ’¡ Dica do Pedro:** Teste com frases em portuguÃªs bem brasileiro para ver a diferenÃ§a!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERCÃCIO: Compare dois modelos de embedding\n",
        "\n",
        "# Modelo 1: JÃ¡ temos o multilingual\n",
        "modelo1 = embeddings_model\n",
        "\n",
        "# Modelo 2: Tente outro modelo (descomente uma opÃ§Ã£o):\n",
        "# modelo2 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# modelo2 = HuggingFaceEmbeddings(model_name=\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "# SEU CÃ“DIGO AQUI!\n",
        "# 1. Crie dois vector stores com o mesmo conteÃºdo mas modelos diferentes\n",
        "# 2. FaÃ§a a mesma busca em ambos\n",
        "# 3. Compare os resultados\n",
        "\n",
        "print(\"ğŸ§ª Implemente a comparaÃ§Ã£o de modelos aqui!\")\n",
        "print(\"ğŸ”¬ CiÃªncia de dados na prÃ¡tica!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Conectando com o RAG: Preparando o Terreno\n\nAgora que dominamos Vector Stores, vamos ver como isso se conecta com RAG (Retrieval-Augmented Generation) que veremos no **prÃ³ximo mÃ³dulo**!\n\n### ğŸ”— Como Vector Store + RAG Funcionam Juntos:\n\n```mermaid\ngraph LR\n    A[Pergunta do UsuÃ¡rio] --> B[Vector Store]\n    B --> C[Documentos Relevantes]\n    C --> D[LLM + Contexto]\n    D --> E[Resposta Fundamentada]\n    \n    F[Base de Conhecimento] --> B\n```\n\n**ğŸ’¡ Dica do Pedro:** O Vector Store Ã© o \"cÃ©rebro\" que lembra das informaÃ§Ãµes. O LLM Ã© a \"boca\" que fala baseado no que o cÃ©rebro lembrou!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Preview do RAG - uma pitadinha do que vem por aÃ­!\n",
        "def preview_rag(query, vector_store, top_k=2):\n",
        "    \"\"\"Mini demonstraÃ§Ã£o de como seria um RAG simples\"\"\"\n",
        "    print(f\"ğŸ¤– PREVIEW RAG: '{query}'\\n\")\n",
        "    \n",
        "    # 1. Busca documentos relevantes (Retrieval)\n",
        "    print(\"1ï¸âƒ£ RETRIEVAL - Buscando documentos relevantes...\")\n",
        "    docs_relevantes = vector_store.similarity_search(query, k=top_k)\n",
        "    \n",
        "    contexto = \"\"\n",
        "    for i, doc in enumerate(docs_relevantes, 1):\n",
        "        contexto += f\"Documento {i}: {doc.page_content}\\n\"\n",
        "        print(f\"   ğŸ“„ Doc {i}: {doc.page_content[:60]}...\")\n",
        "    \n",
        "    print(f\"\\n2ï¸âƒ£ AUGMENTATION - Preparando contexto para o LLM...\")\n",
        "    \n",
        "    # 2. Monta prompt com contexto (Augmentation)\n",
        "    prompt_rag = f\"\"\"\n",
        "    Contexto relevante:\n",
        "    {contexto}\n",
        "    \n",
        "    Pergunta: {query}\n",
        "    \n",
        "    Responda baseado APENAS no contexto fornecido:\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"   âœ… Contexto preparado!\")\n",
        "    print(\"\\n3ï¸âƒ£ GENERATION - Aqui entraria o LLM para gerar a resposta...\")\n",
        "    print(\"   ğŸ”® No prÃ³ximo mÃ³dulo implementaremos isso completo!\")\n",
        "    \n",
        "    return prompt_rag, docs_relevantes\n",
        "\n",
        "# Testando nosso preview\n",
        "print(\"ğŸ¬ DemonstraÃ§Ã£o de como serÃ¡ o RAG completo:\\n\")\n",
        "prompt_exemplo, docs_exemplo = preview_rag(\"Como aprender programaÃ§Ã£o?\", vector_store)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸš€ No MÃ³dulo 8, vamos implementar RAG completo com LangChain!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Resumo: O Que Aprendemos Hoje\n\n**Liiindo!** Que jornada incrÃ­vel pelos Vector Stores e Embeddings! ğŸ‰\n\n### âœ… Conceitos Dominados:\n\n1. **ğŸ§  Embeddings**: \"DNA semÃ¢ntico\" das palavras\n   - Vetores multidimensionais que capturam significado\n   - Similaridade cosseno para medir \"parecimento\"\n\n2. **ğŸ—„ï¸ Vector Stores**: Biblioteca organizada da IA\n   - FAISS para busca ultra-rÃ¡pida\n   - Busca semÃ¢ntica vs busca tradicional\n\n3. **ğŸ” Tipos de Busca**:\n   - Similarity Search: Por relevÃ¢ncia\n   - MMR: Balanceando relevÃ¢ncia e diversidade\n   - Com scores: Para anÃ¡lise quantitativa\n\n4. **âš¡ Performance**: Medindo qualidade e velocidade\n\n5. **ğŸ’¾ Persistence**: Salvando e carregando para reutilizar\n\n### ğŸ”— ConexÃµes com o Curso:\n- **MÃ³dulo 6 â¡ï¸ 7**: Document Loading â†’ Vector Storage\n- **MÃ³dulo 7 â¡ï¸ 8**: Vector Storage â†’ RAG Implementation\n\n### ğŸ¯ PrÃ³ximos Passos:\nNo **MÃ³dulo 8**, vamos juntar tudo:\n- Vector Store (âœ… dominado)\n- LLMs (âœ… do MÃ³dulo 2)\n- Prompts (âœ… do MÃ³dulo 3)\n- = **RAG COMPLETO** ğŸš€\n\n**ğŸ’¡ Dica Final do Pedro:** Vector Stores sÃ£o a base de 80% das aplicaÃ§Ãµes de IA moderna. VocÃª acabou de dominar uma das tecnologias mais importantes do momento!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-07_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŠ ParabÃ©ns! VocÃª Ã© Oficialmente um Vector Store Expert!\n\n**ğŸ† Certificado ImaginÃ¡rio:** *\"Pedro Guth certifica que vocÃª domina Vector Stores e estÃ¡ pronto para RAG!\"*\n\n### ğŸš€ Para Casa:\n1. Experimente diferentes modelos de embedding\n2. Teste com seus prÃ³prios documentos\n3. Brinque com os parÃ¢metros de busca\n4. Se prepare para o RAG no prÃ³ximo mÃ³dulo!\n\n### ğŸ“– Leituras Extras (Opcional):\n- DocumentaÃ§Ã£o oficial do FAISS\n- Sentence Transformers documentation\n- Artigos sobre embedding models em portuguÃªs\n\n**Bora para o prÃ³ximo mÃ³dulo implementar RAG completo! ğŸš€**\n\n---\n*\"A diferenÃ§a entre busca tradicional e semÃ¢ntica Ã© como a diferenÃ§a entre procurar pela palavra exata no dicionÃ¡rio vs entender o que a pessoa realmente quer dizer.\"* - Pedro Guth\n\n**#VectorStore #Embeddings #LangChain #RAG #IA**"
      ]
    }
  ]
}