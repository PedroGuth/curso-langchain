{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Vector Store e Embeddings: A Biblioteca da IA Moderna\n\n**M√≥dulo 7 de 15 - LangChain v0.2**\n\n### Por Pedro Nunes Guth\n\n---\n\nT√°, mas o que √© um Vector Store? Imagina que voc√™ tem uma biblioteca GIGANTE com milh√µes de livros, mas n√£o tem um sistema de cataloga√ß√£o. Como voc√™ acharia o livro que precisa? Imposs√≠vel, n√©?\n\n√â exatamente isso que fazemos com informa√ß√µes quando usamos Vector Stores! Transformamos textos em \"impress√µes digitais matem√°ticas\" (embeddings) e organizamos tudo de forma que a IA consiga achar rapidinho o que precisa.\n\nNos m√≥dulos anteriores, vimos como carregar e dividir documentos. Agora vamos aprender como fazer a IA \"entender\" e organizar essas informa√ß√µes para usar no RAG (que vem no pr√≥ximo m√≥dulo)!\n\n**üéØ O que vamos ver:**\n- O que s√£o embeddings na pr√°tica\n- Como funciona um Vector Store\n- Implementa√ß√£o com LangChain v0.2\n- Similaridade sem√¢ntica\n- Prepara√ß√£o para RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Mas Afinal, O Que S√£o Embeddings?\n\nT√°, vou explicar com uma analogia que todo brasileiro entende: **DNA das palavras**!\n\nSabe como cada pessoa tem um DNA √∫nico que define suas caracter√≠sticas? Embeddings s√£o como o \"DNA sem√¢ntico\" das palavras e frases!\n\n### üìä Matematicamente Falando:\n\nUm embedding √© uma representa√ß√£o vetorial densa de texto em um espa√ßo multidimensional (geralmente 768, 1024 ou 1536 dimens√µes).\n\n$$\\text{embedding}(\\text{\"cachorro\"}) = [0.2, -0.1, 0.8, ..., 0.3] \\in \\mathbb{R}^n$$\n\nonde $n$ √© o n√∫mero de dimens√µes do modelo.\n\n### üß™ A M√°gica da Similaridade:\n\nPalavras com significados similares t√™m embeddings pr√≥ximos no espa√ßo vetorial:\n\n$$\\text{similaridade}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{||v_1|| \\cdot ||v_2||}$$\n\nIsso significa que \"cachorro\" e \"c√£o\" ter√£o vetores bem pr√≥ximos!\n\n**üí° Dica do Pedro:** Pensa assim: se as palavras fossem pessoas numa festa, as que t√™m assuntos em comum ficariam conversando perto umas das outras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Bora instalar as depend√™ncias!\n",
        "!pip install langchain langchain-community langchain-google-genai\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "!pip install numpy matplotlib seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports essenciais para nosso laborat√≥rio de embeddings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangChain imports - a nova gera√ß√£o!\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"üöÄ Todas as bibliotecas carregadas! Bora para a aventura dos vetores!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configurando nossa chave da Google (voc√™ vai precisar da sua!)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Se estiver no Colab, use:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Se estiver local, descomente e coloque sua chave:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_aqui\"\n",
        "\n",
        "print(\"üîê Chaves configuradas! Vamos embeddar tudo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Primeiro Contato: Criando Nossos Primeiros Embeddings\n\nVamos come√ßar com o b√°sico: transformar texto em vetores! √â como traduzir portugu√™s para \"matematiqu√™s\" que a IA entende.\n\n### üîß Tipos de Modelos de Embedding:\n\n1. **Google Embeddings**: Integrados com Gemini\n2. **Sentence Transformers**: Gratuitos e poderosos\n3. **OpenAI Embeddings**: Caros mas eficientes\n\n**üí° Dica do Pedro:** Para estudar, use Sentence Transformers. Para produ√ß√£o s√©ria, Google ou OpenAI!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar nosso primeiro modelo de embedding!\n",
        "# Usando um modelo gratuito para come√ßar\n",
        "\n",
        "# Modelo brasileiro que funciona bem em portugu√™s!\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'}  # Usa CPU mesmo\n",
        ")\n",
        "\n",
        "print(\"üéâ Modelo de embedding carregado!\")\n",
        "print(f\"üìä Dimens√µes do vetor: Vamos descobrir...\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando nosso modelo com frases brasileiras!\n",
        "frases_teste = [\n",
        "    \"Eu amo programar em Python\",\n",
        "    \"Adoro desenvolver com Python\", \n",
        "    \"Python √© uma linguagem incr√≠vel\",\n",
        "    \"Hoje est√° chovendo muito\",\n",
        "    \"O clima est√° chuvoso\",\n",
        "    \"Vou comer pizza hoje\"\n",
        "]\n",
        "\n",
        "# Gerando embeddings para cada frase\n",
        "print(\"üß† Gerando embeddings...\")\n",
        "embeddings_frases = []\n",
        "\n",
        "for i, frase in enumerate(frases_teste):\n",
        "    embedding = embeddings_model.embed_query(frase)\n",
        "    embeddings_frases.append(embedding)\n",
        "    print(f\"üìù Frase {i+1}: '{frase}'\")\n",
        "    print(f\"üî¢ Dimens√µes: {len(embedding)}\")\n",
        "    print(f\"üìä Primeiros 5 valores: {embedding[:5]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"‚úÖ Todos os embeddings gerados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualizando a M√°gica dos Embeddings\n\nT√°, mas como visualizar 384 dimens√µes? √â imposs√≠vel! Mas temos um truque: **PCA (An√°lise de Componentes Principais)**.\n\n√â como pegar uma foto 3D e fazer uma sombra 2D na parede - perdemos informa√ß√£o, mas conseguimos \"ver\" as rela√ß√µes!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-07_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos visualizar nossos embeddings em 2D!\n",
        "# √â como fazer uma \"radiografia\" dos vetores\n",
        "\n",
        "# Convertendo para numpy array para facilitar\n",
        "embeddings_array = np.array(embeddings_frases)\n",
        "print(f\"üìä Shape dos embeddings: {embeddings_array.shape}\")\n",
        "\n",
        "# Aplicando PCA para reduzir para 2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d = pca.fit_transform(embeddings_array)\n",
        "\n",
        "print(f\"üìâ Vari√¢ncia explicada: {pca.explained_variance_ratio_}\")\n",
        "print(f\"üìä Total de vari√¢ncia capturada: {sum(pca.explained_variance_ratio_):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando o gr√°fico dos embeddings - a visualiza√ß√£o da m√°gica!\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Cores diferentes para temas diferentes\n",
        "cores = ['red', 'red', 'red', 'blue', 'blue', 'green']\n",
        "temas = ['Python', 'Python', 'Python', 'Clima', 'Clima', 'Comida']\n",
        "\n",
        "# Plotando os pontos\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    plt.scatter(x, y, c=cores[i], s=100, alpha=0.7)\n",
        "    plt.annotate(f\"{i+1}: {temas[i]}\", \n",
        "                xy=(x, y), \n",
        "                xytext=(5, 5), \n",
        "                textcoords='offset points',\n",
        "                fontsize=10,\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor=cores[i], alpha=0.3))\n",
        "\n",
        "plt.title('üéØ Visualiza√ß√£o dos Embeddings - Frases Similares Ficam Juntas!', fontsize=14)\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Legenda\n",
        "plt.scatter([], [], c='red', label='Programa√ß√£o Python', s=100)\n",
        "plt.scatter([], [], c='blue', label='Clima/Chuva', s=100)\n",
        "plt.scatter([], [], c='green', label='Comida', s=100)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üé® Liiindo! Viu como as frases similares ficam pr√≥ximas?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Calculando Similaridades: A Matem√°tica do \"Parecido\"\n\nAgora vamos calcular exatamente o qu√£o \"parecidas\" s√£o nossas frases usando **similaridade cosseno**.\n\n### üìê F√≥rmula da Similaridade Cosseno:\n\n$$\\cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n\n**Onde:**\n- 1.0 = Id√™nticos\n- 0.0 = Sem rela√ß√£o  \n- -1.0 = Opostos\n\n**üí° Dica do Pedro:** √â como medir o √¢ngulo entre dois vetores. Quanto menor o √¢ngulo, mais parecidos!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Calculando a matriz de similaridade - quem √© parecido com quem?\n",
        "similaridade_matrix = cosine_similarity(embeddings_array)\n",
        "\n",
        "print(\"üßÆ Matriz de Similaridade Cosseno:\")\n",
        "print(\"(1.0 = id√™nticos, 0.0 = sem rela√ß√£o)\\n\")\n",
        "\n",
        "# Criando labels mais leg√≠veis\n",
        "labels = [\n",
        "    \"Amo Python\",\n",
        "    \"Adoro Python\", \n",
        "    \"Python incr√≠vel\",\n",
        "    \"Chovendo muito\",\n",
        "    \"Clima chuvoso\",\n",
        "    \"Comer pizza\"\n",
        "]\n",
        "\n",
        "# Mostrando a matriz bonitinha\n",
        "for i, frase1 in enumerate(labels):\n",
        "    for j, frase2 in enumerate(labels):\n",
        "        if i <= j:  # S√≥ mostra a parte superior da matriz\n",
        "            score = similaridade_matrix[i][j]\n",
        "            emoji = \"üî•\" if score > 0.8 and i != j else \"‚úÖ\" if score > 0.6 and i != j else \"‚û°Ô∏è\" if i == j else \"‚ùå\"\n",
        "            print(f\"{emoji} '{frase1}' vs '{frase2}': {score:.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando um heatmap da similaridade - visual e bonito!\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Heatmap com seaborn\n",
        "sns.heatmap(similaridade_matrix, \n",
        "            xticklabels=labels,\n",
        "            yticklabels=labels,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='RdYlBu_r',\n",
        "            center=0.5,\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Similaridade Cosseno'})\n",
        "\n",
        "plt.title('üî• Mapa de Calor da Similaridade\\nQuanto mais vermelho, mais parecido!', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üé® Agora d√° pra ver claramente quem √© parecido com quem!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÑÔ∏è Vector Stores: A Biblioteca Organizada da IA\n\nT√°, mas e se tivermos milh√µes de documentos? N√£o d√° pra calcular similaridade com tudo na m√£o!\n\nEntra o **Vector Store** - √© como uma biblioteca super organizada onde:\n- üìö Cada livro (documento) tem sua \"impress√£o digital\" (embedding)\n- üîç Quando voc√™ procura algo, ele acha os mais parecidos rapidinho\n- ‚ö° Usa algoritmos especiais (como FAISS) para busca ultra-r√°pida\n\n### üèóÔ∏è Como Funciona um Vector Store:\n\n```mermaid\ngraph TD\n    A[Documento Original] --> B[Gerar Embedding]\n    B --> C[Armazenar no Vector Store]\n    D[Query do Usu√°rio] --> E[Gerar Embedding da Query]\n    E --> F[Buscar Similares no Vector Store]\n    F --> G[Retornar Documentos Mais Relevantes]\n    C --> F\n```\n\n**üí° Dica do Pedro:** √â como o Spotify que recomenda m√∫sicas parecidas com o que voc√™ gosta, mas para textos!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar nosso primeiro Vector Store!\n",
        "# Usando FAISS - Facebook AI Similarity Search (gratuito e r√°pido)\n",
        "\n",
        "# Primeiro, criamos documentos estruturados (lembra do m√≥dulo 6?)\n",
        "documentos_exemplo = [\n",
        "    Document(\n",
        "        page_content=\"Python √© uma linguagem de programa√ß√£o interpretada de alto n√≠vel.\",\n",
        "        metadata={\"fonte\": \"wikipedia\", \"categoria\": \"programa√ß√£o\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"LangChain √© um framework para desenvolver aplica√ß√µes com modelos de linguagem.\",\n",
        "        metadata={\"fonte\": \"docs\", \"categoria\": \"programa√ß√£o\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Intelig√™ncia artificial est√° revolucionando a tecnologia moderna.\",\n",
        "        metadata={\"fonte\": \"artigo\", \"categoria\": \"tecnologia\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Machine learning permite que computadores aprendam sem programa√ß√£o expl√≠cita.\",\n",
        "        metadata={\"fonte\": \"livro\", \"categoria\": \"tecnologia\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"O futebol √© o esporte mais popular do Brasil.\",\n",
        "        metadata={\"fonte\": \"esporte\", \"categoria\": \"esporte\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"A culin√°ria brasileira √© muito diversificada e saborosa.\",\n",
        "        metadata={\"fonte\": \"gastronomia\", \"categoria\": \"cultura\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"üìö Criamos {len(documentos_exemplo)} documentos para nosso vector store!\")\n",
        "print(\"\\nüìã Resumo dos documentos:\")\n",
        "for i, doc in enumerate(documentos_exemplo):\n",
        "    print(f\"{i+1}. {doc.page_content[:50]}... [{doc.metadata['categoria']}]\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Criando nosso Vector Store com FAISS\n",
        "print(\"üèóÔ∏è Construindo o Vector Store...\")\n",
        "print(\"üìä Gerando embeddings para todos os documentos...\")\n",
        "\n",
        "# FAISS.from_documents faz toda a m√°gica:\n",
        "# 1. Gera embeddings para cada documento\n",
        "# 2. Cria o √≠ndice FAISS\n",
        "# 3. Armazena tudo organizadinho\n",
        "vector_store = FAISS.from_documents(\n",
        "    documents=documentos_exemplo,\n",
        "    embedding=embeddings_model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vector Store criado com sucesso!\")\n",
        "print(f\"üìà N√∫mero de documentos indexados: {vector_store.index.ntotal}\")\n",
        "print(\"üöÄ Agora podemos fazer buscas sem√¢nticas ultra-r√°pidas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Busca Sem√¢ntica: A M√°gica Acontece Aqui!\n\nAgora vem a parte mais **LINDA** do Vector Store: busca sem√¢ntica!\n\nDiferente de busca tradicional (que procura palavras exatas), a busca sem√¢ntica entende o **significado**!\n\n### üÜö Compara√ß√£o:\n- **Busca Tradicional:** \"cachorro\" ‚â† \"c√£o\" \n- **Busca Sem√¢ntica:** \"cachorro\" ‚âà \"c√£o\" ‚âà \"pet\" ‚âà \"animal dom√©stico\"\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-07_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos testar nossa busca sem√¢ntica!\n",
        "def busca_semantica(query, k=3):\n",
        "    \"\"\"Fun√ß√£o para fazer busca sem√¢ntica e mostrar resultados bonitinhos\"\"\"\n",
        "    print(f\"üîç Buscando por: '{query}'\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Busca por similaridade\n",
        "    resultados = vector_store.similarity_search_with_score(query, k=k)\n",
        "    \n",
        "    for i, (doc, score) in enumerate(resultados, 1):\n",
        "        # Convertendo score do FAISS (dist√¢ncia) para similaridade\n",
        "        similaridade = 1 / (1 + score)  # Quanto menor a dist√¢ncia, maior a similaridade\n",
        "        \n",
        "        print(f\"üèÜ Resultado {i} - Similaridade: {similaridade:.3f}\")\n",
        "        print(f\"üìù Conte√∫do: {doc.page_content}\")\n",
        "        print(f\"üè∑Ô∏è Categoria: {doc.metadata['categoria']}\")\n",
        "        print(f\"üìä Score FAISS: {score:.3f}\")\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "print(\"üéØ Fun√ß√£o de busca sem√¢ntica pronta! Vamos testar...\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 1: Buscando sobre programa√ß√£o\n",
        "resultados1 = busca_semantica(\"Como programar em linguagens de c√≥digo?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 2: Buscando sobre IA/tecnologia\n",
        "resultados2 = busca_semantica(\"Algoritmos que aprendem sozinhos\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Teste 3: Buscando sobre cultura brasileira\n",
        "resultados3 = busca_semantica(\"Tradi√ß√µes e comida do Brasil\", k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configura√ß√µes Avan√ßadas do Vector Store\n\nBora turbinar nosso Vector Store! Tem v√°rias configura√ß√µes que fazem diferen√ßa na qualidade da busca.\n\n### üéõÔ∏è Par√¢metros Importantes:\n\n1. **k**: Quantos resultados retornar\n2. **fetch_k**: Quantos candidatos buscar internamente (sempre >= k)\n3. **lambda_mult**: Para MMR (Maximum Marginal Relevance) - diversidade vs relev√¢ncia\n4. **score_threshold**: Filtro por score m√≠nimo\n\n**üí° Dica do Pedro:** Fetch_k maior = busca mais precisa mas mais lenta. √â o trade-off cl√°ssico!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando diferentes tipos de busca\n",
        "query_teste = \"Desenvolvimento de software com IA\"\n",
        "\n",
        "print(\"üß™ Comparando diferentes tipos de busca:\\n\")\n",
        "\n",
        "# 1. Busca simples por similaridade\n",
        "print(\"1Ô∏è‚É£ BUSCA POR SIMILARIDADE SIMPLES:\")\n",
        "docs_similaridade = vector_store.similarity_search(query_teste, k=2)\n",
        "for i, doc in enumerate(docs_similaridade, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# 2. Busca com score\n",
        "print(\"2Ô∏è‚É£ BUSCA COM SCORE:\")\n",
        "docs_com_score = vector_store.similarity_search_with_score(query_teste, k=2)\n",
        "for i, (doc, score) in enumerate(docs_com_score, 1):\n",
        "    print(f\"   {i}. Score: {score:.3f} - {doc.page_content[:50]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# 3. MMR (Maximum Marginal Relevance) - busca diversificada\n",
        "print(\"3Ô∏è‚É£ BUSCA COM MMR (diversificada):\")\n",
        "docs_mmr = vector_store.max_marginal_relevance_search(\n",
        "    query_teste, \n",
        "    k=2, \n",
        "    fetch_k=4,  # Busca 4 candidatos internamente\n",
        "    lambda_mult=0.7  # 0.7 = balanceado, 1.0 = s√≥ relev√¢ncia, 0.0 = s√≥ diversidade\n",
        ")\n",
        "for i, doc in enumerate(docs_mmr, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ Viu a diferen√ßa? MMR tenta trazer resultados relevantes MAS diversos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise de Performance: Medindo a Qualidade\n\nComo saber se nosso Vector Store t√° funcionando bem? Vamos criar m√©tricas!\n\n### üìà M√©tricas Importantes:\n\n1. **Tempo de busca**: Velocidade das consultas\n2. **Precis√£o**: Relev√¢ncia dos resultados\n3. **Diversidade**: Variedade dos resultados\n4. **Cobertura**: Capacidade de achar informa√ß√£o relevante\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-07_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos medir a performance do nosso Vector Store!\n",
        "import time\n",
        "\n",
        "def medir_performance_busca(queries, vector_store, k=3, num_testes=5):\n",
        "    \"\"\"Mede tempo e analisa qualidade das buscas\"\"\"\n",
        "    resultados_performance = []\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"‚è±Ô∏è Testando query: '{query}'\")\n",
        "        tempos = []\n",
        "        \n",
        "        # M√∫ltiplos testes para m√©dia confi√°vel\n",
        "        for i in range(num_testes):\n",
        "            start_time = time.time()\n",
        "            docs = vector_store.similarity_search(query, k=k)\n",
        "            end_time = time.time()\n",
        "            tempos.append(end_time - start_time)\n",
        "        \n",
        "        tempo_medio = np.mean(tempos)\n",
        "        tempo_std = np.std(tempos)\n",
        "        \n",
        "        # An√°lise de diversidade (categorias diferentes nos resultados)\n",
        "        categorias = [doc.metadata['categoria'] for doc in docs]\n",
        "        diversidade = len(set(categorias)) / len(categorias)\n",
        "        \n",
        "        resultados_performance.append({\n",
        "            'query': query,\n",
        "            'tempo_medio': tempo_medio,\n",
        "            'tempo_std': tempo_std,\n",
        "            'diversidade': diversidade,\n",
        "            'categorias_encontradas': categorias\n",
        "        })\n",
        "        \n",
        "        print(f\"   ‚ö° Tempo m√©dio: {tempo_medio*1000:.2f}ms (¬±{tempo_std*1000:.2f}ms)\")\n",
        "        print(f\"   üéØ Diversidade: {diversidade:.2f}\")\n",
        "        print(f\"   üìã Categorias: {categorias}\")\n",
        "        print()\n",
        "    \n",
        "    return resultados_performance\n",
        "\n",
        "# Queries de teste\n",
        "queries_teste = [\n",
        "    \"programa√ß√£o e desenvolvimento\",\n",
        "    \"intelig√™ncia artificial\",\n",
        "    \"cultura brasileira\",\n",
        "    \"aprendizado de m√°quina\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ Iniciando testes de performance...\\n\")\n",
        "performance_results = medir_performance_busca(queries_teste, vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualizando os resultados de performance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Tempo de busca\n",
        "queries = [r['query'] for r in performance_results]\n",
        "tempos = [r['tempo_medio']*1000 for r in performance_results]  # em ms\n",
        "erros = [r['tempo_std']*1000 for r in performance_results]\n",
        "\n",
        "bars1 = ax1.bar(range(len(queries)), tempos, yerr=erros, \n",
        "                capsize=5, alpha=0.7, color='skyblue')\n",
        "ax1.set_title('‚ö° Tempo de Busca por Query', fontsize=12)\n",
        "ax1.set_ylabel('Tempo (ms)')\n",
        "ax1.set_xticks(range(len(queries)))\n",
        "ax1.set_xticklabels([q[:15] + '...' if len(q) > 15 else q for q in queries], \n",
        "                    rotation=45, ha='right')\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars1):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + erros[i]/2,\n",
        "             f'{height:.1f}ms', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Gr√°fico 2: Diversidade dos resultados\n",
        "diversidades = [r['diversidade'] for r in performance_results]\n",
        "bars2 = ax2.bar(range(len(queries)), diversidades, \n",
        "                alpha=0.7, color='lightcoral')\n",
        "ax2.set_title('üéØ Diversidade dos Resultados', fontsize=12)\n",
        "ax2.set_ylabel('Score de Diversidade (0-1)')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_xticks(range(len(queries)))\n",
        "ax2.set_xticklabels([q[:15] + '...' if len(q) > 15 else q for q in queries], \n",
        "                    rotation=45, ha='right')\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for i, bar in enumerate(bars2):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "             f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Estat√≠sticas resumo\n",
        "tempo_total_medio = np.mean([r['tempo_medio'] for r in performance_results])\n",
        "diversidade_media = np.mean([r['diversidade'] for r in performance_results])\n",
        "\n",
        "print(f\"üìä RESUMO DA PERFORMANCE:\")\n",
        "print(f\"‚ö° Tempo m√©dio de busca: {tempo_total_medio*1000:.2f}ms\")\n",
        "print(f\"üéØ Diversidade m√©dia: {diversidade_media:.2f}\")\n",
        "print(f\"üí° Vector Store funcionando {'üöÄ EXCELENTE!' if tempo_total_medio < 0.01 else '‚úÖ BEM!'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Salvando e Carregando Vector Stores\n\nE se quisermos salvar nosso Vector Store? N√£o queremos ficar recriando embeddings toda hora!\n\n### üîÑ Persistence Strategies:\n\n1. **Local**: Salva em disco (FAISS suporta nativamente)\n2. **Cloud**: AWS S3, Google Cloud Storage\n3. **Database**: PostgreSQL com pgvector, Pinecone, Weaviate\n\n**üí° Dica do Pedro:** Para desenvolvimento, use local. Para produ√ß√£o, considere solu√ß√µes cloud!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Salvando nosso Vector Store\n",
        "import os\n",
        "\n",
        "# Criando diret√≥rio se n√£o existir\n",
        "vector_store_path = \"./meu_vector_store\"\n",
        "os.makedirs(vector_store_path, exist_ok=True)\n",
        "\n",
        "print(\"üíæ Salvando Vector Store...\")\n",
        "\n",
        "# FAISS permite salvar o √≠ndice\n",
        "vector_store.save_local(vector_store_path)\n",
        "\n",
        "print(f\"‚úÖ Vector Store salvo em: {vector_store_path}\")\n",
        "print(\"üìÅ Arquivos criados:\")\n",
        "for arquivo in os.listdir(vector_store_path):\n",
        "    tamanho = os.path.getsize(os.path.join(vector_store_path, arquivo))\n",
        "    print(f\"   üìÑ {arquivo}: {tamanho} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Carregando Vector Store salvo\n",
        "print(\"üìÇ Carregando Vector Store do disco...\")\n",
        "\n",
        "# Carregando o vector store salvo\n",
        "vector_store_carregado = FAISS.load_local(\n",
        "    vector_store_path, \n",
        "    embeddings_model,\n",
        "    allow_dangerous_deserialization=True  # Necess√°rio para arquivos locais\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vector Store carregado com sucesso!\")\n",
        "print(f\"üìä Documentos no √≠ndice: {vector_store_carregado.index.ntotal}\")\n",
        "\n",
        "# Testando se funciona\n",
        "print(\"\\nüß™ Testando vector store carregado:\")\n",
        "teste_docs = vector_store_carregado.similarity_search(\"Python programming\", k=2)\n",
        "for i, doc in enumerate(teste_docs, 1):\n",
        "    print(f\"   {i}. {doc.page_content[:50]}...\")\n",
        "\n",
        "print(\"\\nüéâ Liiindo! Funciona perfeitamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è‚Äç‚ôÄÔ∏è Exerc√≠cio Pr√°tico 1: Construa Seu Pr√≥prio Vector Store\n\nAgora √© sua vez! Vamos criar um Vector Store com documentos sobre o Brasil.\n\n**üéØ Seu Desafio:**\n1. Criar 8 documentos sobre diferentes aspectos do Brasil\n2. Construir um Vector Store\n3. Fazer 3 buscas diferentes\n4. Analisar os resultados\n\n**üí° Dica do Pedro:** Pense em categorias como: geografia, cultura, economia, hist√≥ria, esporte, culin√°ria!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# SEU C√ìDIGO AQUI!\n",
        "# 1. Crie uma lista com 8 Document objects sobre o Brasil\n",
        "# Exemplo de estrutura:\n",
        "\n",
        "meus_documentos_brasil = [\n",
        "    # Document(\n",
        "    #     page_content=\"Sua informa√ß√£o sobre o Brasil aqui...\",\n",
        "    #     metadata={\"categoria\": \"geografia\", \"fonte\": \"exemplo\"}\n",
        "    # ),\n",
        "    # ... adicione mais 7 documentos\n",
        "]\n",
        "\n",
        "# SEU C√ìDIGO PARA:\n",
        "# 2. Criar o vector store\n",
        "# 3. Fazer 3 buscas diferentes\n",
        "# 4. Mostrar os resultados\n",
        "\n",
        "print(\"üöß Escreva seu c√≥digo aqui!\")\n",
        "print(\"üí™ Voc√™ consegue! Bora praticar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Exerc√≠cio Pr√°tico 2: Comparando Modelos de Embedding\n\nDiferentes modelos de embedding podem dar resultados diferentes! Vamos comparar.\n\n**üéØ Seu Desafio:**\n1. Usar dois modelos diferentes (ex: multilingual vs portugu√™s espec√≠fico)\n2. Fazer a mesma busca com ambos\n3. Comparar os resultados\n4. Analisar qual funciona melhor\n\n**üí° Dica do Pedro:** Teste com frases em portugu√™s bem brasileiro para ver a diferen√ßa!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EXERC√çCIO: Compare dois modelos de embedding\n",
        "\n",
        "# Modelo 1: J√° temos o multilingual\n",
        "modelo1 = embeddings_model\n",
        "\n",
        "# Modelo 2: Tente outro modelo (descomente uma op√ß√£o):\n",
        "# modelo2 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# modelo2 = HuggingFaceEmbeddings(model_name=\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "# SEU C√ìDIGO AQUI!\n",
        "# 1. Crie dois vector stores com o mesmo conte√∫do mas modelos diferentes\n",
        "# 2. Fa√ßa a mesma busca em ambos\n",
        "# 3. Compare os resultados\n",
        "\n",
        "print(\"üß™ Implemente a compara√ß√£o de modelos aqui!\")\n",
        "print(\"üî¨ Ci√™ncia de dados na pr√°tica!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Conectando com o RAG: Preparando o Terreno\n\nAgora que dominamos Vector Stores, vamos ver como isso se conecta com RAG (Retrieval-Augmented Generation) que veremos no **pr√≥ximo m√≥dulo**!\n\n### üîó Como Vector Store + RAG Funcionam Juntos:\n\n```mermaid\ngraph LR\n    A[Pergunta do Usu√°rio] --> B[Vector Store]\n    B --> C[Documentos Relevantes]\n    C --> D[LLM + Contexto]\n    D --> E[Resposta Fundamentada]\n    \n    F[Base de Conhecimento] --> B\n```\n\n**üí° Dica do Pedro:** O Vector Store √© o \"c√©rebro\" que lembra das informa√ß√µes. O LLM √© a \"boca\" que fala baseado no que o c√©rebro lembrou!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Preview do RAG - uma pitadinha do que vem por a√≠!\n",
        "def preview_rag(query, vector_store, top_k=2):\n",
        "    \"\"\"Mini demonstra√ß√£o de como seria um RAG simples\"\"\"\n",
        "    print(f\"ü§ñ PREVIEW RAG: '{query}'\\n\")\n",
        "    \n",
        "    # 1. Busca documentos relevantes (Retrieval)\n",
        "    print(\"1Ô∏è‚É£ RETRIEVAL - Buscando documentos relevantes...\")\n",
        "    docs_relevantes = vector_store.similarity_search(query, k=top_k)\n",
        "    \n",
        "    contexto = \"\"\n",
        "    for i, doc in enumerate(docs_relevantes, 1):\n",
        "        contexto += f\"Documento {i}: {doc.page_content}\\n\"\n",
        "        print(f\"   üìÑ Doc {i}: {doc.page_content[:60]}...\")\n",
        "    \n",
        "    print(f\"\\n2Ô∏è‚É£ AUGMENTATION - Preparando contexto para o LLM...\")\n",
        "    \n",
        "    # 2. Monta prompt com contexto (Augmentation)\n",
        "    prompt_rag = f\"\"\"\n",
        "    Contexto relevante:\n",
        "    {contexto}\n",
        "    \n",
        "    Pergunta: {query}\n",
        "    \n",
        "    Responda baseado APENAS no contexto fornecido:\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"   ‚úÖ Contexto preparado!\")\n",
        "    print(\"\\n3Ô∏è‚É£ GENERATION - Aqui entraria o LLM para gerar a resposta...\")\n",
        "    print(\"   üîÆ No pr√≥ximo m√≥dulo implementaremos isso completo!\")\n",
        "    \n",
        "    return prompt_rag, docs_relevantes\n",
        "\n",
        "# Testando nosso preview\n",
        "print(\"üé¨ Demonstra√ß√£o de como ser√° o RAG completo:\\n\")\n",
        "prompt_exemplo, docs_exemplo = preview_rag(\"Como aprender programa√ß√£o?\", vector_store)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ No M√≥dulo 8, vamos implementar RAG completo com LangChain!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Resumo: O Que Aprendemos Hoje\n\n**Liiindo!** Que jornada incr√≠vel pelos Vector Stores e Embeddings! üéâ\n\n### ‚úÖ Conceitos Dominados:\n\n1. **üß† Embeddings**: \"DNA sem√¢ntico\" das palavras\n   - Vetores multidimensionais que capturam significado\n   - Similaridade cosseno para medir \"parecimento\"\n\n2. **üóÑÔ∏è Vector Stores**: Biblioteca organizada da IA\n   - FAISS para busca ultra-r√°pida\n   - Busca sem√¢ntica vs busca tradicional\n\n3. **üîç Tipos de Busca**:\n   - Similarity Search: Por relev√¢ncia\n   - MMR: Balanceando relev√¢ncia e diversidade\n   - Com scores: Para an√°lise quantitativa\n\n4. **‚ö° Performance**: Medindo qualidade e velocidade\n\n5. **üíæ Persistence**: Salvando e carregando para reutilizar\n\n### üîó Conex√µes com o Curso:\n- **M√≥dulo 6 ‚û°Ô∏è 7**: Document Loading ‚Üí Vector Storage\n- **M√≥dulo 7 ‚û°Ô∏è 8**: Vector Storage ‚Üí RAG Implementation\n\n### üéØ Pr√≥ximos Passos:\nNo **M√≥dulo 8**, vamos juntar tudo:\n- Vector Store (‚úÖ dominado)\n- LLMs (‚úÖ do M√≥dulo 2)\n- Prompts (‚úÖ do M√≥dulo 3)\n- = **RAG COMPLETO** üöÄ\n\n**üí° Dica Final do Pedro:** Vector Stores s√£o a base de 80% das aplica√ß√µes de IA moderna. Voc√™ acabou de dominar uma das tecnologias mais importantes do momento!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-vers√£o-v0.2-modulo-07_img_04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Parab√©ns! Voc√™ √© Oficialmente um Vector Store Expert!\n\n**üèÜ Certificado Imagin√°rio:** *\"Pedro Guth certifica que voc√™ domina Vector Stores e est√° pronto para RAG!\"*\n\n### üöÄ Para Casa:\n1. Experimente diferentes modelos de embedding\n2. Teste com seus pr√≥prios documentos\n3. Brinque com os par√¢metros de busca\n4. Se prepare para o RAG no pr√≥ximo m√≥dulo!\n\n### üìñ Leituras Extras (Opcional):\n- Documenta√ß√£o oficial do FAISS\n- Sentence Transformers documentation\n- Artigos sobre embedding models em portugu√™s\n\n**Bora para o pr√≥ximo m√≥dulo implementar RAG completo! üöÄ**\n\n---\n*\"A diferen√ßa entre busca tradicional e sem√¢ntica √© como a diferen√ßa entre procurar pela palavra exata no dicion√°rio vs entender o que a pessoa realmente quer dizer.\"* - Pedro Guth\n\n**#VectorStore #Embeddings #LangChain #RAG #IA**"
      ]
    }
  ]
}