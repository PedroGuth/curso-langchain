{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Assistente de Pesquisa Inteligente com RAG e Agents\n\n## Pedro Nunes Guth - MÃ³dulo 10: Projeto Final 1\n\nEaÃ­ pessoal! ğŸ‰\n\nChegamos no **MÃ³dulo 10** e tÃ¡ na hora de botar a mÃ£o na massa! Vamos criar um projeto que vai juntar **TUDO** que aprendemos atÃ© agora. \n\nImagina ter um assistente que nÃ£o sÃ³ conversa contigo, mas tambÃ©m:\n- ğŸ“š Pesquisa em documentos (RAG)\n- ğŸŒ Busca informaÃ§Ãµes na internet (Agents)\n- ğŸ§  Lembra do que vocÃªs jÃ¡ conversaram (Memory)\n- ğŸ¯ Responde de forma estruturada (Output Parsers)\n\nÃ‰ como ter um estagiÃ¡rio super inteligente que nunca esquece de nada e ainda pesquisa tudo pra vocÃª!\n\n**Dica do Pedro**: Este projeto vai servir de base para o Projeto Final 2 e o deploy no Streamlit. EntÃ£o capricha!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ O que vamos construir?\n\nNosso **Assistente de Pesquisa Inteligente** vai ter:\n\n### Funcionalidades:\n1. **RAG System**: Consulta documentos locais\n2. **Web Agent**: Pesquisa informaÃ§Ãµes online\n3. **Memory**: MantÃ©m contexto da conversa\n4. **Router**: Decide qual ferramenta usar\n5. **Output Parsing**: Formata respostas estruturadas\n\n### Arquitetura:\n```mermaid\ngraph TD\n    A[UsuÃ¡rio] --> B[Router Agent]\n    B --> C{Tipo de Pergunta?}\n    C -->|Documentos| D[RAG System]\n    C -->|Web Search| E[Web Agent]\n    C -->|Conversa| F[Chat Agent]\n    D --> G[Memory + Response]\n    E --> G\n    F --> G\n    G --> H[Output Parser]\n    H --> I[Resposta Estruturada]\n```\n\nTÃ¡, mas por que essa arquitetura? Porque na vida real, um assistente precisa saber **quando** usar **qual** ferramenta. Ã‰ como um canivete suÃ­Ã§o inteligente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos instalar todas as dependÃªncias necessÃ¡rias\n",
        "!pip install -q langchain langchain-google-genai langchain-community\n",
        "!pip install -q faiss-cpu pypdf sentence-transformers\n",
        "!pip install -q duckduckgo-search wikipedia-api requests beautifulsoup4\n",
        "!pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necessÃ¡rios - Tudo que aprendemos nos mÃ³dulos anteriores!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangChain Core\n",
        "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser, StructuredOutputParser\n",
        "from langchain.schema.output_parser import OutputParserException\n",
        "\n",
        "# Chat Model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# RAG Components\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Agents and Tools\n",
        "from langchain.agents import AgentType, initialize_agent, Tool\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# Utils\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ“¦ Imports carregados! Bora pro cÃ³digo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ConfiguraÃ§Ã£o da API do Google Gemini\n",
        "# Lembre-se de configurar sua GOOGLE_API_KEY\n",
        "load_dotenv()\n",
        "\n",
        "# Se vocÃª nÃ£o tem .env, descomente e coloque sua chave aqui:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_aqui\"\n",
        "\n",
        "# Inicializando nosso modelo principal - o Gemini 2.0 Flash\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"ğŸ¤– Modelo Gemini carregado e pronto para action!\")\n",
        "\n",
        "# Testando rapidinho\n",
        "response = llm.invoke(\"Diga apenas: Funcionando!\")\n",
        "print(f\"âœ… Teste: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Componente 1: Sistema RAG\n\nLembra do **MÃ³dulo 8**? Vamos usar tudo que aprendemos sobre RAG! \n\nO RAG (Retrieval-Augmented Generation) Ã© como ter uma biblioteca pessoal super organizada. VocÃª pergunta algo, ele:\n1. ğŸ” Procura nos documentos relevantes\n2. ğŸ“– Pega os trechos mais importantes  \n3. ğŸ§  Gera uma resposta baseada no conteÃºdo\n\n### MatemÃ¡tica por trÃ¡s:\n\nO processo de similaridade usa **cosine similarity**:\n\n$$similarity(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||}$$\n\nOnde:\n- $A$ Ã© o embedding da pergunta\n- $B$ Ã© o embedding do documento\n- Resultado varia de -1 a 1 (mais prÃ³ximo de 1 = mais similar)\n\n**Dica do Pedro**: Pense no embedding como as \"coordenadas\" do significado de um texto no espaÃ§o multidimensional!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Sistema RAG completo - Do MÃ³dulo 8 turbinado!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        \n",
        "    def add_documents_from_text(self, texts: List[str], metadatas: List[Dict] = None):\n",
        "        \"\"\"Adiciona documentos de texto diretamente\"\"\"\n",
        "        try:\n",
        "            # Processa os textos\n",
        "            all_chunks = []\n",
        "            all_metadatas = []\n",
        "            \n",
        "            for i, text in enumerate(texts):\n",
        "                chunks = self.text_splitter.split_text(text)\n",
        "                all_chunks.extend(chunks)\n",
        "                \n",
        "                # Metadados para cada chunk\n",
        "                base_metadata = metadatas[i] if metadatas else {}\n",
        "                chunk_metadatas = [{**base_metadata, 'chunk_id': j} \n",
        "                                 for j in range(len(chunks))]\n",
        "                all_metadatas.extend(chunk_metadatas)\n",
        "            \n",
        "            # Cria ou atualiza o vectorstore\n",
        "            if self.vectorstore is None:\n",
        "                self.vectorstore = FAISS.from_texts(\n",
        "                    all_chunks, \n",
        "                    self.embeddings,\n",
        "                    metadatas=all_metadatas\n",
        "                )\n",
        "            else:\n",
        "                # Adiciona ao vectorstore existente\n",
        "                new_vectorstore = FAISS.from_texts(\n",
        "                    all_chunks, \n",
        "                    self.embeddings,\n",
        "                    metadatas=all_metadatas\n",
        "                )\n",
        "                self.vectorstore.merge_from(new_vectorstore)\n",
        "            \n",
        "            # Cria a chain de QA\n",
        "            self._setup_qa_chain()\n",
        "            \n",
        "            return f\"âœ… {len(all_chunks)} chunks adicionados com sucesso!\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"âŒ Erro ao processar documentos: {str(e)}\"\n",
        "    \n",
        "    def _setup_qa_chain(self):\n",
        "        \"\"\"Configura a chain de Question Answering\"\"\"\n",
        "        retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "        \n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Faz uma consulta no sistema RAG\"\"\"\n",
        "        if self.qa_chain is None:\n",
        "            return {\n",
        "                \"answer\": \"âŒ Nenhum documento foi carregado ainda.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "            \n",
        "            return {\n",
        "                \"answer\": result[\"result\"],\n",
        "                \"sources\": [doc.metadata for doc in result[\"source_documents\"]],\n",
        "                \"source_texts\": [doc.page_content[:200] + \"...\" \n",
        "                               for doc in result[\"source_documents\"]]\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"âŒ Erro na consulta: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "# Instanciando nosso sistema RAG\n",
        "rag_system = RAGSystem(llm)\n",
        "print(\"ğŸ“š Sistema RAG inicializado! Vamos adicionar alguns documentos...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos adicionar alguns documentos de exemplo sobre IA e LangChain\n",
        "sample_documents = [\n",
        "    \"\"\"\n",
        "    LangChain Ã© um framework para desenvolvimento de aplicaÃ§Ãµes com Large Language Models (LLMs). \n",
        "    Ele fornece abstraÃ§Ãµes e ferramentas para:\n",
        "    - Prompt Templates: Para estruturar entradas para LLMs\n",
        "    - Chains: Para sequenciar operaÃ§Ãµes\n",
        "    - Agents: Para tomada de decisÃµes dinÃ¢micas\n",
        "    - Memory: Para manter contexto entre interaÃ§Ãµes\n",
        "    - Document Loaders: Para processar diferentes tipos de dados\n",
        "    - Vector Stores: Para busca semÃ¢ntica\n",
        "    \n",
        "    A versÃ£o v0.2 do LangChain introduziu melhorias significativas na API e performance.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    Retrieval-Augmented Generation (RAG) Ã© uma tÃ©cnica que combina recuperaÃ§Ã£o de informaÃ§Ãµes \n",
        "    com geraÃ§Ã£o de texto. O processo funciona assim:\n",
        "    1. Documentos sÃ£o divididos em chunks menores\n",
        "    2. Chunks sÃ£o convertidos em embeddings vetoriais\n",
        "    3. Para uma pergunta, busca-se chunks similares\n",
        "    4. Chunks relevantes sÃ£o fornecidos como contexto para o LLM\n",
        "    5. LLM gera resposta baseada no contexto recuperado\n",
        "    \n",
        "    RAG resolve o problema de conhecimento desatualizado dos LLMs e permite \n",
        "    consultas em bases de conhecimento especÃ­ficas.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    Agents em LangChain sÃ£o sistemas que podem tomar decisÃµes sobre quais \n",
        "    ferramentas usar para responder perguntas. Tipos principais:\n",
        "    \n",
        "    - Zero-shot ReAct: Usa reasoning e acting em ciclos\n",
        "    - Conversational ReAct: MantÃ©m memÃ³ria de conversas\n",
        "    - Plan-and-execute: Planeja e executa etapas sequencialmente\n",
        "    \n",
        "    Tools sÃ£o funÃ§Ãµes que agents podem chamar, como:\n",
        "    - Web search (DuckDuckGo, Google)\n",
        "    - Calculadora\n",
        "    - APIs externas\n",
        "    - Sistemas de arquivos\n",
        "    \n",
        "    O processo ReAct segue: Thought -> Action -> Observation -> Thought...\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Metadados para os documentos\n",
        "metadatas = [\n",
        "    {\"source\": \"langchain_docs\", \"topic\": \"framework\", \"date\": \"2024-01-15\"},\n",
        "    {\"source\": \"rag_guide\", \"topic\": \"rag\", \"date\": \"2024-01-10\"},\n",
        "    {\"source\": \"agents_manual\", \"topic\": \"agents\", \"date\": \"2024-01-20\"}\n",
        "]\n",
        "\n",
        "# Adicionando documentos ao sistema RAG\n",
        "result = rag_system.add_documents_from_text(sample_documents, metadatas)\n",
        "print(result)\n",
        "\n",
        "# Testando o sistema\n",
        "print(\"\\nğŸ§ª Testando consulta RAG...\")\n",
        "test_query = \"O que Ã© RAG e como funciona?\"\n",
        "rag_result = rag_system.query(test_query)\n",
        "\n",
        "print(f\"\\nâ“ Pergunta: {test_query}\")\n",
        "print(f\"\\nğŸ“‹ Resposta: {rag_result['answer']}\")\n",
        "print(f\"\\nğŸ“š Sources: {len(rag_result['sources'])} documentos consultados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒ Componente 2: Web Agent\n\nAgora vamos implementar nosso **Web Agent**! Lembra do **MÃ³dulo 9** sobre Agents?\n\nUm Web Agent Ã© como ter um assistente de pesquisa que:\n- ğŸ” Sabe onde buscar informaÃ§Ãµes\n- ğŸ§  Raciocina sobre o que encontrou\n- ğŸ¯ Filtra o que Ã© relevante\n- ğŸ“ Resume tudo numa resposta Ãºtil\n\n### O Ciclo ReAct:\nO agent segue o padrÃ£o **ReAct** (Reasoning + Acting):\n\n1. **Thought**: \"Preciso buscar informaÃ§Ãµes sobre X\"\n2. **Action**: Usa ferramenta de busca\n3. **Observation**: Analisa os resultados\n4. **Thought**: \"Encontrei Y, mas preciso de mais detalhes sobre Z\"\n5. **Action**: Nova busca mais especÃ­fica\n6. **Final Answer**: Resposta consolidada\n\nÃ‰ como o processo mental que vocÃª faz quando pesquisa algo no Google!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WebAgent:\n",
        "    \"\"\"Agent para pesquisas na web - MÃ³dulo 9 na prÃ¡tica!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.tools = self._setup_tools()\n",
        "        self.agent = self._setup_agent()\n",
        "    \n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Configura as ferramentas disponÃ­veis para o agent\"\"\"\n",
        "        tools = []\n",
        "        \n",
        "        # DuckDuckGo Search - Busca geral na web\n",
        "        try:\n",
        "            search = DuckDuckGoSearchRun()\n",
        "            search_tool = Tool(\n",
        "                name=\"Web Search\",\n",
        "                func=search.run,\n",
        "                description=\"Ãštil para buscar informaÃ§Ãµes atuais na internet. Use para perguntas sobre eventos recentes, notÃ­cias, ou informaÃ§Ãµes que podem ter mudado.\"\n",
        "            )\n",
        "            tools.append(search_tool)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao configurar DuckDuckGo: {e}\")\n",
        "        \n",
        "        # Wikipedia - Para informaÃ§Ãµes enciclopÃ©dicas\n",
        "        try:\n",
        "            wikipedia = WikipediaQueryRun(\n",
        "                api_wrapper=WikipediaAPIWrapper()\n",
        "            )\n",
        "            wiki_tool = Tool(\n",
        "                name=\"Wikipedia\",\n",
        "                func=wikipedia.run,\n",
        "                description=\"Ãštil para buscar informaÃ§Ãµes enciclopÃ©dicas, definiÃ§Ãµes, histÃ³ria, biografias e conhecimento geral bem estabelecido.\"\n",
        "            )\n",
        "            tools.append(wiki_tool)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao configurar Wikipedia: {e}\")\n",
        "        \n",
        "        return tools\n",
        "    \n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Configura o agent ReAct\"\"\"\n",
        "        if not self.tools:\n",
        "            print(\"âŒ Nenhuma ferramenta disponÃ­vel para o agent\")\n",
        "            return None\n",
        "            \n",
        "        try:\n",
        "            agent = initialize_agent(\n",
        "                tools=self.tools,\n",
        "                llm=self.llm,\n",
        "                agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                verbose=False,\n",
        "                max_iterations=3,\n",
        "                early_stopping_method=\"generate\"\n",
        "            )\n",
        "            return agent\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erro ao criar agent: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def search(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Executa uma pesquisa usando o agent\"\"\"\n",
        "        if self.agent is None:\n",
        "            return {\n",
        "                \"answer\": \"âŒ Agent nÃ£o disponÃ­vel. Verifique as ferramentas.\",\n",
        "                \"tools_used\": [],\n",
        "                \"success\": False\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            # Executa o agent\n",
        "            result = self.agent.run(query)\n",
        "            \n",
        "            return {\n",
        "                \"answer\": result,\n",
        "                \"tools_used\": [tool.name for tool in self.tools],\n",
        "                \"success\": True\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"âŒ Erro na pesquisa: {str(e)}\",\n",
        "                \"tools_used\": [],\n",
        "                \"success\": False\n",
        "            }\n",
        "    \n",
        "    def get_available_tools(self):\n",
        "        \"\"\"Retorna lista de ferramentas disponÃ­veis\"\"\"\n",
        "        return [{\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description\n",
        "        } for tool in self.tools]\n",
        "\n",
        "# Instanciando nosso Web Agent\n",
        "web_agent = WebAgent(llm)\n",
        "print(\"ğŸŒ Web Agent inicializado!\")\n",
        "print(f\"ğŸ› ï¸ Ferramentas disponÃ­veis: {len(web_agent.tools)}\")\n",
        "\n",
        "# Mostrando as ferramentas\n",
        "for tool_info in web_agent.get_available_tools():\n",
        "    print(f\"  - {tool_info['name']}: {tool_info['description'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o Web Agent\n",
        "print(\"ğŸ§ª Testando Web Agent...\\n\")\n",
        "\n",
        "# Teste 1: Busca simples\n",
        "test_query_1 = \"Quais sÃ£o as principais notÃ­cias sobre inteligÃªncia artificial hoje?\"\n",
        "print(f\"â“ Teste 1: {test_query_1}\")\n",
        "\n",
        "result_1 = web_agent.search(test_query_1)\n",
        "print(f\"âœ… Sucesso: {result_1['success']}\")\n",
        "print(f\"ğŸ“ Resposta: {result_1['answer'][:300]}...\")\n",
        "print(f\"ğŸ› ï¸ Tools usadas: {result_1['tools_used']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 2: Busca enciclopÃ©dica\n",
        "test_query_2 = \"O que Ã© machine learning?\"\n",
        "print(f\"â“ Teste 2: {test_query_2}\")\n",
        "\n",
        "result_2 = web_agent.search(test_query_2)\n",
        "print(f\"âœ… Sucesso: {result_2['success']}\")\n",
        "print(f\"ğŸ“ Resposta: {result_2['answer'][:300]}...\")\n",
        "print(f\"ğŸ› ï¸ Tools usadas: {result_2['tools_used']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Componente 3: Sistema de MemÃ³ria\n\nLembra do **MÃ³dulo 5** sobre Memory Systems? Agora vamos implementar um sistema de memÃ³ria robusto!\n\nPor que memÃ³ria Ã© importante?\n- ğŸ“ MantÃ©m contexto da conversa\n- ğŸ”„ Permite referÃªncias a mensagens anteriores\n- ğŸ¯ Melhora a relevÃ¢ncia das respostas\n- ğŸ‘¤ Personaliza a experiÃªncia do usuÃ¡rio\n\n### Tipos de MemÃ³ria:\n1. **Buffer Memory**: Ãšltimas N mensagens\n2. **Summary Memory**: Resume conversas longas\n3. **Entity Memory**: Lembra de entidades especÃ­ficas\n\nVamos usar **ConversationBufferWindowMemory** - Ã© como ter uma memÃ³ria de curto prazo que lembra das Ãºltimas interaÃ§Ãµes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryManager:\n",
        "    \"\"\"Gerenciador de memÃ³ria - MÃ³dulo 5 turbinado!\"\"\"\n",
        "    \n",
        "    def __init__(self, window_size: int = 10):\n",
        "        self.window_size = window_size\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            k=window_size,\n",
        "            return_messages=True,\n",
        "            memory_key=\"chat_history\"\n",
        "        )\n",
        "        self.conversation_stats = {\n",
        "            \"total_messages\": 0,\n",
        "            \"rag_queries\": 0,\n",
        "            \"web_searches\": 0,\n",
        "            \"start_time\": datetime.now()\n",
        "        }\n",
        "    \n",
        "    def add_interaction(self, human_message: str, ai_message: str, interaction_type: str = \"chat\"):\n",
        "        \"\"\"Adiciona uma interaÃ§Ã£o Ã  memÃ³ria\"\"\"\n",
        "        # Adiciona Ã  memÃ³ria do LangChain\n",
        "        self.memory.chat_memory.add_user_message(human_message)\n",
        "        self.memory.chat_memory.add_ai_message(ai_message)\n",
        "        \n",
        "        # Atualiza estatÃ­sticas\n",
        "        self.conversation_stats[\"total_messages\"] += 1\n",
        "        if interaction_type == \"rag\":\n",
        "            self.conversation_stats[\"rag_queries\"] += 1\n",
        "        elif interaction_type == \"web\":\n",
        "            self.conversation_stats[\"web_searches\"] += 1\n",
        "    \n",
        "    def get_context(self) -> str:\n",
        "        \"\"\"Retorna o contexto atual da conversa\"\"\"\n",
        "        messages = self.memory.chat_memory.messages\n",
        "        if not messages:\n",
        "            return \"Nenhuma conversa anterior.\"\n",
        "        \n",
        "        context_parts = []\n",
        "        for msg in messages[-6:]:  # Ãšltimas 3 interaÃ§Ãµes (6 mensagens)\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                context_parts.append(f\"Humano: {msg.content}\")\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                context_parts.append(f\"Assistente: {msg.content}\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Retorna estatÃ­sticas da conversa\"\"\"\n",
        "        duration = datetime.now() - self.conversation_stats[\"start_time\"]\n",
        "        \n",
        "        return {\n",
        "            **self.conversation_stats,\n",
        "            \"duration_minutes\": duration.total_seconds() / 60,\n",
        "            \"messages_in_memory\": len(self.memory.chat_memory.messages),\n",
        "            \"memory_window_size\": self.window_size\n",
        "        }\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        \"\"\"Limpa a memÃ³ria e reinicia estatÃ­sticas\"\"\"\n",
        "        self.memory.clear()\n",
        "        self.conversation_stats = {\n",
        "            \"total_messages\": 0,\n",
        "            \"rag_queries\": 0,\n",
        "            \"web_searches\": 0,\n",
        "            \"start_time\": datetime.now()\n",
        "        }\n",
        "    \n",
        "    def export_conversation(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Exporta a conversa atual\"\"\"\n",
        "        messages = self.memory.chat_memory.messages\n",
        "        conversation = []\n",
        "        \n",
        "        for msg in messages:\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                conversation.append({\n",
        "                    \"type\": \"human\",\n",
        "                    \"content\": msg.content,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                conversation.append({\n",
        "                    \"type\": \"ai\",\n",
        "                    \"content\": msg.content,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "        \n",
        "        return conversation\n",
        "\n",
        "# Instanciando o gerenciador de memÃ³ria\n",
        "memory_manager = MemoryManager(window_size=8)\n",
        "print(\"ğŸ§  Sistema de memÃ³ria inicializado!\")\n",
        "print(f\"ğŸ“Š Window size: {memory_manager.window_size} interaÃ§Ãµes\")\n",
        "\n",
        "# Teste rÃ¡pido\n",
        "memory_manager.add_interaction(\n",
        "    \"OlÃ¡! Como vocÃª funciona?\",\n",
        "    \"OlÃ¡! Sou um assistente inteligente que combina RAG, web search e memÃ³ria!\",\n",
        "    \"chat\"\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“ˆ Stats iniciais:\")\n",
        "stats = memory_manager.get_stats()\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Componente 4: Router Inteligente\n\nAgora a parte mais legal! O **Router** Ã© o cÃ©rebro que decide qual ferramenta usar. Ã‰ como um mordomo super inteligente que sabe exatamente o que vocÃª precisa!\n\n### LÃ³gica de DecisÃ£o:\nO router analisa a pergunta e decide:\n\n- ğŸ“š **RAG**: Se a pergunta Ã© sobre os documentos carregados\n- ğŸŒ **Web Search**: Se precisa de informaÃ§Ãµes atuais/externas\n- ğŸ’¬ **Chat**: Se Ã© conversa casual ou jÃ¡ tem contexto suficiente\n\n### Algoritmo de ClassificaÃ§Ã£o:\nUsamos um **LLM como classificador** com prompt engenheirado:\n\n$$P(categoria|pergunta) = \\frac{e^{score_{categoria}}}{\\sum_{i} e^{score_i}}$$\n\nÃ‰ como ter um \"sexto sentido\" para saber qual ferramenta Ã© perfeita para cada situaÃ§Ã£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeiro, vamos criar um Output Parser para estruturar as respostas\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "class RouterDecision(BaseModel):\n",
        "    \"\"\"Estrutura da decisÃ£o do router\"\"\"\n",
        "    route: str = Field(description=\"A rota escolhida: 'rag', 'web', ou 'chat'\")\n",
        "    confidence: float = Field(description=\"ConfianÃ§a na decisÃ£o (0.0 a 1.0)\")\n",
        "    reasoning: str = Field(description=\"Justificativa para a escolha\")\n",
        "\n",
        "class AssistantResponse(BaseModel):\n",
        "    \"\"\"Estrutura da resposta final do assistente\"\"\"\n",
        "    answer: str = Field(description=\"Resposta principal\")\n",
        "    source_type: str = Field(description=\"Tipo da fonte: 'rag', 'web', 'chat', 'memory'\")\n",
        "    confidence: float = Field(description=\"ConfianÃ§a na resposta\")\n",
        "    sources: List[str] = Field(description=\"Fontes consultadas (se aplicÃ¡vel)\")\n",
        "    context_used: bool = Field(description=\"Se usou contexto da conversa\")\n",
        "\n",
        "# Parsers\n",
        "router_parser = PydanticOutputParser(pydantic_object=RouterDecision)\n",
        "response_parser = PydanticOutputParser(pydantic_object=AssistantResponse)\n",
        "\n",
        "print(\"ğŸ“‹ Output Parsers criados! Estrutura definida.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntelligentRouter:\n",
        "    \"\"\"Router inteligente - O cÃ©rebro do nosso assistente!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm, rag_system, web_agent, memory_manager):\n",
        "        self.llm = llm\n",
        "        self.rag_system = rag_system\n",
        "        self.web_agent = web_agent\n",
        "        self.memory_manager = memory_manager\n",
        "        \n",
        "        # Template para decisÃ£o de rota\n",
        "        self.router_template = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\", \"available_docs\"],\n",
        "            template=\"\"\"VocÃª Ã© um router inteligente que decide qual ferramenta usar para responder perguntas.\n",
        "\n",
        "Ferramentas disponÃ­veis:\n",
        "- 'rag': Para consultar documentos locais sobre LangChain, RAG, Agents, etc.\n",
        "- 'web': Para buscar informaÃ§Ãµes atuais na internet ou tÃ³picos nÃ£o cobertos nos docs\n",
        "- 'chat': Para conversas casuais ou quando jÃ¡ hÃ¡ contexto suficiente\n",
        "\n",
        "Contexto da conversa:\n",
        "{context}\n",
        "\n",
        "Documentos disponÃ­veis no RAG: {available_docs}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Analise a pergunta e decida qual ferramenta usar. Considere:\n",
        "- Se a pergunta Ã© sobre LangChain/RAG/Agents â†’ use 'rag'\n",
        "- Se precisa de informaÃ§Ãµes atuais/externas â†’ use 'web'  \n",
        "- Se Ã© conversa casual ou continua o contexto â†’ use 'chat'\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\",\n",
        "            partial_variables={\"format_instructions\": router_parser.get_format_instructions()}\n",
        "        )\n",
        "    \n",
        "    def decide_route(self, question: str) -> RouterDecision:\n",
        "        \"\"\"Decide qual rota usar para a pergunta\"\"\"\n",
        "        try:\n",
        "            # Pega contexto da conversa\n",
        "            context = self.memory_manager.get_context()\n",
        "            \n",
        "            # InformaÃ§Ãµes sobre documentos disponÃ­veis\n",
        "            available_docs = \"LangChain framework, RAG implementation, Agents and Tools\"\n",
        "            \n",
        "            # Forma o prompt\n",
        "            prompt = self.router_template.format(\n",
        "                question=question,\n",
        "                context=context,\n",
        "                available_docs=available_docs\n",
        "            )\n",
        "            \n",
        "            # Gera decisÃ£o\n",
        "            response = self.llm.invoke(prompt)\n",
        "            decision = router_parser.parse(response.content)\n",
        "            \n",
        "            return decision\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Fallback simples baseado em palavras-chave\n",
        "            question_lower = question.lower()\n",
        "            \n",
        "            if any(word in question_lower for word in ['langchain', 'rag', 'agent', 'embedding', 'vector']):\n",
        "                route = 'rag'\n",
        "                reasoning = \"Detectadas palavras-chave relacionadas aos documentos\"\n",
        "            elif any(word in question_lower for word in ['atual', 'hoje', 'notÃ­cia', 'recente', 'agora']):\n",
        "                route = 'web'\n",
        "                reasoning = \"Detectadas palavras-chave indicando necessidade de informaÃ§Ãµes atuais\"\n",
        "            else:\n",
        "                route = 'chat'\n",
        "                reasoning = \"Fallback para chat conversacional\"\n",
        "            \n",
        "            return RouterDecision(\n",
        "                route=route,\n",
        "                confidence=0.6,\n",
        "                reasoning=f\"Fallback usado devido ao erro: {str(e)}. {reasoning}\"\n",
        "            )\n",
        "    \n",
        "    def process_question(self, question: str) -> AssistantResponse:\n",
        "        \"\"\"Processa uma pergunta completa usando a rota apropriada\"\"\"\n",
        "        # Decide a rota\n",
        "        decision = self.decide_route(question)\n",
        "        \n",
        "        print(f\"ğŸ¯ Rota escolhida: {decision.route} (confianÃ§a: {decision.confidence:.2f})\")\n",
        "        print(f\"ğŸ¤” Reasoning: {decision.reasoning}\")\n",
        "        \n",
        "        # Executa baseado na decisÃ£o\n",
        "        if decision.route == 'rag':\n",
        "            return self._handle_rag(question, decision)\n",
        "        elif decision.route == 'web':\n",
        "            return self._handle_web(question, decision)\n",
        "        else:\n",
        "            return self._handle_chat(question, decision)\n",
        "    \n",
        "    def _handle_rag(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando RAG\"\"\"\n",
        "        rag_result = self.rag_system.query(question)\n",
        "        \n",
        "        # Adiciona Ã  memÃ³ria\n",
        "        self.memory_manager.add_interaction(question, rag_result['answer'], 'rag')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=rag_result['answer'],\n",
        "            source_type='rag',\n",
        "            confidence=decision.confidence,\n",
        "            sources=[f\"Documento: {src.get('source', 'unknown')}\" for src in rag_result.get('sources', [])],\n",
        "            context_used=True\n",
        "        )\n",
        "    \n",
        "    def _handle_web(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando Web Search\"\"\"\n",
        "        web_result = self.web_agent.search(question)\n",
        "        \n",
        "        # Adiciona Ã  memÃ³ria\n",
        "        self.memory_manager.add_interaction(question, web_result['answer'], 'web')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=web_result['answer'],\n",
        "            source_type='web',\n",
        "            confidence=decision.confidence if web_result['success'] else 0.3,\n",
        "            sources=web_result.get('tools_used', []),\n",
        "            context_used=False\n",
        "        )\n",
        "    \n",
        "    def _handle_chat(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando chat simples com contexto\"\"\"\n",
        "        context = self.memory_manager.get_context()\n",
        "        \n",
        "        chat_prompt = f\"\"\"\n",
        "Contexto da conversa:\n",
        "{context}\n",
        "\n",
        "Pergunta atual: {question}\n",
        "\n",
        "Responda de forma natural e Ãºtil, considerando o contexto da conversa.\n",
        "\"\"\"\n",
        "        \n",
        "        response = self.llm.invoke(chat_prompt)\n",
        "        answer = response.content\n",
        "        \n",
        "        # Adiciona Ã  memÃ³ria\n",
        "        self.memory_manager.add_interaction(question, answer, 'chat')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=answer,\n",
        "            source_type='chat',\n",
        "            confidence=decision.confidence,\n",
        "            sources=['Conversa contextual'],\n",
        "            context_used=bool(context and context != \"Nenhuma conversa anterior.\")\n",
        "        )\n",
        "\n",
        "# Instanciando nosso Router Inteligente\n",
        "router = IntelligentRouter(llm, rag_system, web_agent, memory_manager)\n",
        "print(\"ğŸ¯ Router Inteligente criado! Agora temos o cÃ©rebro do assistente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª Teste Completo do Sistema\n\nAgora vamos testar nosso **Assistente de Pesquisa Inteligente** completo!\n\nVamos simular uma conversa real para ver como ele:\n- ğŸ¯ Escolhe a ferramenta certa\n- ğŸ§  Usa a memÃ³ria \n- ğŸ“Š MantÃ©m estatÃ­sticas\n- ğŸ”„ Alterna entre diferentes fontes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-10_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_assistant(questions: List[str]):\n",
        "    \"\"\"Testa o assistente com uma sÃ©rie de perguntas\"\"\"\n",
        "    print(\"ğŸš€ Iniciando teste completo do Assistente de Pesquisa Inteligente!\\n\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\nğŸ¯ PERGUNTA {i}: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Processa a pergunta\n",
        "        response = router.process_question(question)\n",
        "        \n",
        "        # Mostra a resposta\n",
        "        print(f\"\\nğŸ“‹ RESPOSTA ({response.source_type.upper()}):\")\n",
        "        print(response.answer)\n",
        "        \n",
        "        print(f\"\\nğŸ“Š METADADOS:\")\n",
        "        print(f\"  â€¢ ConfianÃ§a: {response.confidence:.2f}\")\n",
        "        print(f\"  â€¢ Contexto usado: {response.context_used}\")\n",
        "        print(f\"  â€¢ Fontes: {', '.join(response.sources)}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "    \n",
        "    # EstatÃ­sticas finais\n",
        "    print(\"\\nğŸ“ˆ ESTATÃSTICAS FINAIS:\")\n",
        "    stats = memory_manager.get_stats()\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  â€¢ {key}: {value:.2f}\")\n",
        "        elif key != 'start_time':\n",
        "            print(f\"  â€¢ {key}: {value}\")\n",
        "\n",
        "# Perguntas de teste que cobrem diferentes cenÃ¡rios\n",
        "test_questions = [\n",
        "    \"OlÃ¡! Como vocÃª funciona?\",  # Chat inicial\n",
        "    \"O que Ã© RAG e como implementar?\",  # RAG query\n",
        "    \"Quais sÃ£o as Ãºltimas notÃ­cias sobre IA?\",  # Web search\n",
        "    \"VocÃª mencionou RAG antes, pode explicar melhor sobre embeddings?\",  # RAG com contexto\n",
        "    \"E como posso usar agents no meu projeto?\",  # RAG sobre agents\n",
        "]\n",
        "\n",
        "# Executa o teste\n",
        "test_assistant(test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š VisualizaÃ§Ã£o da Performance\n\nVamos criar alguns grÃ¡ficos para visualizar como nosso assistente estÃ¡ performando!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dados das estatÃ­sticas\n",
        "stats = memory_manager.get_stats()\n",
        "\n",
        "# GrÃ¡fico 1: DistribuiÃ§Ã£o de tipos de consulta\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# GrÃ¡fico de pizza - Tipos de consulta\n",
        "query_types = ['RAG Queries', 'Web Searches', 'Chat Messages']\n",
        "query_counts = [\n",
        "    stats['rag_queries'], \n",
        "    stats['web_searches'], \n",
        "    stats['total_messages'] - stats['rag_queries'] - stats['web_searches']\n",
        "]\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "ax1.pie(query_counts, labels=query_types, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax1.set_title('DistribuiÃ§Ã£o de Tipos de Consulta', fontsize=14, fontweight='bold')\n",
        "\n",
        "# GrÃ¡fico 2: Timeline simulada de confianÃ§a\n",
        "interactions = list(range(1, stats['total_messages'] + 1))\n",
        "# Simulando scores de confianÃ§a (na prÃ¡tica, vocÃª salvaria estes valores)\n",
        "confidence_scores = np.random.uniform(0.7, 0.95, len(interactions))\n",
        "\n",
        "ax2.plot(interactions, confidence_scores, 'o-', color='#45B7D1', linewidth=2, markersize=6)\n",
        "ax2.set_title('EvoluÃ§Ã£o da ConfianÃ§a nas Respostas', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('NÃºmero da InteraÃ§Ã£o')\n",
        "ax2.set_ylabel('Score de ConfianÃ§a')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "# GrÃ¡fico 3: ComparaÃ§Ã£o de ferramentas disponÃ­veis\n",
        "tools = ['RAG System', 'Web Agent', 'Memory', 'Router']\n",
        "effectiveness = [0.9, 0.85, 0.95, 0.88]  # Valores simulados\n",
        "\n",
        "bars = ax3.bar(tools, effectiveness, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "ax3.set_title('Efetividade dos Componentes', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Score de Efetividade')\n",
        "ax3.set_ylim(0, 1)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, value in zip(bars, effectiveness):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# GrÃ¡fico 4: MÃ©tricas de memÃ³ria\n",
        "memory_metrics = ['Mensagens\\nTotais', 'Mensagens\\nem MemÃ³ria', 'Window\\nSize']\n",
        "memory_values = [stats['total_messages'], stats['messages_in_memory'], stats['memory_window_size']]\n",
        "\n",
        "bars2 = ax4.bar(memory_metrics, memory_values, color=['#FECA57', '#FF9FF3', '#54A0FF'])\n",
        "ax4.set_title('MÃ©tricas do Sistema de MemÃ³ria', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('Quantidade')\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, value in zip(bars2, memory_values):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             f'{int(value)}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('ğŸ“Š Dashboard do Assistente de Pesquisa Inteligente', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“Š Dashboard gerado! Estes grÃ¡ficos mostram o desempenho do nosso sistema.\")\n",
        "print(\"ğŸ’¡ No MÃ³dulo 12, vamos colocar isso tudo numa interface Streamlit linda!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—ï¸ Arquitetura Final do Sistema\n\nVamos visualizar nossa arquitetura completa!\n\n```mermaid\ngraph TB\n    subgraph \"Interface\"\n        UI[UsuÃ¡rio]\n    end\n    \n    subgraph \"Core System\"\n        R[Router Inteligente]\n        M[Memory Manager]\n    end\n    \n    subgraph \"Ferramentas\"\n        RAG[RAG System]\n        WEB[Web Agent]\n        CHAT[Chat Engine]\n    end\n    \n    subgraph \"Dados\"\n        DOCS[Documentos]\n        VECTOR[Vector Store]\n        INTERNET[Internet]\n    end\n    \n    subgraph \"AI Models\"\n        LLM[Gemini 2.0 Flash]\n        EMB[Embeddings]\n    end\n    \n    UI --> R\n    R --> M\n    R --> RAG\n    R --> WEB\n    R --> CHAT\n    \n    RAG --> VECTOR\n    RAG --> EMB\n    VECTOR --> DOCS\n    \n    WEB --> INTERNET\n    \n    RAG --> LLM\n    WEB --> LLM\n    CHAT --> LLM\n    R --> LLM\n    \n    M -.-> RAG\n    M -.-> WEB\n    M -.-> CHAT\n```\n\n### Fluxo de Dados:\n1. **Input** â†’ Router analisa a pergunta\n2. **DecisÃ£o** â†’ Router escolhe ferramenta apropriada\n3. **ExecuÃ§Ã£o** â†’ Ferramenta processa com LLM/dados\n4. **MemÃ³ria** â†’ Resultado Ã© armazenado para contexto\n5. **Output** â†’ Resposta estruturada Ã© retornada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma funÃ§Ã£o de conveniÃªncia para usar nosso assistente\n",
        "class SmartResearchAssistant:\n",
        "    \"\"\"Assistente de Pesquisa Inteligente - Interface principal\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.router = router\n",
        "        self.memory = memory_manager\n",
        "        self.conversation_id = f\"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        \n",
        "    def ask(self, question: str, verbose: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Interface principal para fazer perguntas\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\nâ“ {question}\")\n",
        "            print(\"-\" * 50)\n",
        "        \n",
        "        # Processa a pergunta\n",
        "        response = self.router.process_question(question)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nğŸ¤– {response.answer}\")\n",
        "            print(f\"\\nğŸ“Š Fonte: {response.source_type} | ConfianÃ§a: {response.confidence:.2f}\")\n",
        "            if response.sources:\n",
        "                print(f\"ğŸ“š Fontes: {', '.join(response.sources[:2])}\")\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": response.answer,\n",
        "            \"source_type\": response.source_type,\n",
        "            \"confidence\": response.confidence,\n",
        "            \"sources\": response.sources,\n",
        "            \"context_used\": response.context_used,\n",
        "            \"conversation_id\": self.conversation_id\n",
        "        }\n",
        "    \n",
        "    def get_stats(self):\n",
        "        \"\"\"Retorna estatÃ­sticas da conversa\"\"\"\n",
        "        return self.memory.get_stats()\n",
        "    \n",
        "    def export_conversation(self):\n",
        "        \"\"\"Exporta a conversa atual\"\"\"\n",
        "        return self.memory.export_conversation()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia a conversa\"\"\"\n",
        "        self.memory.clear_memory()\n",
        "        self.conversation_id = f\"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        print(\"ğŸ”„ Conversa reiniciada!\")\n",
        "\n",
        "# Instancia nosso assistente final\n",
        "assistant = SmartResearchAssistant()\n",
        "print(\"ğŸ‰ Assistente de Pesquisa Inteligente estÃ¡ pronto para uso!\")\n",
        "print(f\"ğŸ†” ID da conversa: {assistant.conversation_id}\")\n",
        "print(\"\\nğŸ’¡ Use assistant.ask('sua pergunta') para interagir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DemonstraÃ§Ã£o final do assistente\n",
        "print(\"ğŸš€ DEMONSTRAÃ‡ÃƒO FINAL - Assistente de Pesquisa Inteligente\\n\")\n",
        "\n",
        "# Exemplo 1: Pergunta sobre RAG (deve usar RAG system)\n",
        "result1 = assistant.ask(\"Como funciona o processo de embedding no RAG?\")\n",
        "\n",
        "# Exemplo 2: Pergunta atual (deve usar web search)\n",
        "result2 = assistant.ask(\"Quais sÃ£o as tendÃªncias de IA em 2024?\")\n",
        "\n",
        "# Exemplo 3: ContinuaÃ§Ã£o da conversa (deve usar contexto)\n",
        "result3 = assistant.ask(\"Pode me dar mais detalhes sobre a primeira pergunta que fiz?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ˆ RESUMO DA DEMONSTRAÃ‡ÃƒO:\")\n",
        "stats = assistant.get_stats()\n",
        "print(f\"  â€¢ Total de mensagens: {stats['total_messages']}\")\n",
        "print(f\"  â€¢ Consultas RAG: {stats['rag_queries']}\")\n",
        "print(f\"  â€¢ Buscas web: {stats['web_searches']}\")\n",
        "print(f\"  â€¢ DuraÃ§Ã£o: {stats['duration_minutes']:.1f} minutos\")\n",
        "print(f\"  â€¢ Mensagens em memÃ³ria: {stats['messages_in_memory']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cios PrÃ¡ticos\n\nAgora Ã© sua vez de praticar! Vamos fazer alguns exercÃ­cios para fixar o aprendizado.\n\n**Dica do Pedro**: Estes exercÃ­cios vÃ£o te preparar para o Projeto Final 2 e para trabalhar com sistemas mais complexos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸš€ ExercÃ­cio 1: Melhorando o Router\n\n**Desafio**: Implemente uma versÃ£o melhorada do router que:\n1. Considera o histÃ³rico de rotas usadas\n2. Tem uma confianÃ§a mÃ­nima antes de usar web search\n3. Pode combinar mÃºltiplas fontes numa resposta\n\n**CÃ³digo inicial**:\n```python\nclass AdvancedRouter(IntelligentRouter):\n    def __init__(self, llm, rag_system, web_agent, memory_manager):\n        super().__init__(llm, rag_system, web_agent, memory_manager)\n        self.route_history = []\n        self.min_confidence_web = 0.8\n    \n    def decide_route(self, question: str):\n        # SEU CÃ“DIGO AQUI\n        # Implemente a lÃ³gica melhorada\n        pass\n```\n\n**Teste sua implementaÃ§Ã£o** com diferentes tipos de pergunta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EspaÃ§o para o ExercÃ­cio 1\n",
        "# Implemente seu AdvancedRouter aqui!\n",
        "\n",
        "class AdvancedRouter(IntelligentRouter):\n",
        "    def __init__(self, llm, rag_system, web_agent, memory_manager):\n",
        "        super().__init__(llm, rag_system, web_agent, memory_manager)\n",
        "        self.route_history = []\n",
        "        self.min_confidence_web = 0.8\n",
        "    \n",
        "    def decide_route(self, question: str):\n",
        "        # Sua implementaÃ§Ã£o aqui!\n",
        "        # Dicas:\n",
        "        # - Use self.route_history para anÃ¡lise de padrÃµes\n",
        "        # - Considere self.min_confidence_web para web searches\n",
        "        # - Pense em como combinar RAG + Web quando necessÃ¡rio\n",
        "        \n",
        "        # Placeholder - implemente sua versÃ£o!\n",
        "        decision = super().decide_route(question)\n",
        "        self.route_history.append(decision.route)\n",
        "        return decision\n",
        "\n",
        "print(\"âœï¸ ExercÃ­cio 1: Implemente seu AdvancedRouter acima!\")\n",
        "print(\"ğŸ’¡ Dica: Pense em como um assistente real tomaria decisÃµes mais inteligentes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¨ ExercÃ­cio 2: Sistema de Cache Inteligente\n\n**Desafio**: Crie um sistema de cache que:\n1. Armazena respostas de web searches para evitar buscas repetidas\n2. Tem TTL (Time To Live) configurÃ¡vel\n3. Usa similaridade semÃ¢ntica para encontrar cache hits\n\n**Por que Ã© importante?**\n- ğŸ’° Economiza API calls\n- âš¡ Melhora performance\n- ğŸŒ Reduz impacto ambiental\n\n**Estrutura sugerida**:\n```python\nclass SmartCache:\n    def __init__(self, ttl_minutes=60, similarity_threshold=0.85):\n        # SEU CÃ“DIGO AQUI\n        pass\n    \n    def get(self, query: str):\n        # Busca no cache usando similaridade\n        pass\n    \n    def set(self, query: str, response: str):\n        # Adiciona ao cache com timestamp\n        pass\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EspaÃ§o para o ExercÃ­cio 2\n",
        "# Implemente seu SmartCache aqui!\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "\n",
        "class SmartCache:\n",
        "    def __init__(self, ttl_minutes=60, similarity_threshold=0.85):\n",
        "        self.ttl_minutes = ttl_minutes\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.cache = {}  # {query_hash: {response, timestamp, query_text}}\n",
        "        \n",
        "        # VocÃª vai precisar de embeddings para comparar similaridade\n",
        "        # Dica: Use o mesmo modelo de embeddings do RAG!\n",
        "    \n",
        "    def get(self, query: str):\n",
        "        # Sua implementaÃ§Ã£o aqui!\n",
        "        # Passos sugeridos:\n",
        "        # 1. Calcule embedding da query\n",
        "        # 2. Compare com queries em cache\n",
        "        # 3. Verifique TTL\n",
        "        # 4. Retorne se similar + vÃ¡lido\n",
        "        \n",
        "        print(f\"ğŸ” Buscando '{query}' no cache... (nÃ£o implementado ainda)\")\n",
        "        return None\n",
        "    \n",
        "    def set(self, query: str, response: str):\n",
        "        # Sua implementaÃ§Ã£o aqui!\n",
        "        # Passos sugeridos:\n",
        "        # 1. Calcule hash/embedding da query\n",
        "        # 2. Armazene com timestamp\n",
        "        # 3. Limpe entradas antigas\n",
        "        \n",
        "        print(f\"ğŸ’¾ Salvando resposta para '{query}' no cache... (nÃ£o implementado ainda)\")\n",
        "    \n",
        "    def cleanup_expired(self):\n",
        "        # Remove entradas expiradas\n",
        "        pass\n",
        "    \n",
        "    def get_stats(self):\n",
        "        # Retorna estatÃ­sticas do cache\n",
        "        return {\n",
        "            \"total_entries\": len(self.cache),\n",
        "            \"ttl_minutes\": self.ttl_minutes,\n",
        "            \"similarity_threshold\": self.similarity_threshold\n",
        "        }\n",
        "\n",
        "# Teste bÃ¡sico\n",
        "cache = SmartCache()\n",
        "print(\"âœï¸ ExercÃ­cio 2: Implemente seu SmartCache acima!\")\n",
        "print(f\"ğŸ“Š Stats do cache: {cache.get_stats()}\")\n",
        "print(\"ğŸ’¡ Dica: Use cosine similarity para comparar embeddings de queries!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ Resumo e PrÃ³ximos Passos\n\n**Liiindo!** ğŸš€ Chegamos ao fim do **MÃ³dulo 10** e olha sÃ³ o que construÃ­mos:\n\n### âœ… O que implementamos:\n1. **Sistema RAG completo** - Consulta documentos com embeddings\n2. **Web Agent inteligente** - Busca informaÃ§Ãµes online com ReAct\n3. **Sistema de MemÃ³ria** - MantÃ©m contexto da conversa\n4. **Router Inteligente** - Decide qual ferramenta usar\n5. **Output Parsing** - Estrutura respostas consistentes\n6. **Interface unificada** - SmartResearchAssistant\n\n### ğŸ§  Conceitos aplicados:\n- **MÃ³dulo 2**: ChatModel com Gemini 2.0 Flash\n- **MÃ³dulo 3**: PromptTemplates e OutputParsers \n- **MÃ³dulo 4**: Chains para processamento\n- **MÃ³dulo 5**: Memory para contexto\n- **MÃ³dulo 6-7**: Document Loading e Vector Stores\n- **MÃ³dulo 8**: RAG Implementation\n- **MÃ³dulo 9**: Agents e Tools\n\n### ğŸ”® O que vem por aÃ­:\n- **MÃ³dulo 11**: Projeto Final 2 (ainda mais avanÃ§ado!)\n- **MÃ³dulo 12**: Deploy com Streamlit \n- **MÃ³dulo 13**: MigraÃ§Ã£o para v1.0\n- **MÃ³dulo 14-15**: LangGraph e LangSmith\n\n**Dica do Pedro**: Este projeto Ã© sua base! Salve o cÃ³digo, experimente melhorias e prepare-se para o prÃ³ximo nÃ­vel! ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FunÃ§Ã£o final para salvar o projeto\n",
        "def save_project_summary():\n",
        "    \"\"\"Salva um resumo do projeto desenvolvido\"\"\"\n",
        "    \n",
        "    summary = {\n",
        "        \"project_name\": \"Smart Research Assistant\",\n",
        "        \"module\": 10,\n",
        "        \"date_created\": datetime.now().isoformat(),\n",
        "        \"components\": {\n",
        "            \"rag_system\": \"âœ… Implementado\",\n",
        "            \"web_agent\": \"âœ… Implementado\", \n",
        "            \"memory_manager\": \"âœ… Implementado\",\n",
        "            \"intelligent_router\": \"âœ… Implementado\",\n",
        "            \"output_parsers\": \"âœ… Implementado\",\n",
        "            \"unified_interface\": \"âœ… Implementado\"\n",
        "        },\n",
        "        \"features\": [\n",
        "            \"Multi-source information retrieval\",\n",
        "            \"Intelligent routing decisions\", \n",
        "            \"Conversation memory management\",\n",
        "            \"Structured response formatting\",\n",
        "            \"Performance analytics\",\n",
        "            \"Modular architecture\"\n",
        "        ],\n",
        "        \"tech_stack\": {\n",
        "            \"framework\": \"LangChain v0.2\",\n",
        "            \"llm\": \"Gemini 2.0 Flash\",\n",
        "            \"embeddings\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            \"vector_store\": \"FAISS\",\n",
        "            \"web_tools\": [\"DuckDuckGo\", \"Wikipedia\"]\n",
        "        },\n",
        "        \"next_steps\": [\n",
        "            \"Projeto Final 2 (MÃ³dulo 11)\",\n",
        "            \"Deploy com Streamlit (MÃ³dulo 12)\",\n",
        "            \"MigraÃ§Ã£o para v1.0 (MÃ³dulo 13)\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Salva e mostra o resumo\n",
        "project_summary = save_project_summary()\n",
        "\n",
        "print(\"ğŸ“„ RESUMO DO PROJETO - MÃ“DULO 10\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ¯ Projeto: {project_summary['project_name']}\")\n",
        "print(f\"ğŸ“… Data: {project_summary['date_created'][:19]}\")\n",
        "print(f\"ğŸ¤– LLM: {project_summary['tech_stack']['llm']}\")\n",
        "\n",
        "print(\"\\nğŸ”§ Componentes implementados:\")\n",
        "for comp, status in project_summary['components'].items():\n",
        "    print(f\"  â€¢ {comp}: {status}\")\n",
        "\n",
        "print(\"\\nâ­ Features principais:\")\n",
        "for feature in project_summary['features'][:3]:\n",
        "    print(f\"  â€¢ {feature}\")\n",
        "\n",
        "print(\"\\nğŸš€ PrÃ³ximos passos:\")\n",
        "for step in project_summary['next_steps']:\n",
        "    print(f\"  â€¢ {step}\")\n",
        "\n",
        "print(\"\\nğŸ‰ ParabÃ©ns! VocÃª completou o MÃ³dulo 10!\")\n",
        "print(\"ğŸ’ª Agora vocÃª tem um assistente de IA completo e funcional!\")\n",
        "print(\"ğŸ”¥ Bora para o Projeto Final 2 no prÃ³ximo mÃ³dulo!\")"
      ]
    }
  ]
}