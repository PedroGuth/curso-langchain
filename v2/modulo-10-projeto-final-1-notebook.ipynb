{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Assistente de Pesquisa Inteligente com RAG e Agents\n\n## Pedro Nunes Guth - Módulo 10: Projeto Final 1\n\nEaí pessoal! 🎉\n\nChegamos no **Módulo 10** e tá na hora de botar a mão na massa! Vamos criar um projeto que vai juntar **TUDO** que aprendemos até agora. \n\nImagina ter um assistente que não só conversa contigo, mas também:\n- 📚 Pesquisa em documentos (RAG)\n- 🌐 Busca informações na internet (Agents)\n- 🧠 Lembra do que vocês já conversaram (Memory)\n- 🎯 Responde de forma estruturada (Output Parsers)\n\nÉ como ter um estagiário super inteligente que nunca esquece de nada e ainda pesquisa tudo pra você!\n\n**Dica do Pedro**: Este projeto vai servir de base para o Projeto Final 2 e o deploy no Streamlit. Então capricha!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 O que vamos construir?\n\nNosso **Assistente de Pesquisa Inteligente** vai ter:\n\n### Funcionalidades:\n1. **RAG System**: Consulta documentos locais\n2. **Web Agent**: Pesquisa informações online\n3. **Memory**: Mantém contexto da conversa\n4. **Router**: Decide qual ferramenta usar\n5. **Output Parsing**: Formata respostas estruturadas\n\n### Arquitetura:\n```mermaid\ngraph TD\n    A[Usuário] --> B[Router Agent]\n    B --> C{Tipo de Pergunta?}\n    C -->|Documentos| D[RAG System]\n    C -->|Web Search| E[Web Agent]\n    C -->|Conversa| F[Chat Agent]\n    D --> G[Memory + Response]\n    E --> G\n    F --> G\n    G --> H[Output Parser]\n    H --> I[Resposta Estruturada]\n```\n\nTá, mas por que essa arquitetura? Porque na vida real, um assistente precisa saber **quando** usar **qual** ferramenta. É como um canivete suíço inteligente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Vamos instalar todas as dependências necessárias\n",
        "!pip install -q langchain langchain-google-genai langchain-community\n",
        "!pip install -q faiss-cpu pypdf sentence-transformers\n",
        "!pip install -q duckduckgo-search wikipedia-api requests beautifulsoup4\n",
        "!pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necessários - Tudo que aprendemos nos módulos anteriores!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangChain Core\n",
        "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser, StructuredOutputParser\n",
        "from langchain.schema.output_parser import OutputParserException\n",
        "\n",
        "# Chat Model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# RAG Components\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Agents and Tools\n",
        "from langchain.agents import AgentType, initialize_agent, Tool\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# Utils\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"📦 Imports carregados! Bora pro código!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuração da API do Google Gemini\n",
        "# Lembre-se de configurar sua GOOGLE_API_KEY\n",
        "load_dotenv()\n",
        "\n",
        "# Se você não tem .env, descomente e coloque sua chave aqui:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_aqui\"\n",
        "\n",
        "# Inicializando nosso modelo principal - o Gemini 2.0 Flash\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"🤖 Modelo Gemini carregado e pronto para action!\")\n",
        "\n",
        "# Testando rapidinho\n",
        "response = llm.invoke(\"Diga apenas: Funcionando!\")\n",
        "print(f\"✅ Teste: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Componente 1: Sistema RAG\n\nLembra do **Módulo 8**? Vamos usar tudo que aprendemos sobre RAG! \n\nO RAG (Retrieval-Augmented Generation) é como ter uma biblioteca pessoal super organizada. Você pergunta algo, ele:\n1. 🔍 Procura nos documentos relevantes\n2. 📖 Pega os trechos mais importantes  \n3. 🧠 Gera uma resposta baseada no conteúdo\n\n### Matemática por trás:\n\nO processo de similaridade usa **cosine similarity**:\n\n$$similarity(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||}$$\n\nOnde:\n- $A$ é o embedding da pergunta\n- $B$ é o embedding do documento\n- Resultado varia de -1 a 1 (mais próximo de 1 = mais similar)\n\n**Dica do Pedro**: Pense no embedding como as \"coordenadas\" do significado de um texto no espaço multidimensional!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Sistema RAG completo - Do Módulo 8 turbinado!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        \n",
        "    def add_documents_from_text(self, texts: List[str], metadatas: List[Dict] = None):\n",
        "        \"\"\"Adiciona documentos de texto diretamente\"\"\"\n",
        "        try:\n",
        "            # Processa os textos\n",
        "            all_chunks = []\n",
        "            all_metadatas = []\n",
        "            \n",
        "            for i, text in enumerate(texts):\n",
        "                chunks = self.text_splitter.split_text(text)\n",
        "                all_chunks.extend(chunks)\n",
        "                \n",
        "                # Metadados para cada chunk\n",
        "                base_metadata = metadatas[i] if metadatas else {}\n",
        "                chunk_metadatas = [{**base_metadata, 'chunk_id': j} \n",
        "                                 for j in range(len(chunks))]\n",
        "                all_metadatas.extend(chunk_metadatas)\n",
        "            \n",
        "            # Cria ou atualiza o vectorstore\n",
        "            if self.vectorstore is None:\n",
        "                self.vectorstore = FAISS.from_texts(\n",
        "                    all_chunks, \n",
        "                    self.embeddings,\n",
        "                    metadatas=all_metadatas\n",
        "                )\n",
        "            else:\n",
        "                # Adiciona ao vectorstore existente\n",
        "                new_vectorstore = FAISS.from_texts(\n",
        "                    all_chunks, \n",
        "                    self.embeddings,\n",
        "                    metadatas=all_metadatas\n",
        "                )\n",
        "                self.vectorstore.merge_from(new_vectorstore)\n",
        "            \n",
        "            # Cria a chain de QA\n",
        "            self._setup_qa_chain()\n",
        "            \n",
        "            return f\"✅ {len(all_chunks)} chunks adicionados com sucesso!\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"❌ Erro ao processar documentos: {str(e)}\"\n",
        "    \n",
        "    def _setup_qa_chain(self):\n",
        "        \"\"\"Configura a chain de Question Answering\"\"\"\n",
        "        retriever = self.vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "        \n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Faz uma consulta no sistema RAG\"\"\"\n",
        "        if self.qa_chain is None:\n",
        "            return {\n",
        "                \"answer\": \"❌ Nenhum documento foi carregado ainda.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "            \n",
        "            return {\n",
        "                \"answer\": result[\"result\"],\n",
        "                \"sources\": [doc.metadata for doc in result[\"source_documents\"]],\n",
        "                \"source_texts\": [doc.page_content[:200] + \"...\" \n",
        "                               for doc in result[\"source_documents\"]]\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"❌ Erro na consulta: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "# Instanciando nosso sistema RAG\n",
        "rag_system = RAGSystem(llm)\n",
        "print(\"📚 Sistema RAG inicializado! Vamos adicionar alguns documentos...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos adicionar alguns documentos de exemplo sobre IA e LangChain\n",
        "sample_documents = [\n",
        "    \"\"\"\n",
        "    LangChain é um framework para desenvolvimento de aplicações com Large Language Models (LLMs). \n",
        "    Ele fornece abstrações e ferramentas para:\n",
        "    - Prompt Templates: Para estruturar entradas para LLMs\n",
        "    - Chains: Para sequenciar operações\n",
        "    - Agents: Para tomada de decisões dinâmicas\n",
        "    - Memory: Para manter contexto entre interações\n",
        "    - Document Loaders: Para processar diferentes tipos de dados\n",
        "    - Vector Stores: Para busca semântica\n",
        "    \n",
        "    A versão v0.2 do LangChain introduziu melhorias significativas na API e performance.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    Retrieval-Augmented Generation (RAG) é uma técnica que combina recuperação de informações \n",
        "    com geração de texto. O processo funciona assim:\n",
        "    1. Documentos são divididos em chunks menores\n",
        "    2. Chunks são convertidos em embeddings vetoriais\n",
        "    3. Para uma pergunta, busca-se chunks similares\n",
        "    4. Chunks relevantes são fornecidos como contexto para o LLM\n",
        "    5. LLM gera resposta baseada no contexto recuperado\n",
        "    \n",
        "    RAG resolve o problema de conhecimento desatualizado dos LLMs e permite \n",
        "    consultas em bases de conhecimento específicas.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    Agents em LangChain são sistemas que podem tomar decisões sobre quais \n",
        "    ferramentas usar para responder perguntas. Tipos principais:\n",
        "    \n",
        "    - Zero-shot ReAct: Usa reasoning e acting em ciclos\n",
        "    - Conversational ReAct: Mantém memória de conversas\n",
        "    - Plan-and-execute: Planeja e executa etapas sequencialmente\n",
        "    \n",
        "    Tools são funções que agents podem chamar, como:\n",
        "    - Web search (DuckDuckGo, Google)\n",
        "    - Calculadora\n",
        "    - APIs externas\n",
        "    - Sistemas de arquivos\n",
        "    \n",
        "    O processo ReAct segue: Thought -> Action -> Observation -> Thought...\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Metadados para os documentos\n",
        "metadatas = [\n",
        "    {\"source\": \"langchain_docs\", \"topic\": \"framework\", \"date\": \"2024-01-15\"},\n",
        "    {\"source\": \"rag_guide\", \"topic\": \"rag\", \"date\": \"2024-01-10\"},\n",
        "    {\"source\": \"agents_manual\", \"topic\": \"agents\", \"date\": \"2024-01-20\"}\n",
        "]\n",
        "\n",
        "# Adicionando documentos ao sistema RAG\n",
        "result = rag_system.add_documents_from_text(sample_documents, metadatas)\n",
        "print(result)\n",
        "\n",
        "# Testando o sistema\n",
        "print(\"\\n🧪 Testando consulta RAG...\")\n",
        "test_query = \"O que é RAG e como funciona?\"\n",
        "rag_result = rag_system.query(test_query)\n",
        "\n",
        "print(f\"\\n❓ Pergunta: {test_query}\")\n",
        "print(f\"\\n📋 Resposta: {rag_result['answer']}\")\n",
        "print(f\"\\n📚 Sources: {len(rag_result['sources'])} documentos consultados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌐 Componente 2: Web Agent\n\nAgora vamos implementar nosso **Web Agent**! Lembra do **Módulo 9** sobre Agents?\n\nUm Web Agent é como ter um assistente de pesquisa que:\n- 🔍 Sabe onde buscar informações\n- 🧠 Raciocina sobre o que encontrou\n- 🎯 Filtra o que é relevante\n- 📝 Resume tudo numa resposta útil\n\n### O Ciclo ReAct:\nO agent segue o padrão **ReAct** (Reasoning + Acting):\n\n1. **Thought**: \"Preciso buscar informações sobre X\"\n2. **Action**: Usa ferramenta de busca\n3. **Observation**: Analisa os resultados\n4. **Thought**: \"Encontrei Y, mas preciso de mais detalhes sobre Z\"\n5. **Action**: Nova busca mais específica\n6. **Final Answer**: Resposta consolidada\n\nÉ como o processo mental que você faz quando pesquisa algo no Google!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WebAgent:\n",
        "    \"\"\"Agent para pesquisas na web - Módulo 9 na prática!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.tools = self._setup_tools()\n",
        "        self.agent = self._setup_agent()\n",
        "    \n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Configura as ferramentas disponíveis para o agent\"\"\"\n",
        "        tools = []\n",
        "        \n",
        "        # DuckDuckGo Search - Busca geral na web\n",
        "        try:\n",
        "            search = DuckDuckGoSearchRun()\n",
        "            search_tool = Tool(\n",
        "                name=\"Web Search\",\n",
        "                func=search.run,\n",
        "                description=\"Útil para buscar informações atuais na internet. Use para perguntas sobre eventos recentes, notícias, ou informações que podem ter mudado.\"\n",
        "            )\n",
        "            tools.append(search_tool)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao configurar DuckDuckGo: {e}\")\n",
        "        \n",
        "        # Wikipedia - Para informações enciclopédicas\n",
        "        try:\n",
        "            wikipedia = WikipediaQueryRun(\n",
        "                api_wrapper=WikipediaAPIWrapper()\n",
        "            )\n",
        "            wiki_tool = Tool(\n",
        "                name=\"Wikipedia\",\n",
        "                func=wikipedia.run,\n",
        "                description=\"Útil para buscar informações enciclopédicas, definições, história, biografias e conhecimento geral bem estabelecido.\"\n",
        "            )\n",
        "            tools.append(wiki_tool)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao configurar Wikipedia: {e}\")\n",
        "        \n",
        "        return tools\n",
        "    \n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Configura o agent ReAct\"\"\"\n",
        "        if not self.tools:\n",
        "            print(\"❌ Nenhuma ferramenta disponível para o agent\")\n",
        "            return None\n",
        "            \n",
        "        try:\n",
        "            agent = initialize_agent(\n",
        "                tools=self.tools,\n",
        "                llm=self.llm,\n",
        "                agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                verbose=False,\n",
        "                max_iterations=3,\n",
        "                early_stopping_method=\"generate\"\n",
        "            )\n",
        "            return agent\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao criar agent: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def search(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Executa uma pesquisa usando o agent\"\"\"\n",
        "        if self.agent is None:\n",
        "            return {\n",
        "                \"answer\": \"❌ Agent não disponível. Verifique as ferramentas.\",\n",
        "                \"tools_used\": [],\n",
        "                \"success\": False\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            # Executa o agent\n",
        "            result = self.agent.run(query)\n",
        "            \n",
        "            return {\n",
        "                \"answer\": result,\n",
        "                \"tools_used\": [tool.name for tool in self.tools],\n",
        "                \"success\": True\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"❌ Erro na pesquisa: {str(e)}\",\n",
        "                \"tools_used\": [],\n",
        "                \"success\": False\n",
        "            }\n",
        "    \n",
        "    def get_available_tools(self):\n",
        "        \"\"\"Retorna lista de ferramentas disponíveis\"\"\"\n",
        "        return [{\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description\n",
        "        } for tool in self.tools]\n",
        "\n",
        "# Instanciando nosso Web Agent\n",
        "web_agent = WebAgent(llm)\n",
        "print(\"🌐 Web Agent inicializado!\")\n",
        "print(f\"🛠️ Ferramentas disponíveis: {len(web_agent.tools)}\")\n",
        "\n",
        "# Mostrando as ferramentas\n",
        "for tool_info in web_agent.get_available_tools():\n",
        "    print(f\"  - {tool_info['name']}: {tool_info['description'][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o Web Agent\n",
        "print(\"🧪 Testando Web Agent...\\n\")\n",
        "\n",
        "# Teste 1: Busca simples\n",
        "test_query_1 = \"Quais são as principais notícias sobre inteligência artificial hoje?\"\n",
        "print(f\"❓ Teste 1: {test_query_1}\")\n",
        "\n",
        "result_1 = web_agent.search(test_query_1)\n",
        "print(f\"✅ Sucesso: {result_1['success']}\")\n",
        "print(f\"📝 Resposta: {result_1['answer'][:300]}...\")\n",
        "print(f\"🛠️ Tools usadas: {result_1['tools_used']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Teste 2: Busca enciclopédica\n",
        "test_query_2 = \"O que é machine learning?\"\n",
        "print(f\"❓ Teste 2: {test_query_2}\")\n",
        "\n",
        "result_2 = web_agent.search(test_query_2)\n",
        "print(f\"✅ Sucesso: {result_2['success']}\")\n",
        "print(f\"📝 Resposta: {result_2['answer'][:300]}...\")\n",
        "print(f\"🛠️ Tools usadas: {result_2['tools_used']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Componente 3: Sistema de Memória\n\nLembra do **Módulo 5** sobre Memory Systems? Agora vamos implementar um sistema de memória robusto!\n\nPor que memória é importante?\n- 📝 Mantém contexto da conversa\n- 🔄 Permite referências a mensagens anteriores\n- 🎯 Melhora a relevância das respostas\n- 👤 Personaliza a experiência do usuário\n\n### Tipos de Memória:\n1. **Buffer Memory**: Últimas N mensagens\n2. **Summary Memory**: Resume conversas longas\n3. **Entity Memory**: Lembra de entidades específicas\n\nVamos usar **ConversationBufferWindowMemory** - é como ter uma memória de curto prazo que lembra das últimas interações!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryManager:\n",
        "    \"\"\"Gerenciador de memória - Módulo 5 turbinado!\"\"\"\n",
        "    \n",
        "    def __init__(self, window_size: int = 10):\n",
        "        self.window_size = window_size\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            k=window_size,\n",
        "            return_messages=True,\n",
        "            memory_key=\"chat_history\"\n",
        "        )\n",
        "        self.conversation_stats = {\n",
        "            \"total_messages\": 0,\n",
        "            \"rag_queries\": 0,\n",
        "            \"web_searches\": 0,\n",
        "            \"start_time\": datetime.now()\n",
        "        }\n",
        "    \n",
        "    def add_interaction(self, human_message: str, ai_message: str, interaction_type: str = \"chat\"):\n",
        "        \"\"\"Adiciona uma interação à memória\"\"\"\n",
        "        # Adiciona à memória do LangChain\n",
        "        self.memory.chat_memory.add_user_message(human_message)\n",
        "        self.memory.chat_memory.add_ai_message(ai_message)\n",
        "        \n",
        "        # Atualiza estatísticas\n",
        "        self.conversation_stats[\"total_messages\"] += 1\n",
        "        if interaction_type == \"rag\":\n",
        "            self.conversation_stats[\"rag_queries\"] += 1\n",
        "        elif interaction_type == \"web\":\n",
        "            self.conversation_stats[\"web_searches\"] += 1\n",
        "    \n",
        "    def get_context(self) -> str:\n",
        "        \"\"\"Retorna o contexto atual da conversa\"\"\"\n",
        "        messages = self.memory.chat_memory.messages\n",
        "        if not messages:\n",
        "            return \"Nenhuma conversa anterior.\"\n",
        "        \n",
        "        context_parts = []\n",
        "        for msg in messages[-6:]:  # Últimas 3 interações (6 mensagens)\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                context_parts.append(f\"Humano: {msg.content}\")\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                context_parts.append(f\"Assistente: {msg.content}\")\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Retorna estatísticas da conversa\"\"\"\n",
        "        duration = datetime.now() - self.conversation_stats[\"start_time\"]\n",
        "        \n",
        "        return {\n",
        "            **self.conversation_stats,\n",
        "            \"duration_minutes\": duration.total_seconds() / 60,\n",
        "            \"messages_in_memory\": len(self.memory.chat_memory.messages),\n",
        "            \"memory_window_size\": self.window_size\n",
        "        }\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        \"\"\"Limpa a memória e reinicia estatísticas\"\"\"\n",
        "        self.memory.clear()\n",
        "        self.conversation_stats = {\n",
        "            \"total_messages\": 0,\n",
        "            \"rag_queries\": 0,\n",
        "            \"web_searches\": 0,\n",
        "            \"start_time\": datetime.now()\n",
        "        }\n",
        "    \n",
        "    def export_conversation(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"Exporta a conversa atual\"\"\"\n",
        "        messages = self.memory.chat_memory.messages\n",
        "        conversation = []\n",
        "        \n",
        "        for msg in messages:\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                conversation.append({\n",
        "                    \"type\": \"human\",\n",
        "                    \"content\": msg.content,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                conversation.append({\n",
        "                    \"type\": \"ai\",\n",
        "                    \"content\": msg.content,\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                })\n",
        "        \n",
        "        return conversation\n",
        "\n",
        "# Instanciando o gerenciador de memória\n",
        "memory_manager = MemoryManager(window_size=8)\n",
        "print(\"🧠 Sistema de memória inicializado!\")\n",
        "print(f\"📊 Window size: {memory_manager.window_size} interações\")\n",
        "\n",
        "# Teste rápido\n",
        "memory_manager.add_interaction(\n",
        "    \"Olá! Como você funciona?\",\n",
        "    \"Olá! Sou um assistente inteligente que combina RAG, web search e memória!\",\n",
        "    \"chat\"\n",
        ")\n",
        "\n",
        "print(\"\\n📈 Stats iniciais:\")\n",
        "stats = memory_manager.get_stats()\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Componente 4: Router Inteligente\n\nAgora a parte mais legal! O **Router** é o cérebro que decide qual ferramenta usar. É como um mordomo super inteligente que sabe exatamente o que você precisa!\n\n### Lógica de Decisão:\nO router analisa a pergunta e decide:\n\n- 📚 **RAG**: Se a pergunta é sobre os documentos carregados\n- 🌐 **Web Search**: Se precisa de informações atuais/externas\n- 💬 **Chat**: Se é conversa casual ou já tem contexto suficiente\n\n### Algoritmo de Classificação:\nUsamos um **LLM como classificador** com prompt engenheirado:\n\n$$P(categoria|pergunta) = \\frac{e^{score_{categoria}}}{\\sum_{i} e^{score_i}}$$\n\nÉ como ter um \"sexto sentido\" para saber qual ferramenta é perfeita para cada situação!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeiro, vamos criar um Output Parser para estruturar as respostas\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "class RouterDecision(BaseModel):\n",
        "    \"\"\"Estrutura da decisão do router\"\"\"\n",
        "    route: str = Field(description=\"A rota escolhida: 'rag', 'web', ou 'chat'\")\n",
        "    confidence: float = Field(description=\"Confiança na decisão (0.0 a 1.0)\")\n",
        "    reasoning: str = Field(description=\"Justificativa para a escolha\")\n",
        "\n",
        "class AssistantResponse(BaseModel):\n",
        "    \"\"\"Estrutura da resposta final do assistente\"\"\"\n",
        "    answer: str = Field(description=\"Resposta principal\")\n",
        "    source_type: str = Field(description=\"Tipo da fonte: 'rag', 'web', 'chat', 'memory'\")\n",
        "    confidence: float = Field(description=\"Confiança na resposta\")\n",
        "    sources: List[str] = Field(description=\"Fontes consultadas (se aplicável)\")\n",
        "    context_used: bool = Field(description=\"Se usou contexto da conversa\")\n",
        "\n",
        "# Parsers\n",
        "router_parser = PydanticOutputParser(pydantic_object=RouterDecision)\n",
        "response_parser = PydanticOutputParser(pydantic_object=AssistantResponse)\n",
        "\n",
        "print(\"📋 Output Parsers criados! Estrutura definida.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntelligentRouter:\n",
        "    \"\"\"Router inteligente - O cérebro do nosso assistente!\"\"\"\n",
        "    \n",
        "    def __init__(self, llm, rag_system, web_agent, memory_manager):\n",
        "        self.llm = llm\n",
        "        self.rag_system = rag_system\n",
        "        self.web_agent = web_agent\n",
        "        self.memory_manager = memory_manager\n",
        "        \n",
        "        # Template para decisão de rota\n",
        "        self.router_template = PromptTemplate(\n",
        "            input_variables=[\"question\", \"context\", \"available_docs\"],\n",
        "            template=\"\"\"Você é um router inteligente que decide qual ferramenta usar para responder perguntas.\n",
        "\n",
        "Ferramentas disponíveis:\n",
        "- 'rag': Para consultar documentos locais sobre LangChain, RAG, Agents, etc.\n",
        "- 'web': Para buscar informações atuais na internet ou tópicos não cobertos nos docs\n",
        "- 'chat': Para conversas casuais ou quando já há contexto suficiente\n",
        "\n",
        "Contexto da conversa:\n",
        "{context}\n",
        "\n",
        "Documentos disponíveis no RAG: {available_docs}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Analise a pergunta e decida qual ferramenta usar. Considere:\n",
        "- Se a pergunta é sobre LangChain/RAG/Agents → use 'rag'\n",
        "- Se precisa de informações atuais/externas → use 'web'  \n",
        "- Se é conversa casual ou continua o contexto → use 'chat'\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\",\n",
        "            partial_variables={\"format_instructions\": router_parser.get_format_instructions()}\n",
        "        )\n",
        "    \n",
        "    def decide_route(self, question: str) -> RouterDecision:\n",
        "        \"\"\"Decide qual rota usar para a pergunta\"\"\"\n",
        "        try:\n",
        "            # Pega contexto da conversa\n",
        "            context = self.memory_manager.get_context()\n",
        "            \n",
        "            # Informações sobre documentos disponíveis\n",
        "            available_docs = \"LangChain framework, RAG implementation, Agents and Tools\"\n",
        "            \n",
        "            # Forma o prompt\n",
        "            prompt = self.router_template.format(\n",
        "                question=question,\n",
        "                context=context,\n",
        "                available_docs=available_docs\n",
        "            )\n",
        "            \n",
        "            # Gera decisão\n",
        "            response = self.llm.invoke(prompt)\n",
        "            decision = router_parser.parse(response.content)\n",
        "            \n",
        "            return decision\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Fallback simples baseado em palavras-chave\n",
        "            question_lower = question.lower()\n",
        "            \n",
        "            if any(word in question_lower for word in ['langchain', 'rag', 'agent', 'embedding', 'vector']):\n",
        "                route = 'rag'\n",
        "                reasoning = \"Detectadas palavras-chave relacionadas aos documentos\"\n",
        "            elif any(word in question_lower for word in ['atual', 'hoje', 'notícia', 'recente', 'agora']):\n",
        "                route = 'web'\n",
        "                reasoning = \"Detectadas palavras-chave indicando necessidade de informações atuais\"\n",
        "            else:\n",
        "                route = 'chat'\n",
        "                reasoning = \"Fallback para chat conversacional\"\n",
        "            \n",
        "            return RouterDecision(\n",
        "                route=route,\n",
        "                confidence=0.6,\n",
        "                reasoning=f\"Fallback usado devido ao erro: {str(e)}. {reasoning}\"\n",
        "            )\n",
        "    \n",
        "    def process_question(self, question: str) -> AssistantResponse:\n",
        "        \"\"\"Processa uma pergunta completa usando a rota apropriada\"\"\"\n",
        "        # Decide a rota\n",
        "        decision = self.decide_route(question)\n",
        "        \n",
        "        print(f\"🎯 Rota escolhida: {decision.route} (confiança: {decision.confidence:.2f})\")\n",
        "        print(f\"🤔 Reasoning: {decision.reasoning}\")\n",
        "        \n",
        "        # Executa baseado na decisão\n",
        "        if decision.route == 'rag':\n",
        "            return self._handle_rag(question, decision)\n",
        "        elif decision.route == 'web':\n",
        "            return self._handle_web(question, decision)\n",
        "        else:\n",
        "            return self._handle_chat(question, decision)\n",
        "    \n",
        "    def _handle_rag(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando RAG\"\"\"\n",
        "        rag_result = self.rag_system.query(question)\n",
        "        \n",
        "        # Adiciona à memória\n",
        "        self.memory_manager.add_interaction(question, rag_result['answer'], 'rag')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=rag_result['answer'],\n",
        "            source_type='rag',\n",
        "            confidence=decision.confidence,\n",
        "            sources=[f\"Documento: {src.get('source', 'unknown')}\" for src in rag_result.get('sources', [])],\n",
        "            context_used=True\n",
        "        )\n",
        "    \n",
        "    def _handle_web(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando Web Search\"\"\"\n",
        "        web_result = self.web_agent.search(question)\n",
        "        \n",
        "        # Adiciona à memória\n",
        "        self.memory_manager.add_interaction(question, web_result['answer'], 'web')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=web_result['answer'],\n",
        "            source_type='web',\n",
        "            confidence=decision.confidence if web_result['success'] else 0.3,\n",
        "            sources=web_result.get('tools_used', []),\n",
        "            context_used=False\n",
        "        )\n",
        "    \n",
        "    def _handle_chat(self, question: str, decision: RouterDecision) -> AssistantResponse:\n",
        "        \"\"\"Processa pergunta usando chat simples com contexto\"\"\"\n",
        "        context = self.memory_manager.get_context()\n",
        "        \n",
        "        chat_prompt = f\"\"\"\n",
        "Contexto da conversa:\n",
        "{context}\n",
        "\n",
        "Pergunta atual: {question}\n",
        "\n",
        "Responda de forma natural e útil, considerando o contexto da conversa.\n",
        "\"\"\"\n",
        "        \n",
        "        response = self.llm.invoke(chat_prompt)\n",
        "        answer = response.content\n",
        "        \n",
        "        # Adiciona à memória\n",
        "        self.memory_manager.add_interaction(question, answer, 'chat')\n",
        "        \n",
        "        return AssistantResponse(\n",
        "            answer=answer,\n",
        "            source_type='chat',\n",
        "            confidence=decision.confidence,\n",
        "            sources=['Conversa contextual'],\n",
        "            context_used=bool(context and context != \"Nenhuma conversa anterior.\")\n",
        "        )\n",
        "\n",
        "# Instanciando nosso Router Inteligente\n",
        "router = IntelligentRouter(llm, rag_system, web_agent, memory_manager)\n",
        "print(\"🎯 Router Inteligente criado! Agora temos o cérebro do assistente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Teste Completo do Sistema\n\nAgora vamos testar nosso **Assistente de Pesquisa Inteligente** completo!\n\nVamos simular uma conversa real para ver como ele:\n- 🎯 Escolhe a ferramenta certa\n- 🧠 Usa a memória \n- 📊 Mantém estatísticas\n- 🔄 Alterna entre diferentes fontes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-10_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_assistant(questions: List[str]):\n",
        "    \"\"\"Testa o assistente com uma série de perguntas\"\"\"\n",
        "    print(\"🚀 Iniciando teste completo do Assistente de Pesquisa Inteligente!\\n\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n🎯 PERGUNTA {i}: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Processa a pergunta\n",
        "        response = router.process_question(question)\n",
        "        \n",
        "        # Mostra a resposta\n",
        "        print(f\"\\n📋 RESPOSTA ({response.source_type.upper()}):\")\n",
        "        print(response.answer)\n",
        "        \n",
        "        print(f\"\\n📊 METADADOS:\")\n",
        "        print(f\"  • Confiança: {response.confidence:.2f}\")\n",
        "        print(f\"  • Contexto usado: {response.context_used}\")\n",
        "        print(f\"  • Fontes: {', '.join(response.sources)}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "    \n",
        "    # Estatísticas finais\n",
        "    print(\"\\n📈 ESTATÍSTICAS FINAIS:\")\n",
        "    stats = memory_manager.get_stats()\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  • {key}: {value:.2f}\")\n",
        "        elif key != 'start_time':\n",
        "            print(f\"  • {key}: {value}\")\n",
        "\n",
        "# Perguntas de teste que cobrem diferentes cenários\n",
        "test_questions = [\n",
        "    \"Olá! Como você funciona?\",  # Chat inicial\n",
        "    \"O que é RAG e como implementar?\",  # RAG query\n",
        "    \"Quais são as últimas notícias sobre IA?\",  # Web search\n",
        "    \"Você mencionou RAG antes, pode explicar melhor sobre embeddings?\",  # RAG com contexto\n",
        "    \"E como posso usar agents no meu projeto?\",  # RAG sobre agents\n",
        "]\n",
        "\n",
        "# Executa o teste\n",
        "test_assistant(test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Visualização da Performance\n\nVamos criar alguns gráficos para visualizar como nosso assistente está performando!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dados das estatísticas\n",
        "stats = memory_manager.get_stats()\n",
        "\n",
        "# Gráfico 1: Distribuição de tipos de consulta\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Gráfico de pizza - Tipos de consulta\n",
        "query_types = ['RAG Queries', 'Web Searches', 'Chat Messages']\n",
        "query_counts = [\n",
        "    stats['rag_queries'], \n",
        "    stats['web_searches'], \n",
        "    stats['total_messages'] - stats['rag_queries'] - stats['web_searches']\n",
        "]\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "ax1.pie(query_counts, labels=query_types, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "ax1.set_title('Distribuição de Tipos de Consulta', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Gráfico 2: Timeline simulada de confiança\n",
        "interactions = list(range(1, stats['total_messages'] + 1))\n",
        "# Simulando scores de confiança (na prática, você salvaria estes valores)\n",
        "confidence_scores = np.random.uniform(0.7, 0.95, len(interactions))\n",
        "\n",
        "ax2.plot(interactions, confidence_scores, 'o-', color='#45B7D1', linewidth=2, markersize=6)\n",
        "ax2.set_title('Evolução da Confiança nas Respostas', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Número da Interação')\n",
        "ax2.set_ylabel('Score de Confiança')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "# Gráfico 3: Comparação de ferramentas disponíveis\n",
        "tools = ['RAG System', 'Web Agent', 'Memory', 'Router']\n",
        "effectiveness = [0.9, 0.85, 0.95, 0.88]  # Valores simulados\n",
        "\n",
        "bars = ax3.bar(tools, effectiveness, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "ax3.set_title('Efetividade dos Componentes', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Score de Efetividade')\n",
        "ax3.set_ylim(0, 1)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, value in zip(bars, effectiveness):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Gráfico 4: Métricas de memória\n",
        "memory_metrics = ['Mensagens\\nTotais', 'Mensagens\\nem Memória', 'Window\\nSize']\n",
        "memory_values = [stats['total_messages'], stats['messages_in_memory'], stats['memory_window_size']]\n",
        "\n",
        "bars2 = ax4.bar(memory_metrics, memory_values, color=['#FECA57', '#FF9FF3', '#54A0FF'])\n",
        "ax4.set_title('Métricas do Sistema de Memória', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('Quantidade')\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, value in zip(bars2, memory_values):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "             f'{int(value)}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('📊 Dashboard do Assistente de Pesquisa Inteligente', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Dashboard gerado! Estes gráficos mostram o desempenho do nosso sistema.\")\n",
        "print(\"💡 No Módulo 12, vamos colocar isso tudo numa interface Streamlit linda!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏗️ Arquitetura Final do Sistema\n\nVamos visualizar nossa arquitetura completa!\n\n```mermaid\ngraph TB\n    subgraph \"Interface\"\n        UI[Usuário]\n    end\n    \n    subgraph \"Core System\"\n        R[Router Inteligente]\n        M[Memory Manager]\n    end\n    \n    subgraph \"Ferramentas\"\n        RAG[RAG System]\n        WEB[Web Agent]\n        CHAT[Chat Engine]\n    end\n    \n    subgraph \"Dados\"\n        DOCS[Documentos]\n        VECTOR[Vector Store]\n        INTERNET[Internet]\n    end\n    \n    subgraph \"AI Models\"\n        LLM[Gemini 2.0 Flash]\n        EMB[Embeddings]\n    end\n    \n    UI --> R\n    R --> M\n    R --> RAG\n    R --> WEB\n    R --> CHAT\n    \n    RAG --> VECTOR\n    RAG --> EMB\n    VECTOR --> DOCS\n    \n    WEB --> INTERNET\n    \n    RAG --> LLM\n    WEB --> LLM\n    CHAT --> LLM\n    R --> LLM\n    \n    M -.-> RAG\n    M -.-> WEB\n    M -.-> CHAT\n```\n\n### Fluxo de Dados:\n1. **Input** → Router analisa a pergunta\n2. **Decisão** → Router escolhe ferramenta apropriada\n3. **Execução** → Ferramenta processa com LLM/dados\n4. **Memória** → Resultado é armazenado para contexto\n5. **Output** → Resposta estruturada é retornada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma função de conveniência para usar nosso assistente\n",
        "class SmartResearchAssistant:\n",
        "    \"\"\"Assistente de Pesquisa Inteligente - Interface principal\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.router = router\n",
        "        self.memory = memory_manager\n",
        "        self.conversation_id = f\"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        \n",
        "    def ask(self, question: str, verbose: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Interface principal para fazer perguntas\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n❓ {question}\")\n",
        "            print(\"-\" * 50)\n",
        "        \n",
        "        # Processa a pergunta\n",
        "        response = self.router.process_question(question)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n🤖 {response.answer}\")\n",
        "            print(f\"\\n📊 Fonte: {response.source_type} | Confiança: {response.confidence:.2f}\")\n",
        "            if response.sources:\n",
        "                print(f\"📚 Fontes: {', '.join(response.sources[:2])}\")\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": response.answer,\n",
        "            \"source_type\": response.source_type,\n",
        "            \"confidence\": response.confidence,\n",
        "            \"sources\": response.sources,\n",
        "            \"context_used\": response.context_used,\n",
        "            \"conversation_id\": self.conversation_id\n",
        "        }\n",
        "    \n",
        "    def get_stats(self):\n",
        "        \"\"\"Retorna estatísticas da conversa\"\"\"\n",
        "        return self.memory.get_stats()\n",
        "    \n",
        "    def export_conversation(self):\n",
        "        \"\"\"Exporta a conversa atual\"\"\"\n",
        "        return self.memory.export_conversation()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia a conversa\"\"\"\n",
        "        self.memory.clear_memory()\n",
        "        self.conversation_id = f\"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        print(\"🔄 Conversa reiniciada!\")\n",
        "\n",
        "# Instancia nosso assistente final\n",
        "assistant = SmartResearchAssistant()\n",
        "print(\"🎉 Assistente de Pesquisa Inteligente está pronto para uso!\")\n",
        "print(f\"🆔 ID da conversa: {assistant.conversation_id}\")\n",
        "print(\"\\n💡 Use assistant.ask('sua pergunta') para interagir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstração final do assistente\n",
        "print(\"🚀 DEMONSTRAÇÃO FINAL - Assistente de Pesquisa Inteligente\\n\")\n",
        "\n",
        "# Exemplo 1: Pergunta sobre RAG (deve usar RAG system)\n",
        "result1 = assistant.ask(\"Como funciona o processo de embedding no RAG?\")\n",
        "\n",
        "# Exemplo 2: Pergunta atual (deve usar web search)\n",
        "result2 = assistant.ask(\"Quais são as tendências de IA em 2024?\")\n",
        "\n",
        "# Exemplo 3: Continuação da conversa (deve usar contexto)\n",
        "result3 = assistant.ask(\"Pode me dar mais detalhes sobre a primeira pergunta que fiz?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📈 RESUMO DA DEMONSTRAÇÃO:\")\n",
        "stats = assistant.get_stats()\n",
        "print(f\"  • Total de mensagens: {stats['total_messages']}\")\n",
        "print(f\"  • Consultas RAG: {stats['rag_queries']}\")\n",
        "print(f\"  • Buscas web: {stats['web_searches']}\")\n",
        "print(f\"  • Duração: {stats['duration_minutes']:.1f} minutos\")\n",
        "print(f\"  • Mensagens em memória: {stats['messages_in_memory']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Exercícios Práticos\n\nAgora é sua vez de praticar! Vamos fazer alguns exercícios para fixar o aprendizado.\n\n**Dica do Pedro**: Estes exercícios vão te preparar para o Projeto Final 2 e para trabalhar com sistemas mais complexos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🚀 Exercício 1: Melhorando o Router\n\n**Desafio**: Implemente uma versão melhorada do router que:\n1. Considera o histórico de rotas usadas\n2. Tem uma confiança mínima antes de usar web search\n3. Pode combinar múltiplas fontes numa resposta\n\n**Código inicial**:\n```python\nclass AdvancedRouter(IntelligentRouter):\n    def __init__(self, llm, rag_system, web_agent, memory_manager):\n        super().__init__(llm, rag_system, web_agent, memory_manager)\n        self.route_history = []\n        self.min_confidence_web = 0.8\n    \n    def decide_route(self, question: str):\n        # SEU CÓDIGO AQUI\n        # Implemente a lógica melhorada\n        pass\n```\n\n**Teste sua implementação** com diferentes tipos de pergunta!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Espaço para o Exercício 1\n",
        "# Implemente seu AdvancedRouter aqui!\n",
        "\n",
        "class AdvancedRouter(IntelligentRouter):\n",
        "    def __init__(self, llm, rag_system, web_agent, memory_manager):\n",
        "        super().__init__(llm, rag_system, web_agent, memory_manager)\n",
        "        self.route_history = []\n",
        "        self.min_confidence_web = 0.8\n",
        "    \n",
        "    def decide_route(self, question: str):\n",
        "        # Sua implementação aqui!\n",
        "        # Dicas:\n",
        "        # - Use self.route_history para análise de padrões\n",
        "        # - Considere self.min_confidence_web para web searches\n",
        "        # - Pense em como combinar RAG + Web quando necessário\n",
        "        \n",
        "        # Placeholder - implemente sua versão!\n",
        "        decision = super().decide_route(question)\n",
        "        self.route_history.append(decision.route)\n",
        "        return decision\n",
        "\n",
        "print(\"✏️ Exercício 1: Implemente seu AdvancedRouter acima!\")\n",
        "print(\"💡 Dica: Pense em como um assistente real tomaria decisões mais inteligentes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎨 Exercício 2: Sistema de Cache Inteligente\n\n**Desafio**: Crie um sistema de cache que:\n1. Armazena respostas de web searches para evitar buscas repetidas\n2. Tem TTL (Time To Live) configurável\n3. Usa similaridade semântica para encontrar cache hits\n\n**Por que é importante?**\n- 💰 Economiza API calls\n- ⚡ Melhora performance\n- 🌍 Reduz impacto ambiental\n\n**Estrutura sugerida**:\n```python\nclass SmartCache:\n    def __init__(self, ttl_minutes=60, similarity_threshold=0.85):\n        # SEU CÓDIGO AQUI\n        pass\n    \n    def get(self, query: str):\n        # Busca no cache usando similaridade\n        pass\n    \n    def set(self, query: str, response: str):\n        # Adiciona ao cache com timestamp\n        pass\n```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Espaço para o Exercício 2\n",
        "# Implemente seu SmartCache aqui!\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "\n",
        "class SmartCache:\n",
        "    def __init__(self, ttl_minutes=60, similarity_threshold=0.85):\n",
        "        self.ttl_minutes = ttl_minutes\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.cache = {}  # {query_hash: {response, timestamp, query_text}}\n",
        "        \n",
        "        # Você vai precisar de embeddings para comparar similaridade\n",
        "        # Dica: Use o mesmo modelo de embeddings do RAG!\n",
        "    \n",
        "    def get(self, query: str):\n",
        "        # Sua implementação aqui!\n",
        "        # Passos sugeridos:\n",
        "        # 1. Calcule embedding da query\n",
        "        # 2. Compare com queries em cache\n",
        "        # 3. Verifique TTL\n",
        "        # 4. Retorne se similar + válido\n",
        "        \n",
        "        print(f\"🔍 Buscando '{query}' no cache... (não implementado ainda)\")\n",
        "        return None\n",
        "    \n",
        "    def set(self, query: str, response: str):\n",
        "        # Sua implementação aqui!\n",
        "        # Passos sugeridos:\n",
        "        # 1. Calcule hash/embedding da query\n",
        "        # 2. Armazene com timestamp\n",
        "        # 3. Limpe entradas antigas\n",
        "        \n",
        "        print(f\"💾 Salvando resposta para '{query}' no cache... (não implementado ainda)\")\n",
        "    \n",
        "    def cleanup_expired(self):\n",
        "        # Remove entradas expiradas\n",
        "        pass\n",
        "    \n",
        "    def get_stats(self):\n",
        "        # Retorna estatísticas do cache\n",
        "        return {\n",
        "            \"total_entries\": len(self.cache),\n",
        "            \"ttl_minutes\": self.ttl_minutes,\n",
        "            \"similarity_threshold\": self.similarity_threshold\n",
        "        }\n",
        "\n",
        "# Teste básico\n",
        "cache = SmartCache()\n",
        "print(\"✏️ Exercício 2: Implemente seu SmartCache acima!\")\n",
        "print(f\"📊 Stats do cache: {cache.get_stats()}\")\n",
        "print(\"💡 Dica: Use cosine similarity para comparar embeddings de queries!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 Resumo e Próximos Passos\n\n**Liiindo!** 🚀 Chegamos ao fim do **Módulo 10** e olha só o que construímos:\n\n### ✅ O que implementamos:\n1. **Sistema RAG completo** - Consulta documentos com embeddings\n2. **Web Agent inteligente** - Busca informações online com ReAct\n3. **Sistema de Memória** - Mantém contexto da conversa\n4. **Router Inteligente** - Decide qual ferramenta usar\n5. **Output Parsing** - Estrutura respostas consistentes\n6. **Interface unificada** - SmartResearchAssistant\n\n### 🧠 Conceitos aplicados:\n- **Módulo 2**: ChatModel com Gemini 2.0 Flash\n- **Módulo 3**: PromptTemplates e OutputParsers \n- **Módulo 4**: Chains para processamento\n- **Módulo 5**: Memory para contexto\n- **Módulo 6-7**: Document Loading e Vector Stores\n- **Módulo 8**: RAG Implementation\n- **Módulo 9**: Agents e Tools\n\n### 🔮 O que vem por aí:\n- **Módulo 11**: Projeto Final 2 (ainda mais avançado!)\n- **Módulo 12**: Deploy com Streamlit \n- **Módulo 13**: Migração para v1.0\n- **Módulo 14-15**: LangGraph e LangSmith\n\n**Dica do Pedro**: Este projeto é sua base! Salve o código, experimente melhorias e prepare-se para o próximo nível! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Função final para salvar o projeto\n",
        "def save_project_summary():\n",
        "    \"\"\"Salva um resumo do projeto desenvolvido\"\"\"\n",
        "    \n",
        "    summary = {\n",
        "        \"project_name\": \"Smart Research Assistant\",\n",
        "        \"module\": 10,\n",
        "        \"date_created\": datetime.now().isoformat(),\n",
        "        \"components\": {\n",
        "            \"rag_system\": \"✅ Implementado\",\n",
        "            \"web_agent\": \"✅ Implementado\", \n",
        "            \"memory_manager\": \"✅ Implementado\",\n",
        "            \"intelligent_router\": \"✅ Implementado\",\n",
        "            \"output_parsers\": \"✅ Implementado\",\n",
        "            \"unified_interface\": \"✅ Implementado\"\n",
        "        },\n",
        "        \"features\": [\n",
        "            \"Multi-source information retrieval\",\n",
        "            \"Intelligent routing decisions\", \n",
        "            \"Conversation memory management\",\n",
        "            \"Structured response formatting\",\n",
        "            \"Performance analytics\",\n",
        "            \"Modular architecture\"\n",
        "        ],\n",
        "        \"tech_stack\": {\n",
        "            \"framework\": \"LangChain v0.2\",\n",
        "            \"llm\": \"Gemini 2.0 Flash\",\n",
        "            \"embeddings\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            \"vector_store\": \"FAISS\",\n",
        "            \"web_tools\": [\"DuckDuckGo\", \"Wikipedia\"]\n",
        "        },\n",
        "        \"next_steps\": [\n",
        "            \"Projeto Final 2 (Módulo 11)\",\n",
        "            \"Deploy com Streamlit (Módulo 12)\",\n",
        "            \"Migração para v1.0 (Módulo 13)\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Salva e mostra o resumo\n",
        "project_summary = save_project_summary()\n",
        "\n",
        "print(\"📄 RESUMO DO PROJETO - MÓDULO 10\")\n",
        "print(\"=\"*50)\n",
        "print(f\"🎯 Projeto: {project_summary['project_name']}\")\n",
        "print(f\"📅 Data: {project_summary['date_created'][:19]}\")\n",
        "print(f\"🤖 LLM: {project_summary['tech_stack']['llm']}\")\n",
        "\n",
        "print(\"\\n🔧 Componentes implementados:\")\n",
        "for comp, status in project_summary['components'].items():\n",
        "    print(f\"  • {comp}: {status}\")\n",
        "\n",
        "print(\"\\n⭐ Features principais:\")\n",
        "for feature in project_summary['features'][:3]:\n",
        "    print(f\"  • {feature}\")\n",
        "\n",
        "print(\"\\n🚀 Próximos passos:\")\n",
        "for step in project_summary['next_steps']:\n",
        "    print(f\"  • {step}\")\n",
        "\n",
        "print(\"\\n🎉 Parabéns! Você completou o Módulo 10!\")\n",
        "print(\"💪 Agora você tem um assistente de IA completo e funcional!\")\n",
        "print(\"🔥 Bora para o Projeto Final 2 no próximo módulo!\")"
      ]
    }
  ]
}