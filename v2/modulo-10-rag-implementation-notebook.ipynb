{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç RAG na Pr√°tica: Criando uma IA que Realmente Sabe das Coisas!\n\n## M√≥dulo 10 - LangChain v0.3 Course\n\nFala, pessoal! Pedro Nunes Guth aqui! üöÄ\n\nBora colocar a m√£o na massa e implementar nosso primeiro RAG completo! Nos m√≥dulos anteriores a gente viu:\n- Como fazer document loading e splitting (M√≥dulo 8)\n- Como criar embeddings e vector stores (M√≥dulo 9)\n- Chains e prompts (M√≥dulos 4 e 6)\n\nAgora √© hora de juntar tudo isso numa implementa√ß√£o RAG que vai fazer sua IA parecer que tem PhD em qualquer assunto!\n\n**T√°, mas o que √© RAG mesmo?** ü§î\n\nImagina que voc√™ tem um amigo super inteligente, mas que s√≥ sabe coisas at√© 2021. A√≠ voc√™ d√° pra ele um monte de livros atualizados e fala: \"√ì, antes de responder qualquer coisa, d√° uma olhada nesses livros aqui\". Isso √© RAG!\n\n**RAG = Retrieval Augmented Generation**\n- **Retrieval**: Busca informa√ß√µes relevantes\n- **Augmented**: Aumenta o conhecimento da IA\n- **Generation**: Gera respostas baseadas nessas informa√ß√µes\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-10_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Como Funciona o RAG na Pr√°tica?\n\nO RAG √© tipo aquele sistema de atendimento ao cliente que funciona de verdade (raridade, n√©?):\n\n1. **Voc√™ faz uma pergunta** üó£Ô∏è\n2. **O sistema busca nos documentos** üîç\n3. **Encontra as partes relevantes** üìÑ\n4. **Monta uma resposta baseada nisso** ü§ñ\n\nA diferen√ßa √© que aqui funciona mesmo! üòÇ\n\n### Fluxo Matem√°tico do RAG:\n\nPara uma query $q$, o RAG funciona assim:\n\n$$\\text{resposta} = \\text{LLM}(q + \\text{retrieve}(q, D))$$\n\nOnde:\n- $q$ = sua pergunta\n- $D$ = base de documentos\n- $\\text{retrieve}(q, D)$ = documentos mais relevantes para $q$\n\n### Etapas Detalhadas:\n\n1. **Embedding da Query**: $e_q = \\text{embedding}(q)$\n2. **Busca por Similaridade**: $\\text{sim}(e_q, e_d) = \\frac{e_q \\cdot e_d}{||e_q|| \\cdot ||e_d||}$\n3. **Sele√ß√£o dos Top-k**: $D_{top} = \\text{top_k}(\\text{sim}(e_q, D))$\n4. **Gera√ß√£o**: $\\text{resposta} = \\text{LLM}(\\text{prompt} + D_{top} + q)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora instalar as depend√™ncias que vamos precisar!\n",
        "!pip install langchain langchain-google-genai langchain-community faiss-cpu python-dotenv -q\n",
        "\n",
        "print(\"Liiindo! Tudo instalado! üöÄ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necess√°rios - j√° conhecemos esses caras dos m√≥dulos anteriores!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain components que j√° vimos\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Para visualiza√ß√£o\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imports feitos! Vamos que vamos! üí™\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura√ß√£o da API do Google (lembra do M√≥dulo 2?)\n",
        "load_dotenv()\n",
        "\n",
        "# Se voc√™ n√£o tem o arquivo .env, descomente e cole sua API key aqui:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_api_key_aqui\"\n",
        "\n",
        "# Verificando se t√° tudo certo\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    print(\"‚ùå Opa! Faltou a API key do Google!\")\n",
        "    print(\"Configure no .env ou descomente a linha acima\")\n",
        "else:\n",
        "    print(\"‚úÖ API key configurada! Bora pro RAG!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Construindo nosso RAG - Passo 1: Preparando os Documentos\n\nLembra do M√≥dulo 8? Vamos usar aqueles conceitos de document loading e splitting!\n\n**Dica!** üí° O splitting √© super importante! Se voc√™ cortar os documentos muito pequenos, perde contexto. Muito grandes, fica confuso. √â tipo cortar um bolo - tem que ser na medida certa!\n\nVamos criar alguns documentos de exemplo sobre tecnologia brasileira:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando documentos de exemplo sobre tecnologia brasileira\n",
        "# (Na vida real, voc√™ carregaria de arquivos, APIs, etc.)\n",
        "\n",
        "documentos_exemplo = [\n",
        "    \"\"\"A Nubank foi fundada em 2013 por David V√©lez, Cristina Junqueira e Edward Wible. \n",
        "    A empresa revolucionou o mercado financeiro brasileiro com seu cart√£o de cr√©dito roxo \n",
        "    e aplicativo mobile. Em 2021, a empresa fez IPO na NYSE e se tornou o banco digital \n",
        "    mais valioso da Am√©rica Latina, com mais de 70 milh√µes de clientes.\"\"\",\n",
        "    \n",
        "    \"\"\"O Pix foi lan√ßado pelo Banco Central do Brasil em novembro de 2020. √â um sistema \n",
        "    de pagamentos instant√¢neos que funciona 24 horas por dia, 7 dias por semana. \n",
        "    O Pix permite transfer√™ncias de dinheiro em at√© 10 segundos e se tornou o meio \n",
        "    de pagamento mais popular do Brasil, processando bilh√µes de transa√ß√µes por m√™s.\"\"\",\n",
        "    \n",
        "    \"\"\"A Stone Pagamentos foi fundada em 2012 e se tornou uma das principais empresas \n",
        "    de meios de pagamento do Brasil. A empresa oferece m√°quinas de cart√£o (maquininhas), \n",
        "    solu√ß√µes de software e servi√ßos financeiros para pequenas e m√©dias empresas. \n",
        "    Est√° listada na NASDAQ desde 2018.\"\"\",\n",
        "    \n",
        "    \"\"\"O iFood √© a maior plataforma de delivery de comida da Am√©rica Latina, fundada em 2011. \n",
        "    A empresa conecta restaurantes, entregadores e consumidores atrav√©s de seu aplicativo. \n",
        "    Durante a pandemia de COVID-19, o iFood expandiu significativamente e hoje opera \n",
        "    em mais de 1000 cidades brasileiras.\"\"\",\n",
        "    \n",
        "    \"\"\"A 99 (anteriormente 99Taxis) foi fundada em 2012 e se tornou a principal \n",
        "    concorrente brasileira da Uber. Em 2018, foi adquirida pela chinesa Didi Chuxing \n",
        "    por 1 bilh√£o de d√≥lares. A empresa oferece servi√ßos de transporte por aplicativo \n",
        "    e delivery, sendo l√≠der em v√°rias cidades brasileiras.\"\"\"\n",
        "]\n",
        "\n",
        "print(f\"Criamos {len(documentos_exemplo)} documentos de exemplo sobre tech brasileira! üáßüá∑\")\n",
        "print(f\"Primeiro documento tem {len(documentos_exemplo[0])} caracteres\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformando em Document objects (padr√£o do LangChain)\n",
        "# Lembra disso do M√≥dulo 8?\n",
        "\n",
        "documents = []\n",
        "for i, texto in enumerate(documentos_exemplo):\n",
        "    doc = Document(\n",
        "        page_content=texto,\n",
        "        metadata={\"source\": f\"tech_brasileira_doc_{i}\", \"tipo\": \"startup\"}\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"‚úÖ Convertidos {len(documents)} textos para Document objects!\")\n",
        "print(f\"Exemplo de metadata: {documents[0].metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting dos documentos (M√≥dulo 8 feelings!)\n",
        "# Configura√ß√µes importantes para um bom RAG\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,        # Tamanho de cada chunk\n",
        "    chunk_overlap=50,      # Sobreposi√ß√£o entre chunks\n",
        "    length_function=len,   # Como medir o tamanho\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Como quebrar o texto\n",
        ")\n",
        "\n",
        "# Fazendo o split\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"üìÑ Documentos originais: {len(documents)}\")\n",
        "print(f\"üî™ Chunks ap√≥s splitting: {len(split_docs)}\")\n",
        "print(f\"\\nüìã Exemplo de chunk:\")\n",
        "print(f\"Conte√∫do: {split_docs[0].page_content[:100]}...\")\n",
        "print(f\"Metadata: {split_docs[0].metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Passo 2: Criando Embeddings e Vector Store\n\nAgora vem a parte que vimos no M√≥dulo 9! Vamos transformar nossos textos em vetores num√©ricos.\n\n**T√°, mas por que vetores?** ü§î\n\n√â tipo transformar palavras em coordenadas num mapa gigante. Palavras parecidas ficam perto, palavras diferentes ficam longe. Assim o computador consegue entender \"proximidade sem√¢ntica\"!\n\n**Dica!** üí° O modelo de embedding √© crucial! Modelos diferentes v√£o dar vetores diferentes. No LangChain v0.3, temos v√°rias op√ß√µes. Aqui vamos usar o Google!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando o modelo de embeddings (lembra do M√≥dulo 9?)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "print(\"üß† Modelo de embeddings criado!\")\n",
        "\n",
        "# Testando com um texto simples\n",
        "teste_embedding = embeddings.embed_query(\"Nubank √© um banco digital\")\n",
        "print(f\"üìä Dimens√£o do vetor: {len(teste_embedding)}\")\n",
        "print(f\"üî¢ Primeiros 5 valores: {teste_embedding[:5]}\")\n",
        "\n",
        "# Cada palavra vira um vetor de 768 n√∫meros! Liiindo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando o Vector Store com FAISS\n",
        "# FAISS = Facebook AI Similarity Search (agora Meta AI)\n",
        "# √â uma biblioteca super eficiente para busca por similaridade!\n",
        "\n",
        "print(\"üèóÔ∏è Criando vector store... Isso pode demorar um pouco!\")\n",
        "\n",
        "vector_store = FAISS.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vector store criado com sucesso!\")\n",
        "print(f\"üìö Armazenados {len(split_docs)} chunks como vetores\")\n",
        "\n",
        "# Testando uma busca simples\n",
        "resultados = vector_store.similarity_search(\"Nubank cart√£o roxo\", k=2)\n",
        "print(f\"\\nüîç Teste de busca por 'Nubank cart√£o roxo':\")\n",
        "for i, doc in enumerate(resultados):\n",
        "    print(f\"  {i+1}. {doc.page_content[:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Visualizando a Similaridade dos Embeddings\n\nBora ver como fica a similaridade entre diferentes queries e nossos documentos! √â tipo ver o \"mapa da proximidade sem√¢ntica\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar diferentes queries e ver as similaridades\n",
        "queries_teste = [\n",
        "    \"cart√£o de cr√©dito roxo\",\n",
        "    \"pagamento instant√¢neo\",\n",
        "    \"delivery de comida\",\n",
        "    \"transporte por aplicativo\",\n",
        "    \"m√°quina de cart√£o\"\n",
        "]\n",
        "\n",
        "# Para cada query, vamos buscar os documentos mais similares\n",
        "resultados_similaridade = []\n",
        "\n",
        "for query in queries_teste:\n",
        "    # Busca com score de similaridade\n",
        "    docs_com_score = vector_store.similarity_search_with_score(query, k=3)\n",
        "    \n",
        "    # Guardando os scores\n",
        "    scores = [score for _, score in docs_com_score]\n",
        "    resultados_similaridade.append(scores)\n",
        "    \n",
        "    print(f\"üîç Query: '{query}'\")\n",
        "    for i, (doc, score) in enumerate(docs_com_score):\n",
        "        print(f\"  Top {i+1} (score: {score:.3f}): {doc.page_content[:60]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando os scores de similaridade\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Preparando dados para o gr√°fico\n",
        "x_pos = np.arange(len(queries_teste))\n",
        "width = 0.25\n",
        "\n",
        "# Plotando os top 3 resultados para cada query\n",
        "for i in range(3):\n",
        "    scores_pos = [resultado[i] for resultado in resultados_similaridade]\n",
        "    ax.bar(x_pos + i*width, scores_pos, width, \n",
        "           label=f'Top {i+1}', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Queries de Teste')\n",
        "ax.set_ylabel('Score de Similaridade (menor = mais similar)')\n",
        "ax.set_title('üìä Scores de Similaridade por Query\\n(Quanto menor, mais similar!)')\n",
        "ax.set_xticks(x_pos + width)\n",
        "ax.set_xticklabels(queries_teste, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° Lembre-se: no FAISS, scores MENORES indicam MAIOR similaridade!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Passo 3: Criando o Modelo de Chat\n\nAgora vamos criar nosso modelo de linguagem! Lembra do M√≥dulo 2? Vamos usar o Gemini que j√° conhecemos bem!\n\n**Dica!** üí° Para RAG, configura√ß√µes como temperatura e max_tokens s√£o importantes. Queremos respostas precisas baseadas nos documentos, ent√£o temperatura baixa √© ideal!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando o modelo de chat (nosso velho conhecido do M√≥dulo 2!)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",  # Modelo mais recente\n",
        "    temperature=0.1,               # Baixa para ser mais preciso\n",
        "    max_tokens=512                 # Controle do tamanho da resposta\n",
        ")\n",
        "\n",
        "print(\"ü§ñ Modelo de chat criado!\")\n",
        "print(f\"Model: {llm.model}\")\n",
        "print(f\"Temperature: {llm.temperature}\")\n",
        "print(f\"Max tokens: {llm.max_tokens}\")\n",
        "\n",
        "# Teste r√°pido\n",
        "teste_resposta = llm.invoke(\"Ol√°! Como voc√™ est√°?\")\n",
        "print(f\"\\nüß™ Teste: {teste_resposta.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Passo 4: Criando um Prompt Template Eficiente\n\nAgora vem a parte crucial! Lembra do M√≥dulo 4 sobre Prompt Templates? Vamos criar um prompt que:\n1. Instruir a IA sobre como usar os documentos\n2. Dar contexto relevante\n3. Fazer a pergunta do usu√°rio\n\n**Dica!** üí° Um bom prompt de RAG √© tipo uma receita de bolo: tem que ter todos os ingredientes na ordem certa!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um prompt template espec√≠fico para RAG\n",
        "# Isso √© arte! Um prompt bem feito faz toda a diferen√ßa\n",
        "\n",
        "rag_prompt_template = \"\"\"\n",
        "Voc√™ √© um assistente especializado em tecnologia e startups brasileiras.\n",
        "\n",
        "Use APENAS as informa√ß√µes fornecidas no contexto abaixo para responder √† pergunta.\n",
        "Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o tem essa informa√ß√£o dispon√≠vel.\n",
        "\n",
        "Seja preciso, objetivo e baseie sua resposta exclusivamente no contexto fornecido.\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "PERGUNTA: {question}\n",
        "\n",
        "RESPOSTA:\n",
        "\"\"\"\n",
        "\n",
        "# Criando o PromptTemplate (lembra do M√≥dulo 4?)\n",
        "rag_prompt = PromptTemplate(\n",
        "    template=rag_prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "print(\"üìù Prompt template para RAG criado!\")\n",
        "print(\"\\nüéØ Vari√°veis do prompt:\", rag_prompt.input_variables)\n",
        "\n",
        "# Testando o formato do prompt\n",
        "exemplo_prompt = rag_prompt.format(\n",
        "    context=\"Nubank foi fundada em 2013...\",\n",
        "    question=\"Quando foi fundado o Nubank?\"\n",
        ")\n",
        "print(f\"\\nüìã Exemplo de prompt formatado:\\n{exemplo_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Passo 5: Montando a Chain RAG Completa\n\nAgora √© a hora da verdade! Vamos juntar tudo que vimos nos m√≥dulos anteriores numa chain RAG!\n\n**O que vai rolar:**\n1. Recebe a pergunta\n2. Busca documentos relevantes no vector store\n3. Monta o prompt com contexto + pergunta\n4. Envia pro LLM\n5. Retorna a resposta baseada nos documentos\n\n√â tipo montar um hamb√∫rguer: cada camada tem sua fun√ß√£o! üçî"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando a chain RAG completa!\n",
        "# Isso aqui √© a magia acontecendo - todos os m√≥dulos anteriores se juntando!\n",
        "\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,                           # Nosso modelo de chat\n",
        "    chain_type=\"stuff\",                # Tipo de chain (\"stuff\" = coloca tudo junto)\n",
        "    retriever=vector_store.as_retriever(  # Nosso retriever\n",
        "        search_type=\"similarity\",      # Tipo de busca\n",
        "        search_kwargs={\"k\": 3}         # Quantos documentos buscar\n",
        "    ),\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": rag_prompt            # Nosso prompt customizado\n",
        "    },\n",
        "    return_source_documents=True       # Retorna os documentos usados\n",
        ")\n",
        "\n",
        "print(\"üîó Chain RAG criada com sucesso!\")\n",
        "print(\"‚úÖ Componentes conectados:\")\n",
        "print(\"  - LLM: Gemini 2.0 Flash\")\n",
        "print(\"  - Vector Store: FAISS\")\n",
        "print(\"  - Embeddings: Google\")\n",
        "print(\"  - Prompt: Customizado para RAG\")\n",
        "print(\"  - Retriever: Top 3 documentos mais similares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Testando Nosso RAG na Pr√°tica!\n\nChegou a hora da verdade! Vamos fazer perguntas pro nosso RAG e ver se ele consegue responder baseado nos documentos.\n\n**Dica!** üí° Observe que as respostas v√£o citar informa√ß√µes espec√≠ficas dos documentos que carregamos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeira pergunta: sobre o Nubank\n",
        "pergunta1 = \"Quando foi fundado o Nubank e quem foram os fundadores?\"\n",
        "\n",
        "print(f\"ü§î Pergunta: {pergunta1}\")\n",
        "print(\"\\nüîç Buscando informa√ß√µes...\")\n",
        "\n",
        "resposta1 = rag_chain({\"query\": pergunta1})\n",
        "\n",
        "print(f\"\\nü§ñ Resposta: {resposta1['result']}\")\n",
        "print(f\"\\nüìö Documentos consultados:\")\n",
        "for i, doc in enumerate(resposta1['source_documents']):\n",
        "    print(f\"  {i+1}. Fonte: {doc.metadata['source']}\")\n",
        "    print(f\"     Trecho: {doc.page_content[:80]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segunda pergunta: sobre o Pix\n",
        "pergunta2 = \"O que √© o Pix e quando foi lan√ßado?\"\n",
        "\n",
        "print(f\"ü§î Pergunta: {pergunta2}\")\n",
        "print(\"\\nüîç Buscando informa√ß√µes...\")\n",
        "\n",
        "resposta2 = rag_chain({\"query\": pergunta2})\n",
        "\n",
        "print(f\"\\nü§ñ Resposta: {resposta2['result']}\")\n",
        "print(f\"\\nüìö Documentos consultados:\")\n",
        "for i, doc in enumerate(resposta2['source_documents']):\n",
        "    print(f\"  {i+1}. {doc.page_content[:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Terceira pergunta: testando os limites\n",
        "pergunta3 = \"Qual √© a capital do Jap√£o?\"\n",
        "\n",
        "print(f\"ü§î Pergunta: {pergunta3}\")\n",
        "print(\"\\nüîç Buscando informa√ß√µes...\")\n",
        "\n",
        "resposta3 = rag_chain({\"query\": pergunta3})\n",
        "\n",
        "print(f\"\\nü§ñ Resposta: {resposta3['result']}\")\n",
        "print(\"\\nüí° Repare que o RAG n√£o inventa informa√ß√µes que n√£o est√£o nos documentos!\")\n",
        "print(\"Isso √© uma caracter√≠stica MUITO importante do RAG bem implementado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Analisando a Performance do RAG\n\nVamos criar algumas m√©tricas simples para entender como nosso RAG est√° se saindo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos testar v√°rias perguntas e analisar os resultados\n",
        "perguntas_teste = [\n",
        "    \"Quando foi fundado o Nubank?\",\n",
        "    \"O que √© o Pix?\", \n",
        "    \"Quem fundou a Stone?\",\n",
        "    \"Em que ano o iFood foi criado?\",\n",
        "    \"Quanto a Didi pagou pela 99?\",\n",
        "    \"Qual a cor do cart√£o do Nubank?\",\n",
        "    \"Como funciona o delivery do iFood?\"\n",
        "]\n",
        "\n",
        "resultados_teste = []\n",
        "tempos_resposta = []\n",
        "\n",
        "print(\"üß™ Testando m√∫ltiplas perguntas...\\n\")\n",
        "\n",
        "import time\n",
        "\n",
        "for i, pergunta in enumerate(perguntas_teste):\n",
        "    print(f\"  {i+1}/7 - Testando: {pergunta[:40]}...\")\n",
        "    \n",
        "    # Medindo tempo de resposta\n",
        "    inicio = time.time()\n",
        "    resultado = rag_chain({\"query\": pergunta})\n",
        "    fim = time.time()\n",
        "    \n",
        "    tempo = fim - inicio\n",
        "    tempos_resposta.append(tempo)\n",
        "    \n",
        "    # Analisando a resposta\n",
        "    resposta = resultado['result']\n",
        "    num_docs = len(resultado['source_documents'])\n",
        "    tamanho_resposta = len(resposta)\n",
        "    \n",
        "    resultados_teste.append({\n",
        "        'pergunta': pergunta,\n",
        "        'resposta': resposta,\n",
        "        'tempo': tempo,\n",
        "        'num_docs': num_docs,\n",
        "        'tamanho': tamanho_resposta\n",
        "    })\n",
        "\n",
        "print(\"\\n‚úÖ Testes conclu√≠dos!\")\n",
        "print(f\"üìä Tempo m√©dio de resposta: {np.mean(tempos_resposta):.2f} segundos\")\n",
        "print(f\"üìè Tamanho m√©dio das respostas: {np.mean([r['tamanho'] for r in resultados_teste]):.0f} caracteres\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando a performance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Tempos de Resposta\n",
        "ax1.bar(range(len(tempos_resposta)), tempos_resposta, color='skyblue', alpha=0.8)\n",
        "ax1.set_xlabel('Pergunta #')\n",
        "ax1.set_ylabel('Tempo (segundos)')\n",
        "ax1.set_title('‚è±Ô∏è Tempos de Resposta do RAG')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Tamanho das Respostas\n",
        "tamanhos = [r['tamanho'] for r in resultados_teste]\n",
        "ax2.bar(range(len(tamanhos)), tamanhos, color='lightcoral', alpha=0.8)\n",
        "ax2.set_xlabel('Pergunta #')\n",
        "ax2.set_ylabel('Caracteres')\n",
        "ax2.set_title('üìè Tamanho das Respostas')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Estat√≠sticas\n",
        "print(f\"üìà Estat√≠sticas do RAG:\")\n",
        "print(f\"  ‚è±Ô∏è Tempo m√©dio: {np.mean(tempos_resposta):.2f}s (¬±{np.std(tempos_resposta):.2f}s)\")\n",
        "print(f\"  üìè Resposta m√©dia: {np.mean(tamanhos):.0f} chars (¬±{np.std(tamanhos):.0f} chars)\")\n",
        "print(f\"  üöÄ Tempo m√≠nimo: {np.min(tempos_resposta):.2f}s\")\n",
        "print(f\"  üêå Tempo m√°ximo: {np.max(tempos_resposta):.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configura√ß√µes Avan√ßadas do RAG\n\nAgora que temos um RAG funcionando, vamos ver algumas configura√ß√µes avan√ßadas que podem melhorar muito a performance!\n\n**Configura√ß√µes importantes:**\n- **k (n√∫mero de documentos)**: Quantos chunks buscar\n- **score_threshold**: Filtro de qualidade\n- **search_type**: Tipo de busca (similarity, mmr, etc.)\n\n**Dica!** üí° Nem sempre mais documentos = melhor resposta. √Äs vezes, poucos documentos relevantes s√£o melhores que muitos medianos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando diferentes configura√ß√µes de k (n√∫mero de documentos)\n",
        "ks_para_testar = [1, 2, 3, 5]\n",
        "pergunta_teste = \"O que voc√™ sabe sobre o Nubank?\"\n",
        "\n",
        "print(f\"üß™ Testando diferentes valores de k com a pergunta: '{pergunta_teste}'\\n\")\n",
        "\n",
        "for k in ks_para_testar:\n",
        "    print(f\"üìä Testando k={k}:\")\n",
        "    \n",
        "    # Criando retriever com k espec√≠fico\n",
        "    retriever_k = vector_store.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": k}\n",
        "    )\n",
        "    \n",
        "    # Chain tempor√°ria com esse k\n",
        "    chain_temp = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever_k,\n",
        "        chain_type_kwargs={\"prompt\": rag_prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    resultado = chain_temp({\"query\": pergunta_teste})\n",
        "    \n",
        "    print(f\"  üìù Resposta ({len(resultado['result'])} chars): {resultado['result'][:100]}...\")\n",
        "    print(f\"  üìö Documentos usados: {len(resultado['source_documents'])}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando busca com MMR (Maximum Marginal Relevance)\n",
        "# MMR ajuda a diversificar os resultados, evitando documentos muito similares\n",
        "\n",
        "print(\"üîç Comparando busca por Similarity vs MMR\\n\")\n",
        "\n",
        "pergunta_mmr = \"Me fale sobre startups de pagamento no Brasil\"\n",
        "\n",
        "# Busca por similaridade normal\n",
        "retriever_similarity = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "# Busca com MMR\n",
        "retriever_mmr = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\n",
        "        \"k\": 3,\n",
        "        \"lambda_mult\": 0.7  # Balance entre relev√¢ncia e diversidade\n",
        "    }\n",
        ")\n",
        "\n",
        "# Testando similarity\n",
        "docs_similarity = retriever_similarity.get_relevant_documents(pergunta_mmr)\n",
        "print(\"üìä Busca por Similarity:\")\n",
        "for i, doc in enumerate(docs_similarity):\n",
        "    print(f\"  {i+1}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\nüéØ Busca por MMR (mais diversificada):\")\n",
        "docs_mmr = retriever_mmr.get_relevant_documents(pergunta_mmr)\n",
        "for i, doc in enumerate(docs_mmr):\n",
        "    print(f\"  {i+1}. {doc.page_content[:60]}...\")\n",
        "\n",
        "print(\"\\nüí° MMR tenta trazer documentos relevantes MAS diversos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Exerc√≠cio Pr√°tico: Seu Primeiro RAG Customizado\n\nAgora √© sua vez! Vamos criar um RAG sobre um tema diferente.\n\n**Desafio:** üèÜ\n1. Crie documentos sobre **futebol brasileiro** \n2. Configure um RAG com esses documentos\n3. Teste perguntas sobre times, jogadores, etc.\n4. Compare diferentes configura√ß√µes de k\n\n**Dica!** üí° Use o c√≥digo que j√° vimos como base. Mude apenas os documentos e teste!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEU C√ìDIGO AQUI!\n",
        "# Crie documentos sobre futebol brasileiro\n",
        "\n",
        "documentos_futebol = [\n",
        "    # TODO: Adicione pelo menos 4 documentos sobre:\n",
        "    # - Times brasileiros\n",
        "    # - Jogadores famosos \n",
        "    # - Copas do Mundo\n",
        "    # - Campeonatos\n",
        "    \n",
        "    \"\"\"O Pel√©, cujo nome real √© Edson Arantes do Nascimento, √© considerado \n",
        "    o maior jogador de futebol de todos os tempos. Nascido em 1940, \n",
        "    jogou no Santos FC e na Sele√ß√£o Brasileira, conquistando 3 Copas do Mundo \n",
        "    (1958, 1962, 1970). Marcou mais de 1000 gols na carreira.\"\"\",\n",
        "    \n",
        "    # Adicione mais documentos aqui...\n",
        "]\n",
        "\n",
        "print(\"‚öΩ Exerc√≠cio: Crie seus documentos de futebol aqui!\")\n",
        "print(\"üìù Dica: Siga o mesmo padr√£o que usamos para tech brasileira\")\n",
        "\n",
        "# Complete o resto da implementa√ß√£o!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Melhorias Avan√ßadas para RAG\n\nAgora que voc√™ j√° domina o b√°sico, vamos ver algumas t√©cnicas avan√ßadas que podem turbinar seu RAG!\n\n### T√©cnicas Avan√ßadas:\n\n1. **Re-ranking**: Reordenar resultados com modelo adicional\n2. **Query Expansion**: Expandir a query original\n3. **Hybrid Search**: Combinar busca sem√¢ntica + keyword\n4. **Metadata Filtering**: Filtrar por metadados\n5. **Multi-hop RAG**: RAG com m√∫ltiplas etapas\n\n**Dica!** üí° Cada t√©cnica resolve um problema espec√≠fico. N√£o use todas de uma vez!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo: Filtros por Metadata\n",
        "# Vamos buscar apenas documentos de um tipo espec√≠fico\n",
        "\n",
        "print(\"üîç Testando filtros por metadata...\\n\")\n",
        "\n",
        "# Primeiro, vamos ver os metadados dispon√≠veis\n",
        "print(\"üìã Metadados dispon√≠veis nos documentos:\")\n",
        "for i, doc in enumerate(split_docs[:3]):\n",
        "    print(f\"  Doc {i+1}: {doc.metadata}\")\n",
        "\n",
        "# Busca com filtro (simulando que temos documentos de tipos diferentes)\n",
        "# Na vida real, voc√™ poderia filtrar por data, autor, categoria, etc.\n",
        "\n",
        "pergunta_filtro = \"Me fale sobre bancos digitais\"\n",
        "\n",
        "# Busca normal (sem filtro)\n",
        "resultados_normais = vector_store.similarity_search(pergunta_filtro, k=3)\n",
        "\n",
        "print(f\"\\nüîç Busca normal para: '{pergunta_filtro}'\")\n",
        "for i, doc in enumerate(resultados_normais):\n",
        "    print(f\"  {i+1}. Fonte: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"     Conte√∫do: {doc.page_content[:80]}...\\n\")\n",
        "\n",
        "print(\"üí° Em projetos reais, voc√™ pode filtrar por data, categoria, autor, etc.!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Fluxo Completo do RAG - Diagrama\n\nVamos visualizar todo o processo que implementamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando um diagrama do fluxo RAG\n",
        "from IPython.display import HTML\n",
        "\n",
        "diagrama_rag = \"\"\"\n",
        "<div class=\"mermaid\">\n",
        "graph TD\n",
        "    A[üë§ Usu√°rio faz pergunta] --> B[üîç Embedding da pergunta]\n",
        "    B --> C[üìä Busca no Vector Store]\n",
        "    C --> D[üìÑ Top-k documentos mais similares]\n",
        "    D --> E[üìù Monta prompt com contexto]\n",
        "    E --> F[ü§ñ LLM gera resposta]\n",
        "    F --> G[‚úÖ Resposta baseada nos documentos]\n",
        "    \n",
        "    H[üìö Base de Documentos] --> I[‚úÇÔ∏è Text Splitting]\n",
        "    I --> J[üß† Embeddings dos chunks]\n",
        "    J --> K[üíæ Armazenamento no FAISS]\n",
        "    K --> C\n",
        "    \n",
        "    style A fill:#e1f5fe\n",
        "    style G fill:#e8f5e8\n",
        "    style F fill:#fff3e0\n",
        "    style C fill:#f3e5f5\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "print(\"üéØ Fluxo Completo do RAG que Implementamos:\")\n",
        "print(\"\")\n",
        "print(\"üì• ETAPA 1 - PREPARA√á√ÉO (offline):\")\n",
        "print(\"  1. Carregar documentos\")\n",
        "print(\"  2. Fazer text splitting\")\n",
        "print(\"  3. Gerar embeddings\")\n",
        "print(\"  4. Armazenar no vector store\")\n",
        "print(\"\")\n",
        "print(\"üîÑ ETAPA 2 - CONSULTA (runtime):\")\n",
        "print(\"  1. Usu√°rio faz pergunta\")\n",
        "print(\"  2. Embedding da pergunta\")\n",
        "print(\"  3. Busca similaridade no vector store\")\n",
        "print(\"  4. Recupera top-k documentos\")\n",
        "print(\"  5. Monta prompt com contexto\")\n",
        "print(\"  6. LLM gera resposta\")\n",
        "print(\"  7. Retorna resposta + fontes\")\n",
        "\n",
        "HTML(diagrama_rag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö® Problemas Comuns e Solu√ß√µes\n\nTodo RAG tem seus desafios. Vamos ver os mais comuns e como resolver:\n\n### Problemas T√≠picos:\n\n1. **Chunk Size Ruim**: Peda√ßos muito grandes ou pequenos\n2. **Embeddings Inadequados**: Modelo n√£o captura bem o dom√≠nio\n3. **Prompt Mal Feito**: LLM n√£o entende como usar o contexto\n4. **Muitos/Poucos Documentos**: k muito alto/baixo\n5. **Qualidade dos Docs**: Documentos com informa√ß√£o ruim\n\n**Dica!** üí° RAG √© mais arte que ci√™ncia. Tem que testar e ajustar!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo: Diagn√≥stico de problemas comuns\n",
        "print(\"ü©∫ Diagn√≥stico do nosso RAG:\\n\")\n",
        "\n",
        "# 1. An√°lise do chunk size\n",
        "tamanhos_chunks = [len(doc.page_content) for doc in split_docs]\n",
        "print(f\"üìè An√°lise dos Chunks:\")\n",
        "print(f\"  - Tamanho m√©dio: {np.mean(tamanhos_chunks):.0f} chars\")\n",
        "print(f\"  - Menor chunk: {np.min(tamanhos_chunks)} chars\")\n",
        "print(f\"  - Maior chunk: {np.max(tamanhos_chunks)} chars\")\n",
        "print(f\"  - Total de chunks: {len(split_docs)}\")\n",
        "\n",
        "if np.mean(tamanhos_chunks) < 100:\n",
        "    print(\"  ‚ö†Ô∏è AVISO: Chunks muito pequenos podem perder contexto!\")\n",
        "elif np.mean(tamanhos_chunks) > 1000:\n",
        "    print(\"  ‚ö†Ô∏è AVISO: Chunks muito grandes podem confundir o LLM!\")\n",
        "else:\n",
        "    print(\"  ‚úÖ Tamanho dos chunks parece adequado!\")\n",
        "\n",
        "# 2. An√°lise da distribui√ß√£o\n",
        "print(f\"\\nüìä Distribui√ß√£o dos tamanhos:\")\n",
        "hist, bins = np.histogram(tamanhos_chunks, bins=5)\n",
        "for i in range(len(hist)):\n",
        "    print(f\"  {bins[i]:.0f}-{bins[i+1]:.0f} chars: {hist[i]} chunks\")\n",
        "\n",
        "# 3. Teste de qualidade das buscas\n",
        "print(f\"\\nüéØ Teste de qualidade:\")\n",
        "query_teste = \"Nubank\"\n",
        "docs_teste = vector_store.similarity_search_with_score(query_teste, k=3)\n",
        "\n",
        "scores = [score for _, score in docs_teste]\n",
        "print(f\"  - Scores de similaridade: {[f'{s:.3f}' for s in scores]}\")\n",
        "print(f\"  - Diferen√ßa max-min: {max(scores) - min(scores):.3f}\")\n",
        "\n",
        "if max(scores) - min(scores) < 0.1:\n",
        "    print(\"  ‚ö†Ô∏è AVISO: Scores muito pr√≥ximos - documentos podem ser muito similares!\")\n",
        "else:\n",
        "    print(\"  ‚úÖ Boa varia√ß√£o nos scores de similaridade!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Resumo: O que Aprendemos sobre RAG!\n\nCara, que jornada! üöÄ Implementamos um RAG completo do zero!\n\n### ‚úÖ O que fizemos:\n1. **Carregamos e processamos documentos** (M√≥dulo 8 vibes)\n2. **Criamos embeddings e vector store** (M√≥dulo 9 power)\n3. **Configuramos LLM e prompts** (M√≥dulos 2, 4, 6 unidos!)\n4. **Montamos a chain RAG completa**\n5. **Testamos e otimizamos**\n6. **Analisamos performance**\n\n### üß† Conceitos principais:\n- **RAG = Retrieval + Augmented + Generation**\n- **Similaridade sem√¢ntica** com embeddings\n- **Chunk size** √© cr√≠tico\n- **Prompt engineering** para RAG\n- **k (n√∫mero de documentos)** afeta qualidade\n\n### üîÆ Pr√≥ximos passos (M√≥dulo 11):\nNo pr√≥ximo m√≥dulo vamos ver **Agents e Tools** - imagina um RAG que pode usar ferramentas externas! ü§Ø\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-10_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste final: vamos fazer uma pergunta complexa!\n",
        "pergunta_final = \"Compare as diferentes empresas de tecnologia brasileira mencionadas nos documentos\"\n",
        "\n",
        "print(f\"üéØ TESTE FINAL: {pergunta_final}\\n\")\n",
        "\n",
        "resposta_final = rag_chain({\"query\": pergunta_final})\n",
        "\n",
        "print(f\"ü§ñ Resposta Final:\")\n",
        "print(resposta_final['result'])\n",
        "\n",
        "print(f\"\\nüìö Documentos consultados: {len(resposta_final['source_documents'])}\")\n",
        "print(f\"üìä Tamanho da resposta: {len(resposta_final['result'])} caracteres\")\n",
        "\n",
        "print(\"\\nüéâ Parab√©ns! Voc√™ implementou seu primeiro RAG com LangChain v0.3!\")\n",
        "print(\"üöÄ Agora voc√™ pode criar IAs que sabem de qualquer assunto!\")\n",
        "print(\"üìñ Nos pr√≥ximos m√≥dulos vamos ver Agents, Tools e muito mais!\")\n",
        "print(\"\\nüí™ Bora continuar essa jornada incr√≠vel!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Exerc√≠cio Final: Desafio RAG Completo\n\n**Seu desafio:** üèÜ\n\nImplemente um RAG sobre **culin√°ria brasileira** com:\n1. Pelo menos 5 documentos sobre pratos t√≠picos\n2. Metadata com regi√£o (Norte, Sul, etc.)\n3. Teste diferentes valores de k\n4. Compare similarity vs MMR\n5. Crie visualiza√ß√µes dos resultados\n\n**Bonus:** üåü\n- Adicione filtros por regi√£o\n- Teste diferentes chunk sizes\n- Calcule m√©tricas de performance\n\n**Dica!** üí° Use todo o c√≥digo que vimos como refer√™ncia. Voc√™ j√° tem tudo que precisa!\n\nLiiindo! Agora voc√™ √© um expert em RAG! üéä"
      ]
    }
  ]
}