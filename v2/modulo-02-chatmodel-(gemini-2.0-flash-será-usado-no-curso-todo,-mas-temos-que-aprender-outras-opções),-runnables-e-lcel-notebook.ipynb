{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 ChatModels, Runnables e LCEL: Os Ingredientes Mágicos do LangChain\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_01.png)\n\n**Bem-vindos ao Módulo 2! 🚀**\n\nTá, mas o que vamos ver aqui? Se no Módulo 1 a gente entendeu **por que** o LangChain é incrível, agora vamos botar a mão na massa e aprender os **ingredientes básicos** que fazem toda a magia acontecer!\n\nPensa assim: se o LangChain fosse uma cozinha brasileira, os **ChatModels** seriam os chefs (cada um com seu jeitinho), os **Runnables** seriam as panelas e utensílios que fazem tudo funcionar, e o **LCEL** seria a receita que conecta tudo de forma elegante!\n\n**Neste módulo você vai aprender:**\n- 🤖 **ChatModels**: Como conversar com diferentes IAs (Gemini, OpenAI, Anthropic...)\n- ⚙️ **Runnables**: A base de tudo no LangChain v0.2\n- 🔗 **LCEL**: A linguagem que conecta tudo de forma linda\n- 🎯 **Casos práticos**: Exemplos que você vai usar no dia a dia\n\n**Bora começar essa jornada incrível!** 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🛠️ Setup Inicial: Preparando Nossa Cozinha\n\nAntes de começar a cozinhar, precisamos organizar nossa cozinha! Vamos instalar e importar tudo que precisamos.\n\n**Dica do Pedro:** Sempre mantenha suas bibliotecas atualizadas. O mundo da IA muda mais rápido que moda de TikTok! 😄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instalando as bibliotecas necessárias\n!pip install langchain langchain-google-genai langchain-openai langchain-anthropic python-dotenv matplotlib -q\n\nprint(\"✅ Bibliotecas instaladas com sucesso!\")\nprint(\"🚀 Agora vamos importar tudo que precisamos...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports essenciais\nimport os\nfrom dotenv import load_dotenv\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\n\n# LangChain Core\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_core.runnables import RunnableLambda, RunnableSequence\nfrom langchain_core.output_parsers import StrOutputParser\n\n# ChatModels - Nossa coleção de chefs!\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n# from langchain_openai import ChatOpenAI  # Descomente se tiver API key\n# from langchain_anthropic import ChatAnthropic  # Descomente se tiver API key\n\nprint(\"📚 Imports realizados!\")\nprint(\"🎯 Vamos configurar as API keys agora...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configuração das API Keys\n# Dica do Pedro: NUNCA commite suas chaves no GitHub! Use variáveis de ambiente sempre!\n\n# Carrega variáveis de ambiente (se existir arquivo .env)\nload_dotenv()\n\n# Para o Google Gemini (nosso chef principal do curso)\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY') or input(\"Digite sua Google API Key (https://aistudio.google.com/): \")\nos.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n\n# Verificação\nif GOOGLE_API_KEY:\n    print(\"✅ Google API Key configurada!\")\n    print(\"🚀 Pronto para usar o Gemini 2.0 Flash!\")\nelse:\n    print(\"❌ Ops! Precisa da API Key do Google para continuar.\")\n    print(\"💡 Pegue a sua em: https://aistudio.google.com/\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 ChatModels: Conhecendo Nossos Chefs de IA\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_02.png)\n\nTá, mas o que é um **ChatModel**? \n\nPensa assim: um ChatModel é como um chef especializado. Cada um tem sua personalidade, seus pontos fortes e seu jeito único de \"cozinhar\" respostas. No LangChain, todos eles seguem a mesma \"etiqueta de cozinha\" - ou seja, a mesma interface!\n\n### 🔍 Principais ChatModels:\n\n1. **Gemini 2.0 Flash** (Google) - Nosso chef principal! Rápido, inteligente e gratuito\n2. **GPT-4** (OpenAI) - O chef famoso, muito bom mas pago\n3. **Claude** (Anthropic) - O chef cuidadoso e ético\n4. **Llama** (Meta) - O chef open source\n\n**Dica do Pedro:** Mesmo que você use só o Gemini no curso, entender a flexibilidade do LangChain é fundamental! Amanhã você pode precisar trocar de modelo com apenas 1 linha de código! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criando nosso ChatModel principal: Gemini 2.0 Flash\n# Por que Gemini? É gratuito, rápido e muito bom!\n\ngemini_model = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash-exp\",  # A versão mais nova e rápida\n    temperature=0.7,  # Criatividade moderada\n    max_tokens=1000,  # Limite de tokens na resposta\n    timeout=30,  # Timeout de 30 segundos\n)\n\nprint(\"🤖 ChatModel Gemini 2.0 Flash criado!\")\nprint(f\"📝 Modelo: {gemini_model.model_name}\")\nprint(f\"🌡️ Temperature: {gemini_model.temperature}\")\nprint(\"\\n💡 Dica: Temperature baixa = mais focado, alta = mais criativo\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Primeiro teste: conversando com o Gemini\n# Vamos fazer uma pergunta simples para testar\n\nmessage = HumanMessage(content=\"Explique em 2 frases o que é inteligência artificial\")\n\n# Invocando o modelo - aqui é onde a magia acontece!\nresponse = gemini_model.invoke([message])\n\nprint(\"🗣️ Pergunta:\", message.content)\nprint(\"\\n🤖 Resposta do Gemini:\")\nprint(response.content)\nprint(f\"\\n📊 Tipo da resposta: {type(response)}\")\nprint(f\"📈 Tamanho da resposta: {len(response.content)} caracteres\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📊 Visualizando Comparação de ChatModels\n\nVamos criar um gráfico para comparar as características dos principais ChatModels!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Comparação visual dos ChatModels\nmodels = ['Gemini 2.0\\nFlash', 'GPT-4o', 'Claude 3.5\\nSonnet', 'Llama 3.1']\nspeed = [9, 7, 6, 8]  # Velocidade (1-10)\ncost = [10, 3, 4, 10]  # Custo-benefício (gratuito = 10)\nquality = [9, 10, 9, 7]  # Qualidade das respostas\n\nx = np.arange(len(models))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Criando as barras\nrects1 = ax.bar(x - width, speed, width, label='Velocidade', color='#4285F4', alpha=0.8)\nrects2 = ax.bar(x, cost, width, label='Custo-benefício', color='#34A853', alpha=0.8)\nrects3 = ax.bar(x + width, quality, width, label='Qualidade', color='#EA4335', alpha=0.8)\n\n# Configuração do gráfico\nax.set_ylabel('Score (1-10)', fontsize=12)\nax.set_title('🤖 Comparação de ChatModels - Por que escolhemos Gemini?', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Adicionando valores nas barras\ndef add_value_labels(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate(f'{height}',\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', fontweight='bold')\n\nadd_value_labels(rects1)\nadd_value_labels(rects2)\nadd_value_labels(rects3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📈 Como você pode ver, o Gemini 2.0 Flash tem o melhor equilíbrio!\")\nprint(\"💰 Principalmente por ser gratuito e muito rápido!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ⚙️ Runnables: A Base de Tudo no LangChain v0.2\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_03.png)\n\nAgora vem a parte **FUNDAMENTAL**: os **Runnables**!\n\nTá, mas o que diabos é um Runnable? 🤔\n\nPensa assim: um Runnable é como uma **peça de LEGO padronizada**. Não importa se é uma peça pequena ou grande, todas têm aqueles \"pontinhos\" que se encaixam perfeitamente em outras peças!\n\nNo LangChain v0.2, **TUDO** é um Runnable:\n- ChatModels são Runnables\n- Prompt Templates são Runnables  \n- Output Parsers são Runnables\n- Chains são Runnables\n\n### 🎯 Interface Padrão dos Runnables:\n\n$$\\text{Runnable} = \\{\\text{invoke, stream, batch, ainvoke, astream, abatch}\\}$$\n\n**Dica do Pedro:** Essa padronização é GENIAL! Uma vez que você aprende como um Runnable funciona, você sabe como TODOS funcionam! É como aprender a dirigir - depois você dirige qualquer carro! 🚗"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrando que ChatModel É um Runnable\nprint(\"🔍 Vamos investigar nosso ChatModel...\")\nprint(f\"📋 Tipo: {type(gemini_model)}\")\nprint(f\"🧬 É um Runnable? {hasattr(gemini_model, 'invoke')}\")\n\n# Verificando os métodos disponíveis\nrunnable_methods = ['invoke', 'stream', 'batch', 'ainvoke', 'astream', 'abatch']\navailable_methods = [method for method in runnable_methods if hasattr(gemini_model, method)]\n\nprint(f\"\\n⚙️ Métodos Runnable disponíveis: {available_methods}\")\nprint(f\"✅ Total: {len(available_methods)}/{len(runnable_methods)}\")\n\nprint(\"\\n💡 Isso significa que podemos usar o ChatModel de várias formas diferentes!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testando diferentes formas de usar um Runnable\nfrom langchain_core.messages import HumanMessage\n\n# 1. invoke() - Forma síncrona simples\nprint(\"1️⃣ INVOKE - Chamada simples:\")\nresult1 = gemini_model.invoke([HumanMessage(content=\"Diga olá em português\")])\nprint(f\"   Resposta: {result1.content}\")\n\n# 2. batch() - Múltiplas perguntas de uma vez\nprint(\"\\n2️⃣ BATCH - Múltiplas perguntas:\")\nbatch_messages = [\n    [HumanMessage(content=\"Qual é a capital do Brasil?\")],\n    [HumanMessage(content=\"Qual é a capital da França?\")],\n    [HumanMessage(content=\"Qual é a capital do Japão?\")]\n]\n\nresults = gemini_model.batch(batch_messages)\nfor i, result in enumerate(results, 1):\n    print(f\"   Pergunta {i}: {result.content}\")\n\nprint(\"\\n💡 O batch é muito útil para processar várias perguntas em paralelo!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📊 Diagrama: Anatomia de um Runnable\n\nVamos visualizar como um Runnable funciona internamente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Input] --> B[Runnable.invoke]\n    B --> C{Processamento Interno}\n    C --> D[Transform]\n    D --> E[Execute]\n    E --> F[Output]\n    \n    G[Batch Input] --> H[Runnable.batch]\n    H --> I[Parallel Processing]\n    I --> J[Multiple Outputs]\n    \n    K[Stream Input] --> L[Runnable.stream]\n    L --> M[Streaming Output]\n    \n    style B fill:#4285F4,stroke:#333,stroke-width:3px,color:#fff\n    style H fill:#34A853,stroke:#333,stroke-width:3px,color:#fff\n    style L fill:#EA4335,stroke:#333,stroke-width:3px,color:#fff\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔗 LCEL: A Linguagem Que Une Tudo\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_04.png)\n\n**LCEL** = **LangChain Expression Language**\n\nTá, agora vem a parte mais **LINDA** do LangChain v0.2! 🤩\n\nO LCEL é como se fosse a **gramática** que conecta todos os Runnables de forma super elegante. É tipo aquele amigo que apresenta todo mundo na festa e faz todo mundo se dar bem!\n\n### 🎯 A Magia do Pipe Operator:\n\n$$\\text{chain} = \\text{runnable}_1 \\; | \\; \\text{runnable}_2 \\; | \\; \\text{runnable}_3$$\n\n**Analogia do Pedro:** É como uma linha de produção de brigadeiro! 🍫\n- Primeiro você faz o doce (ChatModel)\n- Depois você enrola (OutputParser) \n- Por fim você coloca na forminha (Formatter)\n\nCada etapa recebe o resultado da anterior e passa para a próxima!\n\n**Dica do Pedro:** O pipe `|` no LCEL funciona igual ao pipe do Linux/Unix. Se você já usou terminal, vai se sentir em casa! 😎"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criando nossa primeira chain com LCEL\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n# Vamos criar uma chain que:\n# 1. Recebe uma pergunta\n# 2. Adiciona contexto brasileiro \n# 3. Passa para o ChatModel\n# 4. Formata a saída\n\n# Função para adicionar contexto brasileiro\ndef add_brazilian_context(topic: str) -> list:\n    context_message = f\"Responda sobre '{topic}' considerando o contexto brasileiro e de forma didática.\"\n    return [HumanMessage(content=context_message)]\n\n# Função para formatar a saída\ndef format_response(ai_message) -> str:\n    return f\"🤖 **Resposta do Gemini:**\\n{ai_message.content}\\n\\n✅ Processado em {datetime.now().strftime('%H:%M:%S')}\"\n\n# Criando os Runnables\ncontext_adder = RunnableLambda(add_brazilian_context)\noutput_parser = StrOutputParser()  # Extrai só o conteúdo da mensagem\nformatter = RunnableLambda(format_response)\n\nprint(\"🔧 Runnables individuais criados:\")\nprint(f\"   📝 Context Adder: {type(context_adder)}\")\nprint(f\"   🤖 Gemini Model: {type(gemini_model)}\")\nprint(f\"   📤 Formatter: {type(formatter)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AQUI VEM A MAGIA DO LCEL! ✨\n# Conectando tudo com o pipe operator\n\nbrazilian_ai_chain = (\n    context_adder          # Adiciona contexto brasileiro\n    | gemini_model        # Processa com Gemini\n    | formatter           # Formata a resposta\n)\n\nprint(\"🎯 Chain criada com LCEL!\")\nprint(f\"📋 Tipo da chain: {type(brazilian_ai_chain)}\")\nprint(f\"🧬 A chain também é um Runnable? {hasattr(brazilian_ai_chain, 'invoke')}\")\n\n# Testando nossa chain\nprint(\"\\n🚀 Testando a chain...\")\ntopic = \"inteligência artificial no agronegócio\"\nresult = brazilian_ai_chain.invoke(topic)\n\nprint(f\"\\n📥 Input: {topic}\")\nprint(f\"📤 Output:\\n{result}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📊 Visualizando Performance: Invoke vs Batch vs Stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n\n# Vamos comparar a performance dos diferentes métodos Runnable\ntopics = [\"energia solar no Brasil\", \"carnaval de rua\", \"programação em Python\"]\n\n# Teste 1: Invoke sequencial\nstart_time = time.time()\nsequential_results = []\nfor topic in topics:\n    result = brazilian_ai_chain.invoke(topic)\n    sequential_results.append(result)\nsequential_time = time.time() - start_time\n\n# Teste 2: Batch paralelo\nstart_time = time.time()\nbatch_results = brazilian_ai_chain.batch(topics)\nbatch_time = time.time() - start_time\n\n# Criando gráfico de comparação\nmethods = ['Sequential\\n(invoke)', 'Parallel\\n(batch)']\ntimes = [sequential_time, batch_time]\ncolors = ['#EA4335', '#34A853']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Gráfico de tempo\nbars = ax1.bar(methods, times, color=colors, alpha=0.8)\nax1.set_ylabel('Tempo (segundos)', fontsize=12)\nax1.set_title('⚡ Performance: Sequential vs Batch', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Adicionando valores nas barras\nfor bar, time_val in zip(bars, times):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n\n# Gráfico de eficiência\nefficiency = [100, (sequential_time / batch_time) * 100]\nax2.bar(methods, efficiency, color=colors, alpha=0.8)\nax2.set_ylabel('Eficiência (%)', fontsize=12)\nax2.set_title('📈 Eficiência Relativa', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Adicionando valores\nfor i, eff in enumerate(efficiency):\n    ax2.text(i, eff + 5, f'{eff:.1f}%', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"🏁 Resultados:\")\nprint(f\"   ⏱️ Sequential: {sequential_time:.2f}s\")\nprint(f\"   ⚡ Batch: {batch_time:.2f}s\")\nprint(f\"   📊 Speedup: {sequential_time/batch_time:.1f}x mais rápido!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Casos Práticos: LCEL na Vida Real\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_05.png)\n\nAgora vamos ver como usar tudo isso na **vida real**! Vou mostrar 3 casos super práticos que você vai usar no dia a dia.\n\n**Dica do Pedro:** Esses exemplos são a base para os próximos módulos. No Módulo 3 vamos ver Prompt Templates, no 4 Chains mais complexas, e assim por diante! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso Prático 1: Chatbot Especializado em Tecnologia Brasileira\nfrom langchain_core.runnables import RunnableLambda\n\ndef create_tech_context(question: str) -> list:\n    \"\"\"Cria contexto especializado em tecnologia brasileira\"\"\"\n    system_msg = SystemMessage(content=\"\"\"\n    Você é um especialista em tecnologia e inovação no Brasil.\n    Suas respostas devem:\n    - Mencionar exemplos brasileiros quando possível\n    - Ser técnicas mas acessíveis\n    - Incluir perspectivas do mercado nacional\n    \"\"\")\n    \n    human_msg = HumanMessage(content=question)\n    return [system_msg, human_msg]\n\ndef format_tech_response(ai_message) -> str:\n    \"\"\"Formata resposta técnica\"\"\"\n    return f\"\"\"\n🇧🇷 **TechBot Brasil** 🤖\n\n{ai_message.content}\n\n---\n💡 *Resposta gerada em {datetime.now().strftime('%d/%m/%Y às %H:%M')}*\n    \"\"\".strip()\n\n# Criando a chain especializada\ntech_bot_chain = (\n    RunnableLambda(create_tech_context)  # Contexto tech brasileiro\n    | gemini_model                       # Processamento IA\n    | RunnableLambda(format_tech_response)  # Formatação\n)\n\n# Teste do chatbot\nquestion = \"Quais são as principais startups de IA no Brasil?\"\nresponse = tech_bot_chain.invoke(question)\n\nprint(\"💬 Pergunta:\", question)\nprint(\"\\n\" + \"=\"*60)\nprint(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso Prático 2: Sistema de Análise de Sentimento\ndef analyze_sentiment(text: str) -> list:\n    \"\"\"Prepara texto para análise de sentimento\"\"\"\n    prompt = f\"\"\"\n    Analise o sentimento do seguinte texto brasileiro:\n    \n    \"{text}\"\n    \n    Responda apenas com:\n    - Sentimento: [POSITIVO/NEGATIVO/NEUTRO]\n    - Confiança: [0-100]%\n    - Palavras-chave: [liste as principais]\n    \"\"\"\n    return [HumanMessage(content=prompt)]\n\ndef parse_sentiment(ai_message) -> dict:\n    \"\"\"Converte resposta em dicionário estruturado\"\"\"\n    content = ai_message.content\n    \n    # Parse simples (em produção usaríamos regex ou parser mais robusto)\n    lines = content.split('\\n')\n    result = {\n        'text_analyzed': 'Texto analisado',\n        'sentiment': 'N/A',\n        'confidence': 'N/A', \n        'keywords': 'N/A',\n        'raw_response': content\n    }\n    \n    for line in lines:\n        if 'Sentimento:' in line:\n            result['sentiment'] = line.split(':')[1].strip()\n        elif 'Confiança:' in line:\n            result['confidence'] = line.split(':')[1].strip()\n        elif 'Palavras-chave:' in line:\n            result['keywords'] = line.split(':')[1].strip()\n    \n    return result\n\n# Chain de análise de sentimento\nsentiment_chain = (\n    RunnableLambda(analyze_sentiment)    # Prepara análise\n    | gemini_model                       # Processa\n    | RunnableLambda(parse_sentiment)    # Estrutura resultado\n)\n\n# Teste com diferentes textos\ntexts = [\n    \"Adorei o novo update do app! Ficou muito mais rápido e fácil de usar!\",\n    \"O atendimento foi péssimo, demorou 2 horas para resolver um problema simples.\",\n    \"O produto chegou conforme esperado, dentro do prazo.\"\n]\n\nprint(\"🔍 Análise de Sentimento em Lote:\")\nprint(\"=\"*50)\n\nresults = sentiment_chain.batch(texts)\nfor i, (text, result) in enumerate(zip(texts, results), 1):\n    print(f\"\\n{i}. Texto: {text[:50]}...\")\n    print(f\"   Sentimento: {result['sentiment']}\")\n    print(f\"   Confiança: {result['confidence']}\")\n    print(f\"   Palavras-chave: {result['keywords']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔄 Diagrama: Fluxo Completo de uma Chain LCEL\n\nVamos visualizar como nossa chain de sentimento funciona:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\nflowchart TD\n    A[\"Input: Texto para análise\"] --> B[\"RunnableLambda: analyze_sentiment\"]\n    B --> C[\"Preparação do Prompt\"]\n    C --> D[\"ChatModel: Gemini 2.0\"]\n    D --> E[\"IA processa e analisa\"]\n    E --> F[\"RunnableLambda: parse_sentiment\"]\n    F --> G[\"Estruturação dos dados\"]\n    G --> H[\"Output: Dict estruturado\"]\n    \n    I[\"Batch Input: Lista de textos\"] --> J[\"Processamento Paralelo\"]\n    J --> K[\"Múltiplos Outputs\"]\n    \n    style D fill:#4285F4,stroke:#333,stroke-width:3px,color:#fff\n    style B fill:#34A853,stroke:#333,stroke-width:2px,color:#fff\n    style F fill:#34A853,stroke:#333,stroke-width:2px,color:#fff\n    style H fill:#EA4335,stroke:#333,stroke-width:2px,color:#fff\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso Prático 3: Tradutor Contextual com Explicações\ndef create_translation_prompt(data: dict) -> list:\n    \"\"\"Cria prompt para tradução contextual\"\"\"\n    text = data['text']\n    source_lang = data['from']\n    target_lang = data['to']\n    \n    prompt = f\"\"\"\n    Traduza o seguinte texto do {source_lang} para {target_lang}.\n    \n    Texto original: \"{text}\"\n    \n    Forneça:\n    1. Tradução direta\n    2. Tradução contextual (se diferente)\n    3. Explicação de nuances culturais (se houver)\n    \n    Seja preciso e didático.\n    \"\"\"\n    \n    return [HumanMessage(content=prompt)]\n\ndef format_translation(ai_message) -> str:\n    \"\"\"Formata a resposta de tradução\"\"\"\n    return f\"\"\"\n🌍 **Tradutor Contextual IA**\n\n{ai_message.content}\n\n---\n🤖 *Powered by Gemini 2.0 Flash - {datetime.now().strftime('%H:%M:%S')}*\n    \"\"\".strip()\n\n# Chain de tradução\ntranslation_chain = (\n    RunnableLambda(create_translation_prompt)  # Prepara tradução\n    | gemini_model                             # Traduz\n    | RunnableLambda(format_translation)       # Formata\n)\n\n# Teste de tradução\ntranslation_request = {\n    'text': 'Saudade é uma palavra única do português que expressa a nostalgia por algo ou alguém.',\n    'from': 'português',\n    'to': 'inglês'\n}\n\nresult = translation_chain.invoke(translation_request)\nprint(result)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Monitoramento e Debug de Chains\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_06.png)\n\nUma coisa **FUNDAMENTAL** quando você trabalha com chains em produção: saber o que está acontecendo! Vamos aprender a monitorar e debugar nossas chains.\n\n**Dica do Pedro:** Em produção, monitoramento não é luxo, é **NECESSIDADE**! Você precisa saber se sua IA está funcionando bem ou se está \"maluquinha\"! 😅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sistema de monitoramento simples para chains\nimport time\nfrom typing import Any\n\nclass ChainMonitor:\n    def __init__(self):\n        self.metrics = {\n            'total_calls': 0,\n            'total_time': 0,\n            'errors': 0,\n            'avg_response_time': 0,\n            'calls_history': []\n        }\n    \n    def monitor_call(self, func):\n        \"\"\"Decorator para monitorar chamadas de chain\"\"\"\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            self.metrics['total_calls'] += 1\n            \n            try:\n                result = func(*args, **kwargs)\n                success = True\n                error = None\n            except Exception as e:\n                self.metrics['errors'] += 1\n                success = False\n                error = str(e)\n                result = None\n            \n            end_time = time.time()\n            call_time = end_time - start_time\n            self.metrics['total_time'] += call_time\n            self.metrics['avg_response_time'] = self.metrics['total_time'] / self.metrics['total_calls']\n            \n            # Histórico da chamada\n            self.metrics['calls_history'].append({\n                'timestamp': datetime.now(),\n                'duration': call_time,\n                'success': success,\n                'error': error,\n                'input_size': len(str(args[0])) if args else 0\n            })\n            \n            return result\n        return wrapper\n    \n    def get_stats(self):\n        \"\"\"Retorna estatísticas atuais\"\"\"\n        return self.metrics.copy()\n    \n    def plot_performance(self):\n        \"\"\"Plota gráfico de performance\"\"\"\n        if not self.metrics['calls_history']:\n            print(\"Nenhuma chamada registrada ainda!\")\n            return\n        \n        times = [call['duration'] for call in self.metrics['calls_history']]\n        calls = list(range(1, len(times) + 1))\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Gráfico de tempo por chamada\n        ax1.plot(calls, times, 'o-', color='#4285F4', linewidth=2, markersize=8)\n        ax1.axhline(y=self.metrics['avg_response_time'], color='#EA4335', \n                   linestyle='--', label=f'Média: {self.metrics[\"avg_response_time\"]:.2f}s')\n        ax1.set_xlabel('Número da Chamada')\n        ax1.set_ylabel('Tempo de Resposta (s)')\n        ax1.set_title('⏱️ Performance por Chamada')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Gráfico de estatísticas gerais\n        stats_labels = ['Total\\nChamadas', 'Sucessos', 'Erros', 'Tempo\\nMédio (s)']\n        stats_values = [\n            self.metrics['total_calls'],\n            self.metrics['total_calls'] - self.metrics['errors'],\n            self.metrics['errors'],\n            self.metrics['avg_response_time']\n        ]\n        \n        colors = ['#4285F4', '#34A853', '#EA4335', '#FBBC04']\n        bars = ax2.bar(stats_labels, stats_values, color=colors, alpha=0.8)\n        ax2.set_title('📊 Estatísticas Gerais')\n        ax2.set_ylabel('Valores')\n        \n        # Adicionando valores nas barras\n        for bar, value in zip(bars, stats_values):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stats_values)*0.01,\n                    f'{value:.2f}' if isinstance(value, float) else str(value),\n                    ha='center', va='bottom', fontweight='bold')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Criando monitor\nmonitor = ChainMonitor()\n\nprint(\"📊 Monitor de Chain criado!\")\nprint(\"🔍 Agora vamos testar com nossa chain de sentimento...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testando monitoramento com diferentes inputs\ntest_texts = [\n    \"Este produto é incrível!\",\n    \"Que serviço horrível, nunca mais volto!\",\n    \"O atendimento foi ok, nada demais.\",\n    \"Excelente qualidade, recomendo muito!\",\n    \"Preço muito alto para o que oferece.\"\n]\n\n# Wrapper para monitorar nossa chain\n@monitor.monitor_call\ndef monitored_sentiment_analysis(text):\n    return sentiment_chain.invoke(text)\n\nprint(\"🚀 Executando análises monitoradas...\")\nprint(\"=\"*50)\n\nfor i, text in enumerate(test_texts, 1):\n    print(f\"\\n📝 Teste {i}: {text}\")\n    result = monitored_sentiment_analysis(text)\n    print(f\"   ✅ Sentimento: {result['sentiment']} | Confiança: {result['confidence']}\")\n\n# Mostrando estatísticas\nstats = monitor.get_stats()\nprint(f\"\\n📊 **Estatísticas Finais:**\")\nprint(f\"   📞 Total de chamadas: {stats['total_calls']}\")\nprint(f\"   ⏱️ Tempo médio: {stats['avg_response_time']:.2f}s\")\nprint(f\"   ✅ Taxa de sucesso: {((stats['total_calls'] - stats['errors']) / stats['total_calls'] * 100):.1f}%\")\nprint(f\"   🕒 Tempo total: {stats['total_time']:.2f}s\")\n\n# Plotando performance\nmonitor.plot_performance()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎮 Exercícios Práticos\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_07.png)\n\nAgora é sua vez de botar a mão na massa! Vou dar dois desafios para você praticar tudo que aprendemos.\n\n**Dica do Pedro:** Não tenha medo de errar! Programação se aprende errando e consertando. É como aprender a andar de bicicleta - você vai cair algumas vezes, mas depois nunca mais esquece! 🚴‍♂️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercício 1: Criador de Piadas Brasileiras\n\n**Desafio:** Crie uma chain que:\n1. Receba um tema\n2. Gere uma piada brasileira sobre o tema\n3. Avalie se a piada é adequada para crianças\n4. Formate a saída de forma divertida\n\n**Requisitos:**\n- Use LCEL para conectar os Runnables\n- Inclua contexto brasileiro nas piadas\n- A saída deve ser estruturada e bonita\n\n```python\n# Seu código aqui!\n# Dica: Use as funções que criamos como inspiração\n\n# def create_joke_prompt(theme: str) -> list:\n#     # Sua implementação\n\n# def evaluate_joke(ai_message) -> dict:\n#     # Sua implementação\n\n# joke_chain = (\n#     RunnableLambda(create_joke_prompt)\n#     | gemini_model\n#     | RunnableLambda(evaluate_joke)\n# )\n\n# Teste com: joke_chain.invoke(\"programação\")\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Espaço para sua solução do Exercício 1\n# Descomente e complete o código abaixo:\n\n# def create_joke_prompt(theme: str) -> list:\n#     \"\"\"Cria prompt para gerar piada brasileira\"\"\"\n#     # SEU CÓDIGO AQUI\n#     pass\n\n# def evaluate_joke(ai_message) -> dict:\n#     \"\"\"Avalia e formata a piada\"\"\"\n#     # SEU CÓDIGO AQUI\n#     pass\n\n# # Criando a chain\n# joke_chain = (\n#     RunnableLambda(create_joke_prompt)\n#     | gemini_model\n#     | RunnableLambda(evaluate_joke)\n# )\n\n# # Teste\n# result = joke_chain.invoke(\"programação\")\n# print(result)\n\nprint(\"💭 Complete o exercício acima descomentando e implementando as funções!\")\nprint(\"🎯 Objetivo: Criar um gerador de piadas brasileiras inteligente\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎯 Exercício 2: Sistema de Recomendação de Filmes\n\n**Desafio:** Crie uma chain que:\n1. Receba preferências do usuário (gênero, ano, país)\n2. Gere recomendações personalizadas\n3. Inclua justificativas para cada recomendação\n4. Permita processar múltiplos usuários em batch\n\n**Requisitos:**\n- Use batch processing para múltiplos usuários\n- Inclua filmes brasileiros nas recomendações\n- Estruture a saída como JSON\n- Adicione sistema de rating\n\n```python\n# Estrutura sugerida:\n# user_preferences = {\n#     'genres': ['ação', 'comédia'],\n#     'year_range': '2010-2023',\n#     'include_brazilian': True,\n#     'max_recommendations': 5\n# }\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Espaço para sua solução do Exercício 2\n# Implemente um sistema de recomendação de filmes\n\n# Exemplo de entrada:\nuser_preferences_example = {\n    'genres': ['ação', 'comédia'],\n    'year_range': '2010-2023', \n    'include_brazilian': True,\n    'max_recommendations': 3\n}\n\nprint(\"🎬 Exercício 2: Sistema de Recomendação de Filmes\")\nprint(f\"📝 Exemplo de entrada: {user_preferences_example}\")\nprint(\"\\n💡 Dicas:\")\nprint(\"   - Use o padrão LCEL que aprendemos\")\nprint(\"   - Teste com batch para múltiplos usuários\")\nprint(\"   - Inclua filmes nacionais nas recomendações\")\nprint(\"   - Estruture bem a saída\")\n\n# SEU CÓDIGO AQUI!\n# def create_movie_prompt(preferences: dict) -> list:\n#     pass\n\n# def parse_recommendations(ai_message) -> dict:\n#     pass\n\n# movie_chain = ...\n\nprint(\"\\n🚀 Implemente sua solução acima!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Resumo do Módulo: O que Aprendemos?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_08.png)\n\n**Liiindo!** 🎉 Chegamos ao final do Módulo 2! Vamos fazer uma recap do que conquistamos:\n\n### 🧠 **Conceitos Fundamentais:**\n\n1. **🤖 ChatModels** - Os chefs de IA\n   - Gemini 2.0 Flash como nosso modelo principal\n   - Interface padronizada para todos os modelos\n   - Flexibilidade para trocar modelos facilmente\n\n2. **⚙️ Runnables** - A base de tudo\n   - Interface unificada: invoke, batch, stream\n   - Composabilidade total\n   - Processamento paralelo com batch\n\n3. **🔗 LCEL** - A linguagem elegante\n   - Pipe operator `|` para conectar tudo\n   - Chains como Runnables\n   - Código limpo e legível\n\n### 📊 **Fórmula do Sucesso:**\n\n$$\\text{ChatModel} + \\text{LCEL} + \\text{Runnables} = \\text{Aplicações Incríveis}$$\n\n### 🚀 **Preparação para os Próximos Módulos:**\n- **Módulo 3:** Prompt Templates (para deixar nossos prompts mais profissionais)\n- **Módulo 4:** Chains (chains mais complexas e poderosas)\n- **Módulo 5:** Memory Systems (para dar memória às conversas)\n\n**Dica do Pedro:** O que aprendemos aqui é a **BASE** de tudo! Nos próximos módulos vamos construir coisas cada vez mais incríveis em cima desses conceitos! 🏗️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 🎊 Demonstração Final: Juntando Tudo que Aprendemos!\n# Vamos criar uma chain completa que demonstra todos os conceitos\n\ndef create_final_demo():\n    \"\"\"Demonstração final integrando todos os conceitos\"\"\"\n    \n    # 1. ChatModel configurado\n    model = gemini_model\n    \n    # 2. Runnables customizados\n    def demo_input_processor(data: dict) -> list:\n        user_name = data.get('name', 'Usuário')\n        topic = data.get('topic', 'tecnologia')\n        \n        system_msg = SystemMessage(content=f\"\"\"\n        Você é um assistente educativo brasileiro chamado GeminiBot.\n        Sempre seja didático, use exemplos brasileiros e mantenha um tom amigável.\n        O usuário se chama {user_name}.\n        \"\"\")\n        \n        human_msg = HumanMessage(content=f\"Explique sobre {topic} de forma simples e interessante\")\n        return [system_msg, human_msg]\n    \n    def demo_output_formatter(ai_message) -> dict:\n        return {\n            'response': ai_message.content,\n            'timestamp': datetime.now().isoformat(),\n            'model_used': 'gemini-2.0-flash-exp',\n            'tokens_approx': len(ai_message.content.split()),\n            'processed_by': 'LangChain v0.2 + LCEL'\n        }\n    \n    # 3. LCEL Chain\n    final_chain = (\n        RunnableLambda(demo_input_processor)  # Processa entrada\n        | model                               # ChatModel processa\n        | RunnableLambda(demo_output_formatter)  # Formata saída\n    )\n    \n    return final_chain\n\n# Criando e testando nossa demonstração final\nfinal_demo_chain = create_final_demo()\n\n# Teste individual\ntest_data = {\n    'name': 'Pedro',\n    'topic': 'inteligência artificial no Brasil'\n}\n\nresult = final_demo_chain.invoke(test_data)\n\nprint(\"🎊 **DEMONSTRAÇÃO FINAL - MÓDULO 2** 🎊\")\nprint(\"=\"*60)\nprint(f\"👤 Usuário: {test_data['name']}\")\nprint(f\"📝 Tópico: {test_data['topic']}\")\nprint(f\"\\n🤖 **Resposta do GeminiBot:**\")\nprint(result['response'])\nprint(f\"\\n📊 **Metadados:**\")\nprint(f\"   ⏰ Timestamp: {result['timestamp']}\")\nprint(f\"   🤖 Modelo: {result['model_used']}\")\nprint(f\"   📏 Tokens aprox: {result['tokens_approx']}\")\nprint(f\"   ⚙️ Processado por: {result['processed_by']}\")\n\nprint(f\"\\n✅ **Parabéns! Você dominou os fundamentos do LangChain v0.2!** 🎉\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔜 Próximos Passos: Sua Jornada Continua!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-02_img_09.png)\n\n**Bora que a jornada não para por aqui!** 🚀\n\n### 📚 **No Módulo 3 - Prompt Templates e Output Parsers:**\n- Como criar prompts profissionais e reutilizáveis\n- Parsers para estruturar respostas da IA\n- Templates dinâmicos e condicionais\n- Validação e tratamento de erros\n\n### 💡 **Dica do Pedro para Continuar Estudando:**\n1. **Pratique os exercícios** - A prática leva à perfeição!\n2. **Experimente variações** - Mude parâmetros, teste diferentes inputs\n3. **Monitore performance** - Use as ferramentas que criamos\n4. **Documente seu aprendizado** - Faça anotações e exemplos próprios\n\n### 🎯 **Conecte-se:**\n- Continue praticando com o Gemini 2.0 Flash\n- Experimente outros ChatModels se tiver acesso\n- Compartilhe suas criações com a comunidade\n- Prepare-se para os desafios dos próximos módulos!\n\n**Até o próximo módulo, e lembrem-se: \"A melhor forma de aprender IA é fazendo IA!\"** 🤖✨\n\n---\n\n*Pedro Nunes Guth - Instrutor de IA e AWS*  \n*\"Transformando conceitos complexos em conhecimento prático, um módulo de cada vez!\"* 🧠💡"
      ]
    }
  ]
}