{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  ChatModels, Runnables e LCEL: Os Ingredientes MÃ¡gicos do LangChain\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_01.png)\n\n**Bem-vindos ao MÃ³dulo 2! ğŸš€**\n\nTÃ¡, mas o que vamos ver aqui? Se no MÃ³dulo 1 a gente entendeu **por que** o LangChain Ã© incrÃ­vel, agora vamos botar a mÃ£o na massa e aprender os **ingredientes bÃ¡sicos** que fazem toda a magia acontecer!\n\nPensa assim: se o LangChain fosse uma cozinha brasileira, os **ChatModels** seriam os chefs (cada um com seu jeitinho), os **Runnables** seriam as panelas e utensÃ­lios que fazem tudo funcionar, e o **LCEL** seria a receita que conecta tudo de forma elegante!\n\n**Neste mÃ³dulo vocÃª vai aprender:**\n- ğŸ¤– **ChatModels**: Como conversar com diferentes IAs (Gemini, OpenAI, Anthropic...)\n- âš™ï¸ **Runnables**: A base de tudo no LangChain v0.2\n- ğŸ”— **LCEL**: A linguagem que conecta tudo de forma linda\n- ğŸ¯ **Casos prÃ¡ticos**: Exemplos que vocÃª vai usar no dia a dia\n\n**Bora comeÃ§ar essa jornada incrÃ­vel!** ğŸ‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ Setup Inicial: Preparando Nossa Cozinha\n\nAntes de comeÃ§ar a cozinhar, precisamos organizar nossa cozinha! Vamos instalar e importar tudo que precisamos.\n\n**Dica do Pedro:** Sempre mantenha suas bibliotecas atualizadas. O mundo da IA muda mais rÃ¡pido que moda de TikTok! ğŸ˜„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instalando as bibliotecas necessÃ¡rias\n!pip install langchain langchain-google-genai langchain-openai langchain-anthropic python-dotenv matplotlib -q\n\nprint(\"âœ… Bibliotecas instaladas com sucesso!\")\nprint(\"ğŸš€ Agora vamos importar tudo que precisamos...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports essenciais\nimport os\nfrom dotenv import load_dotenv\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\n\n# LangChain Core\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain_core.runnables import RunnableLambda, RunnableSequence\nfrom langchain_core.output_parsers import StrOutputParser\n\n# ChatModels - Nossa coleÃ§Ã£o de chefs!\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n# from langchain_openai import ChatOpenAI  # Descomente se tiver API key\n# from langchain_anthropic import ChatAnthropic  # Descomente se tiver API key\n\nprint(\"ğŸ“š Imports realizados!\")\nprint(\"ğŸ¯ Vamos configurar as API keys agora...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ConfiguraÃ§Ã£o das API Keys\n# Dica do Pedro: NUNCA commite suas chaves no GitHub! Use variÃ¡veis de ambiente sempre!\n\n# Carrega variÃ¡veis de ambiente (se existir arquivo .env)\nload_dotenv()\n\n# Para o Google Gemini (nosso chef principal do curso)\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY') or input(\"Digite sua Google API Key (https://aistudio.google.com/): \")\nos.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n\n# VerificaÃ§Ã£o\nif GOOGLE_API_KEY:\n    print(\"âœ… Google API Key configurada!\")\n    print(\"ğŸš€ Pronto para usar o Gemini 2.0 Flash!\")\nelse:\n    print(\"âŒ Ops! Precisa da API Key do Google para continuar.\")\n    print(\"ğŸ’¡ Pegue a sua em: https://aistudio.google.com/\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– ChatModels: Conhecendo Nossos Chefs de IA\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_02.png)\n\nTÃ¡, mas o que Ã© um **ChatModel**? \n\nPensa assim: um ChatModel Ã© como um chef especializado. Cada um tem sua personalidade, seus pontos fortes e seu jeito Ãºnico de \"cozinhar\" respostas. No LangChain, todos eles seguem a mesma \"etiqueta de cozinha\" - ou seja, a mesma interface!\n\n### ğŸ” Principais ChatModels:\n\n1. **Gemini 2.0 Flash** (Google) - Nosso chef principal! RÃ¡pido, inteligente e gratuito\n2. **GPT-4** (OpenAI) - O chef famoso, muito bom mas pago\n3. **Claude** (Anthropic) - O chef cuidadoso e Ã©tico\n4. **Llama** (Meta) - O chef open source\n\n**Dica do Pedro:** Mesmo que vocÃª use sÃ³ o Gemini no curso, entender a flexibilidade do LangChain Ã© fundamental! AmanhÃ£ vocÃª pode precisar trocar de modelo com apenas 1 linha de cÃ³digo! ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criando nosso ChatModel principal: Gemini 2.0 Flash\n# Por que Gemini? Ã‰ gratuito, rÃ¡pido e muito bom!\n\ngemini_model = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash-exp\",  # A versÃ£o mais nova e rÃ¡pida\n    temperature=0.7,  # Criatividade moderada\n    max_tokens=1000,  # Limite de tokens na resposta\n    timeout=30,  # Timeout de 30 segundos\n)\n\nprint(\"ğŸ¤– ChatModel Gemini 2.0 Flash criado!\")\nprint(f\"ğŸ“ Modelo: {gemini_model.model_name}\")\nprint(f\"ğŸŒ¡ï¸ Temperature: {gemini_model.temperature}\")\nprint(\"\\nğŸ’¡ Dica: Temperature baixa = mais focado, alta = mais criativo\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Primeiro teste: conversando com o Gemini\n# Vamos fazer uma pergunta simples para testar\n\nmessage = HumanMessage(content=\"Explique em 2 frases o que Ã© inteligÃªncia artificial\")\n\n# Invocando o modelo - aqui Ã© onde a magia acontece!\nresponse = gemini_model.invoke([message])\n\nprint(\"ğŸ—£ï¸ Pergunta:\", message.content)\nprint(\"\\nğŸ¤– Resposta do Gemini:\")\nprint(response.content)\nprint(f\"\\nğŸ“Š Tipo da resposta: {type(response)}\")\nprint(f\"ğŸ“ˆ Tamanho da resposta: {len(response.content)} caracteres\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Visualizando ComparaÃ§Ã£o de ChatModels\n\nVamos criar um grÃ¡fico para comparar as caracterÃ­sticas dos principais ChatModels!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ComparaÃ§Ã£o visual dos ChatModels\nmodels = ['Gemini 2.0\\nFlash', 'GPT-4o', 'Claude 3.5\\nSonnet', 'Llama 3.1']\nspeed = [9, 7, 6, 8]  # Velocidade (1-10)\ncost = [10, 3, 4, 10]  # Custo-benefÃ­cio (gratuito = 10)\nquality = [9, 10, 9, 7]  # Qualidade das respostas\n\nx = np.arange(len(models))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Criando as barras\nrects1 = ax.bar(x - width, speed, width, label='Velocidade', color='#4285F4', alpha=0.8)\nrects2 = ax.bar(x, cost, width, label='Custo-benefÃ­cio', color='#34A853', alpha=0.8)\nrects3 = ax.bar(x + width, quality, width, label='Qualidade', color='#EA4335', alpha=0.8)\n\n# ConfiguraÃ§Ã£o do grÃ¡fico\nax.set_ylabel('Score (1-10)', fontsize=12)\nax.set_title('ğŸ¤– ComparaÃ§Ã£o de ChatModels - Por que escolhemos Gemini?', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Adicionando valores nas barras\ndef add_value_labels(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate(f'{height}',\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', fontweight='bold')\n\nadd_value_labels(rects1)\nadd_value_labels(rects2)\nadd_value_labels(rects3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"ğŸ“ˆ Como vocÃª pode ver, o Gemini 2.0 Flash tem o melhor equilÃ­brio!\")\nprint(\"ğŸ’° Principalmente por ser gratuito e muito rÃ¡pido!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Runnables: A Base de Tudo no LangChain v0.2\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_03.png)\n\nAgora vem a parte **FUNDAMENTAL**: os **Runnables**!\n\nTÃ¡, mas o que diabos Ã© um Runnable? ğŸ¤”\n\nPensa assim: um Runnable Ã© como uma **peÃ§a de LEGO padronizada**. NÃ£o importa se Ã© uma peÃ§a pequena ou grande, todas tÃªm aqueles \"pontinhos\" que se encaixam perfeitamente em outras peÃ§as!\n\nNo LangChain v0.2, **TUDO** Ã© um Runnable:\n- ChatModels sÃ£o Runnables\n- Prompt Templates sÃ£o Runnables  \n- Output Parsers sÃ£o Runnables\n- Chains sÃ£o Runnables\n\n### ğŸ¯ Interface PadrÃ£o dos Runnables:\n\n$$\\text{Runnable} = \\{\\text{invoke, stream, batch, ainvoke, astream, abatch}\\}$$\n\n**Dica do Pedro:** Essa padronizaÃ§Ã£o Ã© GENIAL! Uma vez que vocÃª aprende como um Runnable funciona, vocÃª sabe como TODOS funcionam! Ã‰ como aprender a dirigir - depois vocÃª dirige qualquer carro! ğŸš—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demonstrando que ChatModel Ã‰ um Runnable\nprint(\"ğŸ” Vamos investigar nosso ChatModel...\")\nprint(f\"ğŸ“‹ Tipo: {type(gemini_model)}\")\nprint(f\"ğŸ§¬ Ã‰ um Runnable? {hasattr(gemini_model, 'invoke')}\")\n\n# Verificando os mÃ©todos disponÃ­veis\nrunnable_methods = ['invoke', 'stream', 'batch', 'ainvoke', 'astream', 'abatch']\navailable_methods = [method for method in runnable_methods if hasattr(gemini_model, method)]\n\nprint(f\"\\nâš™ï¸ MÃ©todos Runnable disponÃ­veis: {available_methods}\")\nprint(f\"âœ… Total: {len(available_methods)}/{len(runnable_methods)}\")\n\nprint(\"\\nğŸ’¡ Isso significa que podemos usar o ChatModel de vÃ¡rias formas diferentes!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testando diferentes formas de usar um Runnable\nfrom langchain_core.messages import HumanMessage\n\n# 1. invoke() - Forma sÃ­ncrona simples\nprint(\"1ï¸âƒ£ INVOKE - Chamada simples:\")\nresult1 = gemini_model.invoke([HumanMessage(content=\"Diga olÃ¡ em portuguÃªs\")])\nprint(f\"   Resposta: {result1.content}\")\n\n# 2. batch() - MÃºltiplas perguntas de uma vez\nprint(\"\\n2ï¸âƒ£ BATCH - MÃºltiplas perguntas:\")\nbatch_messages = [\n    [HumanMessage(content=\"Qual Ã© a capital do Brasil?\")],\n    [HumanMessage(content=\"Qual Ã© a capital da FranÃ§a?\")],\n    [HumanMessage(content=\"Qual Ã© a capital do JapÃ£o?\")]\n]\n\nresults = gemini_model.batch(batch_messages)\nfor i, result in enumerate(results, 1):\n    print(f\"   Pergunta {i}: {result.content}\")\n\nprint(\"\\nğŸ’¡ O batch Ã© muito Ãºtil para processar vÃ¡rias perguntas em paralelo!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Diagrama: Anatomia de um Runnable\n\nVamos visualizar como um Runnable funciona internamente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\ngraph TD\n    A[Input] --> B[Runnable.invoke]\n    B --> C{Processamento Interno}\n    C --> D[Transform]\n    D --> E[Execute]\n    E --> F[Output]\n    \n    G[Batch Input] --> H[Runnable.batch]\n    H --> I[Parallel Processing]\n    I --> J[Multiple Outputs]\n    \n    K[Stream Input] --> L[Runnable.stream]\n    L --> M[Streaming Output]\n    \n    style B fill:#4285F4,stroke:#333,stroke-width:3px,color:#fff\n    style H fill:#34A853,stroke:#333,stroke-width:3px,color:#fff\n    style L fill:#EA4335,stroke:#333,stroke-width:3px,color:#fff\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”— LCEL: A Linguagem Que Une Tudo\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_04.png)\n\n**LCEL** = **LangChain Expression Language**\n\nTÃ¡, agora vem a parte mais **LINDA** do LangChain v0.2! ğŸ¤©\n\nO LCEL Ã© como se fosse a **gramÃ¡tica** que conecta todos os Runnables de forma super elegante. Ã‰ tipo aquele amigo que apresenta todo mundo na festa e faz todo mundo se dar bem!\n\n### ğŸ¯ A Magia do Pipe Operator:\n\n$$\\text{chain} = \\text{runnable}_1 \\; | \\; \\text{runnable}_2 \\; | \\; \\text{runnable}_3$$\n\n**Analogia do Pedro:** Ã‰ como uma linha de produÃ§Ã£o de brigadeiro! ğŸ«\n- Primeiro vocÃª faz o doce (ChatModel)\n- Depois vocÃª enrola (OutputParser) \n- Por fim vocÃª coloca na forminha (Formatter)\n\nCada etapa recebe o resultado da anterior e passa para a prÃ³xima!\n\n**Dica do Pedro:** O pipe `|` no LCEL funciona igual ao pipe do Linux/Unix. Se vocÃª jÃ¡ usou terminal, vai se sentir em casa! ğŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criando nossa primeira chain com LCEL\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\n\n# Vamos criar uma chain que:\n# 1. Recebe uma pergunta\n# 2. Adiciona contexto brasileiro \n# 3. Passa para o ChatModel\n# 4. Formata a saÃ­da\n\n# FunÃ§Ã£o para adicionar contexto brasileiro\ndef add_brazilian_context(topic: str) -> list:\n    context_message = f\"Responda sobre '{topic}' considerando o contexto brasileiro e de forma didÃ¡tica.\"\n    return [HumanMessage(content=context_message)]\n\n# FunÃ§Ã£o para formatar a saÃ­da\ndef format_response(ai_message) -> str:\n    return f\"ğŸ¤– **Resposta do Gemini:**\\n{ai_message.content}\\n\\nâœ… Processado em {datetime.now().strftime('%H:%M:%S')}\"\n\n# Criando os Runnables\ncontext_adder = RunnableLambda(add_brazilian_context)\noutput_parser = StrOutputParser()  # Extrai sÃ³ o conteÃºdo da mensagem\nformatter = RunnableLambda(format_response)\n\nprint(\"ğŸ”§ Runnables individuais criados:\")\nprint(f\"   ğŸ“ Context Adder: {type(context_adder)}\")\nprint(f\"   ğŸ¤– Gemini Model: {type(gemini_model)}\")\nprint(f\"   ğŸ“¤ Formatter: {type(formatter)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AQUI VEM A MAGIA DO LCEL! âœ¨\n# Conectando tudo com o pipe operator\n\nbrazilian_ai_chain = (\n    context_adder          # Adiciona contexto brasileiro\n    | gemini_model        # Processa com Gemini\n    | formatter           # Formata a resposta\n)\n\nprint(\"ğŸ¯ Chain criada com LCEL!\")\nprint(f\"ğŸ“‹ Tipo da chain: {type(brazilian_ai_chain)}\")\nprint(f\"ğŸ§¬ A chain tambÃ©m Ã© um Runnable? {hasattr(brazilian_ai_chain, 'invoke')}\")\n\n# Testando nossa chain\nprint(\"\\nğŸš€ Testando a chain...\")\ntopic = \"inteligÃªncia artificial no agronegÃ³cio\"\nresult = brazilian_ai_chain.invoke(topic)\n\nprint(f\"\\nğŸ“¥ Input: {topic}\")\nprint(f\"ğŸ“¤ Output:\\n{result}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Visualizando Performance: Invoke vs Batch vs Stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n\n# Vamos comparar a performance dos diferentes mÃ©todos Runnable\ntopics = [\"energia solar no Brasil\", \"carnaval de rua\", \"programaÃ§Ã£o em Python\"]\n\n# Teste 1: Invoke sequencial\nstart_time = time.time()\nsequential_results = []\nfor topic in topics:\n    result = brazilian_ai_chain.invoke(topic)\n    sequential_results.append(result)\nsequential_time = time.time() - start_time\n\n# Teste 2: Batch paralelo\nstart_time = time.time()\nbatch_results = brazilian_ai_chain.batch(topics)\nbatch_time = time.time() - start_time\n\n# Criando grÃ¡fico de comparaÃ§Ã£o\nmethods = ['Sequential\\n(invoke)', 'Parallel\\n(batch)']\ntimes = [sequential_time, batch_time]\ncolors = ['#EA4335', '#34A853']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# GrÃ¡fico de tempo\nbars = ax1.bar(methods, times, color=colors, alpha=0.8)\nax1.set_ylabel('Tempo (segundos)', fontsize=12)\nax1.set_title('âš¡ Performance: Sequential vs Batch', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Adicionando valores nas barras\nfor bar, time_val in zip(bars, times):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n\n# GrÃ¡fico de eficiÃªncia\nefficiency = [100, (sequential_time / batch_time) * 100]\nax2.bar(methods, efficiency, color=colors, alpha=0.8)\nax2.set_ylabel('EficiÃªncia (%)', fontsize=12)\nax2.set_title('ğŸ“ˆ EficiÃªncia Relativa', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Adicionando valores\nfor i, eff in enumerate(efficiency):\n    ax2.text(i, eff + 5, f'{eff:.1f}%', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"ğŸ Resultados:\")\nprint(f\"   â±ï¸ Sequential: {sequential_time:.2f}s\")\nprint(f\"   âš¡ Batch: {batch_time:.2f}s\")\nprint(f\"   ğŸ“Š Speedup: {sequential_time/batch_time:.1f}x mais rÃ¡pido!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Casos PrÃ¡ticos: LCEL na Vida Real\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_05.png)\n\nAgora vamos ver como usar tudo isso na **vida real**! Vou mostrar 3 casos super prÃ¡ticos que vocÃª vai usar no dia a dia.\n\n**Dica do Pedro:** Esses exemplos sÃ£o a base para os prÃ³ximos mÃ³dulos. No MÃ³dulo 3 vamos ver Prompt Templates, no 4 Chains mais complexas, e assim por diante! ğŸ¯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso PrÃ¡tico 1: Chatbot Especializado em Tecnologia Brasileira\nfrom langchain_core.runnables import RunnableLambda\n\ndef create_tech_context(question: str) -> list:\n    \"\"\"Cria contexto especializado em tecnologia brasileira\"\"\"\n    system_msg = SystemMessage(content=\"\"\"\n    VocÃª Ã© um especialista em tecnologia e inovaÃ§Ã£o no Brasil.\n    Suas respostas devem:\n    - Mencionar exemplos brasileiros quando possÃ­vel\n    - Ser tÃ©cnicas mas acessÃ­veis\n    - Incluir perspectivas do mercado nacional\n    \"\"\")\n    \n    human_msg = HumanMessage(content=question)\n    return [system_msg, human_msg]\n\ndef format_tech_response(ai_message) -> str:\n    \"\"\"Formata resposta tÃ©cnica\"\"\"\n    return f\"\"\"\nğŸ‡§ğŸ‡· **TechBot Brasil** ğŸ¤–\n\n{ai_message.content}\n\n---\nğŸ’¡ *Resposta gerada em {datetime.now().strftime('%d/%m/%Y Ã s %H:%M')}*\n    \"\"\".strip()\n\n# Criando a chain especializada\ntech_bot_chain = (\n    RunnableLambda(create_tech_context)  # Contexto tech brasileiro\n    | gemini_model                       # Processamento IA\n    | RunnableLambda(format_tech_response)  # FormataÃ§Ã£o\n)\n\n# Teste do chatbot\nquestion = \"Quais sÃ£o as principais startups de IA no Brasil?\"\nresponse = tech_bot_chain.invoke(question)\n\nprint(\"ğŸ’¬ Pergunta:\", question)\nprint(\"\\n\" + \"=\"*60)\nprint(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso PrÃ¡tico 2: Sistema de AnÃ¡lise de Sentimento\ndef analyze_sentiment(text: str) -> list:\n    \"\"\"Prepara texto para anÃ¡lise de sentimento\"\"\"\n    prompt = f\"\"\"\n    Analise o sentimento do seguinte texto brasileiro:\n    \n    \"{text}\"\n    \n    Responda apenas com:\n    - Sentimento: [POSITIVO/NEGATIVO/NEUTRO]\n    - ConfianÃ§a: [0-100]%\n    - Palavras-chave: [liste as principais]\n    \"\"\"\n    return [HumanMessage(content=prompt)]\n\ndef parse_sentiment(ai_message) -> dict:\n    \"\"\"Converte resposta em dicionÃ¡rio estruturado\"\"\"\n    content = ai_message.content\n    \n    # Parse simples (em produÃ§Ã£o usarÃ­amos regex ou parser mais robusto)\n    lines = content.split('\\n')\n    result = {\n        'text_analyzed': 'Texto analisado',\n        'sentiment': 'N/A',\n        'confidence': 'N/A', \n        'keywords': 'N/A',\n        'raw_response': content\n    }\n    \n    for line in lines:\n        if 'Sentimento:' in line:\n            result['sentiment'] = line.split(':')[1].strip()\n        elif 'ConfianÃ§a:' in line:\n            result['confidence'] = line.split(':')[1].strip()\n        elif 'Palavras-chave:' in line:\n            result['keywords'] = line.split(':')[1].strip()\n    \n    return result\n\n# Chain de anÃ¡lise de sentimento\nsentiment_chain = (\n    RunnableLambda(analyze_sentiment)    # Prepara anÃ¡lise\n    | gemini_model                       # Processa\n    | RunnableLambda(parse_sentiment)    # Estrutura resultado\n)\n\n# Teste com diferentes textos\ntexts = [\n    \"Adorei o novo update do app! Ficou muito mais rÃ¡pido e fÃ¡cil de usar!\",\n    \"O atendimento foi pÃ©ssimo, demorou 2 horas para resolver um problema simples.\",\n    \"O produto chegou conforme esperado, dentro do prazo.\"\n]\n\nprint(\"ğŸ” AnÃ¡lise de Sentimento em Lote:\")\nprint(\"=\"*50)\n\nresults = sentiment_chain.batch(texts)\nfor i, (text, result) in enumerate(zip(texts, results), 1):\n    print(f\"\\n{i}. Texto: {text[:50]}...\")\n    print(f\"   Sentimento: {result['sentiment']}\")\n    print(f\"   ConfianÃ§a: {result['confidence']}\")\n    print(f\"   Palavras-chave: {result['keywords']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”„ Diagrama: Fluxo Completo de uma Chain LCEL\n\nVamos visualizar como nossa chain de sentimento funciona:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\nflowchart TD\n    A[\"Input: Texto para anÃ¡lise\"] --> B[\"RunnableLambda: analyze_sentiment\"]\n    B --> C[\"PreparaÃ§Ã£o do Prompt\"]\n    C --> D[\"ChatModel: Gemini 2.0\"]\n    D --> E[\"IA processa e analisa\"]\n    E --> F[\"RunnableLambda: parse_sentiment\"]\n    F --> G[\"EstruturaÃ§Ã£o dos dados\"]\n    G --> H[\"Output: Dict estruturado\"]\n    \n    I[\"Batch Input: Lista de textos\"] --> J[\"Processamento Paralelo\"]\n    J --> K[\"MÃºltiplos Outputs\"]\n    \n    style D fill:#4285F4,stroke:#333,stroke-width:3px,color:#fff\n    style B fill:#34A853,stroke:#333,stroke-width:2px,color:#fff\n    style F fill:#34A853,stroke:#333,stroke-width:2px,color:#fff\n    style H fill:#EA4335,stroke:#333,stroke-width:2px,color:#fff\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Caso PrÃ¡tico 3: Tradutor Contextual com ExplicaÃ§Ãµes\ndef create_translation_prompt(data: dict) -> list:\n    \"\"\"Cria prompt para traduÃ§Ã£o contextual\"\"\"\n    text = data['text']\n    source_lang = data['from']\n    target_lang = data['to']\n    \n    prompt = f\"\"\"\n    Traduza o seguinte texto do {source_lang} para {target_lang}.\n    \n    Texto original: \"{text}\"\n    \n    ForneÃ§a:\n    1. TraduÃ§Ã£o direta\n    2. TraduÃ§Ã£o contextual (se diferente)\n    3. ExplicaÃ§Ã£o de nuances culturais (se houver)\n    \n    Seja preciso e didÃ¡tico.\n    \"\"\"\n    \n    return [HumanMessage(content=prompt)]\n\ndef format_translation(ai_message) -> str:\n    \"\"\"Formata a resposta de traduÃ§Ã£o\"\"\"\n    return f\"\"\"\nğŸŒ **Tradutor Contextual IA**\n\n{ai_message.content}\n\n---\nğŸ¤– *Powered by Gemini 2.0 Flash - {datetime.now().strftime('%H:%M:%S')}*\n    \"\"\".strip()\n\n# Chain de traduÃ§Ã£o\ntranslation_chain = (\n    RunnableLambda(create_translation_prompt)  # Prepara traduÃ§Ã£o\n    | gemini_model                             # Traduz\n    | RunnableLambda(format_translation)       # Formata\n)\n\n# Teste de traduÃ§Ã£o\ntranslation_request = {\n    'text': 'Saudade Ã© uma palavra Ãºnica do portuguÃªs que expressa a nostalgia por algo ou alguÃ©m.',\n    'from': 'portuguÃªs',\n    'to': 'inglÃªs'\n}\n\nresult = translation_chain.invoke(translation_request)\nprint(result)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Monitoramento e Debug de Chains\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_06.png)\n\nUma coisa **FUNDAMENTAL** quando vocÃª trabalha com chains em produÃ§Ã£o: saber o que estÃ¡ acontecendo! Vamos aprender a monitorar e debugar nossas chains.\n\n**Dica do Pedro:** Em produÃ§Ã£o, monitoramento nÃ£o Ã© luxo, Ã© **NECESSIDADE**! VocÃª precisa saber se sua IA estÃ¡ funcionando bem ou se estÃ¡ \"maluquinha\"! ğŸ˜…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sistema de monitoramento simples para chains\nimport time\nfrom typing import Any\n\nclass ChainMonitor:\n    def __init__(self):\n        self.metrics = {\n            'total_calls': 0,\n            'total_time': 0,\n            'errors': 0,\n            'avg_response_time': 0,\n            'calls_history': []\n        }\n    \n    def monitor_call(self, func):\n        \"\"\"Decorator para monitorar chamadas de chain\"\"\"\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            self.metrics['total_calls'] += 1\n            \n            try:\n                result = func(*args, **kwargs)\n                success = True\n                error = None\n            except Exception as e:\n                self.metrics['errors'] += 1\n                success = False\n                error = str(e)\n                result = None\n            \n            end_time = time.time()\n            call_time = end_time - start_time\n            self.metrics['total_time'] += call_time\n            self.metrics['avg_response_time'] = self.metrics['total_time'] / self.metrics['total_calls']\n            \n            # HistÃ³rico da chamada\n            self.metrics['calls_history'].append({\n                'timestamp': datetime.now(),\n                'duration': call_time,\n                'success': success,\n                'error': error,\n                'input_size': len(str(args[0])) if args else 0\n            })\n            \n            return result\n        return wrapper\n    \n    def get_stats(self):\n        \"\"\"Retorna estatÃ­sticas atuais\"\"\"\n        return self.metrics.copy()\n    \n    def plot_performance(self):\n        \"\"\"Plota grÃ¡fico de performance\"\"\"\n        if not self.metrics['calls_history']:\n            print(\"Nenhuma chamada registrada ainda!\")\n            return\n        \n        times = [call['duration'] for call in self.metrics['calls_history']]\n        calls = list(range(1, len(times) + 1))\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # GrÃ¡fico de tempo por chamada\n        ax1.plot(calls, times, 'o-', color='#4285F4', linewidth=2, markersize=8)\n        ax1.axhline(y=self.metrics['avg_response_time'], color='#EA4335', \n                   linestyle='--', label=f'MÃ©dia: {self.metrics[\"avg_response_time\"]:.2f}s')\n        ax1.set_xlabel('NÃºmero da Chamada')\n        ax1.set_ylabel('Tempo de Resposta (s)')\n        ax1.set_title('â±ï¸ Performance por Chamada')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # GrÃ¡fico de estatÃ­sticas gerais\n        stats_labels = ['Total\\nChamadas', 'Sucessos', 'Erros', 'Tempo\\nMÃ©dio (s)']\n        stats_values = [\n            self.metrics['total_calls'],\n            self.metrics['total_calls'] - self.metrics['errors'],\n            self.metrics['errors'],\n            self.metrics['avg_response_time']\n        ]\n        \n        colors = ['#4285F4', '#34A853', '#EA4335', '#FBBC04']\n        bars = ax2.bar(stats_labels, stats_values, color=colors, alpha=0.8)\n        ax2.set_title('ğŸ“Š EstatÃ­sticas Gerais')\n        ax2.set_ylabel('Valores')\n        \n        # Adicionando valores nas barras\n        for bar, value in zip(bars, stats_values):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(stats_values)*0.01,\n                    f'{value:.2f}' if isinstance(value, float) else str(value),\n                    ha='center', va='bottom', fontweight='bold')\n        \n        plt.tight_layout()\n        plt.show()\n\n# Criando monitor\nmonitor = ChainMonitor()\n\nprint(\"ğŸ“Š Monitor de Chain criado!\")\nprint(\"ğŸ” Agora vamos testar com nossa chain de sentimento...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testando monitoramento com diferentes inputs\ntest_texts = [\n    \"Este produto Ã© incrÃ­vel!\",\n    \"Que serviÃ§o horrÃ­vel, nunca mais volto!\",\n    \"O atendimento foi ok, nada demais.\",\n    \"Excelente qualidade, recomendo muito!\",\n    \"PreÃ§o muito alto para o que oferece.\"\n]\n\n# Wrapper para monitorar nossa chain\n@monitor.monitor_call\ndef monitored_sentiment_analysis(text):\n    return sentiment_chain.invoke(text)\n\nprint(\"ğŸš€ Executando anÃ¡lises monitoradas...\")\nprint(\"=\"*50)\n\nfor i, text in enumerate(test_texts, 1):\n    print(f\"\\nğŸ“ Teste {i}: {text}\")\n    result = monitored_sentiment_analysis(text)\n    print(f\"   âœ… Sentimento: {result['sentiment']} | ConfianÃ§a: {result['confidence']}\")\n\n# Mostrando estatÃ­sticas\nstats = monitor.get_stats()\nprint(f\"\\nğŸ“Š **EstatÃ­sticas Finais:**\")\nprint(f\"   ğŸ“ Total de chamadas: {stats['total_calls']}\")\nprint(f\"   â±ï¸ Tempo mÃ©dio: {stats['avg_response_time']:.2f}s\")\nprint(f\"   âœ… Taxa de sucesso: {((stats['total_calls'] - stats['errors']) / stats['total_calls'] * 100):.1f}%\")\nprint(f\"   ğŸ•’ Tempo total: {stats['total_time']:.2f}s\")\n\n# Plotando performance\nmonitor.plot_performance()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ® ExercÃ­cios PrÃ¡ticos\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_07.png)\n\nAgora Ã© sua vez de botar a mÃ£o na massa! Vou dar dois desafios para vocÃª praticar tudo que aprendemos.\n\n**Dica do Pedro:** NÃ£o tenha medo de errar! ProgramaÃ§Ã£o se aprende errando e consertando. Ã‰ como aprender a andar de bicicleta - vocÃª vai cair algumas vezes, mas depois nunca mais esquece! ğŸš´â€â™‚ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¯ ExercÃ­cio 1: Criador de Piadas Brasileiras\n\n**Desafio:** Crie uma chain que:\n1. Receba um tema\n2. Gere uma piada brasileira sobre o tema\n3. Avalie se a piada Ã© adequada para crianÃ§as\n4. Formate a saÃ­da de forma divertida\n\n**Requisitos:**\n- Use LCEL para conectar os Runnables\n- Inclua contexto brasileiro nas piadas\n- A saÃ­da deve ser estruturada e bonita\n\n```python\n# Seu cÃ³digo aqui!\n# Dica: Use as funÃ§Ãµes que criamos como inspiraÃ§Ã£o\n\n# def create_joke_prompt(theme: str) -> list:\n#     # Sua implementaÃ§Ã£o\n\n# def evaluate_joke(ai_message) -> dict:\n#     # Sua implementaÃ§Ã£o\n\n# joke_chain = (\n#     RunnableLambda(create_joke_prompt)\n#     | gemini_model\n#     | RunnableLambda(evaluate_joke)\n# )\n\n# Teste com: joke_chain.invoke(\"programaÃ§Ã£o\")\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EspaÃ§o para sua soluÃ§Ã£o do ExercÃ­cio 1\n# Descomente e complete o cÃ³digo abaixo:\n\n# def create_joke_prompt(theme: str) -> list:\n#     \"\"\"Cria prompt para gerar piada brasileira\"\"\"\n#     # SEU CÃ“DIGO AQUI\n#     pass\n\n# def evaluate_joke(ai_message) -> dict:\n#     \"\"\"Avalia e formata a piada\"\"\"\n#     # SEU CÃ“DIGO AQUI\n#     pass\n\n# # Criando a chain\n# joke_chain = (\n#     RunnableLambda(create_joke_prompt)\n#     | gemini_model\n#     | RunnableLambda(evaluate_joke)\n# )\n\n# # Teste\n# result = joke_chain.invoke(\"programaÃ§Ã£o\")\n# print(result)\n\nprint(\"ğŸ’­ Complete o exercÃ­cio acima descomentando e implementando as funÃ§Ãµes!\")\nprint(\"ğŸ¯ Objetivo: Criar um gerador de piadas brasileiras inteligente\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ¯ ExercÃ­cio 2: Sistema de RecomendaÃ§Ã£o de Filmes\n\n**Desafio:** Crie uma chain que:\n1. Receba preferÃªncias do usuÃ¡rio (gÃªnero, ano, paÃ­s)\n2. Gere recomendaÃ§Ãµes personalizadas\n3. Inclua justificativas para cada recomendaÃ§Ã£o\n4. Permita processar mÃºltiplos usuÃ¡rios em batch\n\n**Requisitos:**\n- Use batch processing para mÃºltiplos usuÃ¡rios\n- Inclua filmes brasileiros nas recomendaÃ§Ãµes\n- Estruture a saÃ­da como JSON\n- Adicione sistema de rating\n\n```python\n# Estrutura sugerida:\n# user_preferences = {\n#     'genres': ['aÃ§Ã£o', 'comÃ©dia'],\n#     'year_range': '2010-2023',\n#     'include_brazilian': True,\n#     'max_recommendations': 5\n# }\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EspaÃ§o para sua soluÃ§Ã£o do ExercÃ­cio 2\n# Implemente um sistema de recomendaÃ§Ã£o de filmes\n\n# Exemplo de entrada:\nuser_preferences_example = {\n    'genres': ['aÃ§Ã£o', 'comÃ©dia'],\n    'year_range': '2010-2023', \n    'include_brazilian': True,\n    'max_recommendations': 3\n}\n\nprint(\"ğŸ¬ ExercÃ­cio 2: Sistema de RecomendaÃ§Ã£o de Filmes\")\nprint(f\"ğŸ“ Exemplo de entrada: {user_preferences_example}\")\nprint(\"\\nğŸ’¡ Dicas:\")\nprint(\"   - Use o padrÃ£o LCEL que aprendemos\")\nprint(\"   - Teste com batch para mÃºltiplos usuÃ¡rios\")\nprint(\"   - Inclua filmes nacionais nas recomendaÃ§Ãµes\")\nprint(\"   - Estruture bem a saÃ­da\")\n\n# SEU CÃ“DIGO AQUI!\n# def create_movie_prompt(preferences: dict) -> list:\n#     pass\n\n# def parse_recommendations(ai_message) -> dict:\n#     pass\n\n# movie_chain = ...\n\nprint(\"\\nğŸš€ Implemente sua soluÃ§Ã£o acima!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Resumo do MÃ³dulo: O que Aprendemos?\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_08.png)\n\n**Liiindo!** ğŸ‰ Chegamos ao final do MÃ³dulo 2! Vamos fazer uma recap do que conquistamos:\n\n### ğŸ§  **Conceitos Fundamentais:**\n\n1. **ğŸ¤– ChatModels** - Os chefs de IA\n   - Gemini 2.0 Flash como nosso modelo principal\n   - Interface padronizada para todos os modelos\n   - Flexibilidade para trocar modelos facilmente\n\n2. **âš™ï¸ Runnables** - A base de tudo\n   - Interface unificada: invoke, batch, stream\n   - Composabilidade total\n   - Processamento paralelo com batch\n\n3. **ğŸ”— LCEL** - A linguagem elegante\n   - Pipe operator `|` para conectar tudo\n   - Chains como Runnables\n   - CÃ³digo limpo e legÃ­vel\n\n### ğŸ“Š **FÃ³rmula do Sucesso:**\n\n$$\\text{ChatModel} + \\text{LCEL} + \\text{Runnables} = \\text{AplicaÃ§Ãµes IncrÃ­veis}$$\n\n### ğŸš€ **PreparaÃ§Ã£o para os PrÃ³ximos MÃ³dulos:**\n- **MÃ³dulo 3:** Prompt Templates (para deixar nossos prompts mais profissionais)\n- **MÃ³dulo 4:** Chains (chains mais complexas e poderosas)\n- **MÃ³dulo 5:** Memory Systems (para dar memÃ³ria Ã s conversas)\n\n**Dica do Pedro:** O que aprendemos aqui Ã© a **BASE** de tudo! Nos prÃ³ximos mÃ³dulos vamos construir coisas cada vez mais incrÃ­veis em cima desses conceitos! ğŸ—ï¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ğŸŠ DemonstraÃ§Ã£o Final: Juntando Tudo que Aprendemos!\n# Vamos criar uma chain completa que demonstra todos os conceitos\n\ndef create_final_demo():\n    \"\"\"DemonstraÃ§Ã£o final integrando todos os conceitos\"\"\"\n    \n    # 1. ChatModel configurado\n    model = gemini_model\n    \n    # 2. Runnables customizados\n    def demo_input_processor(data: dict) -> list:\n        user_name = data.get('name', 'UsuÃ¡rio')\n        topic = data.get('topic', 'tecnologia')\n        \n        system_msg = SystemMessage(content=f\"\"\"\n        VocÃª Ã© um assistente educativo brasileiro chamado GeminiBot.\n        Sempre seja didÃ¡tico, use exemplos brasileiros e mantenha um tom amigÃ¡vel.\n        O usuÃ¡rio se chama {user_name}.\n        \"\"\")\n        \n        human_msg = HumanMessage(content=f\"Explique sobre {topic} de forma simples e interessante\")\n        return [system_msg, human_msg]\n    \n    def demo_output_formatter(ai_message) -> dict:\n        return {\n            'response': ai_message.content,\n            'timestamp': datetime.now().isoformat(),\n            'model_used': 'gemini-2.0-flash-exp',\n            'tokens_approx': len(ai_message.content.split()),\n            'processed_by': 'LangChain v0.2 + LCEL'\n        }\n    \n    # 3. LCEL Chain\n    final_chain = (\n        RunnableLambda(demo_input_processor)  # Processa entrada\n        | model                               # ChatModel processa\n        | RunnableLambda(demo_output_formatter)  # Formata saÃ­da\n    )\n    \n    return final_chain\n\n# Criando e testando nossa demonstraÃ§Ã£o final\nfinal_demo_chain = create_final_demo()\n\n# Teste individual\ntest_data = {\n    'name': 'Pedro',\n    'topic': 'inteligÃªncia artificial no Brasil'\n}\n\nresult = final_demo_chain.invoke(test_data)\n\nprint(\"ğŸŠ **DEMONSTRAÃ‡ÃƒO FINAL - MÃ“DULO 2** ğŸŠ\")\nprint(\"=\"*60)\nprint(f\"ğŸ‘¤ UsuÃ¡rio: {test_data['name']}\")\nprint(f\"ğŸ“ TÃ³pico: {test_data['topic']}\")\nprint(f\"\\nğŸ¤– **Resposta do GeminiBot:**\")\nprint(result['response'])\nprint(f\"\\nğŸ“Š **Metadados:**\")\nprint(f\"   â° Timestamp: {result['timestamp']}\")\nprint(f\"   ğŸ¤– Modelo: {result['model_used']}\")\nprint(f\"   ğŸ“ Tokens aprox: {result['tokens_approx']}\")\nprint(f\"   âš™ï¸ Processado por: {result['processed_by']}\")\n\nprint(f\"\\nâœ… **ParabÃ©ns! VocÃª dominou os fundamentos do LangChain v0.2!** ğŸ‰\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”œ PrÃ³ximos Passos: Sua Jornada Continua!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versÃ£o-v0.2-modulo-02_img_09.png)\n\n**Bora que a jornada nÃ£o para por aqui!** ğŸš€\n\n### ğŸ“š **No MÃ³dulo 3 - Prompt Templates e Output Parsers:**\n- Como criar prompts profissionais e reutilizÃ¡veis\n- Parsers para estruturar respostas da IA\n- Templates dinÃ¢micos e condicionais\n- ValidaÃ§Ã£o e tratamento de erros\n\n### ğŸ’¡ **Dica do Pedro para Continuar Estudando:**\n1. **Pratique os exercÃ­cios** - A prÃ¡tica leva Ã  perfeiÃ§Ã£o!\n2. **Experimente variaÃ§Ãµes** - Mude parÃ¢metros, teste diferentes inputs\n3. **Monitore performance** - Use as ferramentas que criamos\n4. **Documente seu aprendizado** - FaÃ§a anotaÃ§Ãµes e exemplos prÃ³prios\n\n### ğŸ¯ **Conecte-se:**\n- Continue praticando com o Gemini 2.0 Flash\n- Experimente outros ChatModels se tiver acesso\n- Compartilhe suas criaÃ§Ãµes com a comunidade\n- Prepare-se para os desafios dos prÃ³ximos mÃ³dulos!\n\n**AtÃ© o prÃ³ximo mÃ³dulo, e lembrem-se: \"A melhor forma de aprender IA Ã© fazendo IA!\"** ğŸ¤–âœ¨\n\n---\n\n*Pedro Nunes Guth - Instrutor de IA e AWS*  \n*\"Transformando conceitos complexos em conhecimento prÃ¡tico, um mÃ³dulo de cada vez!\"* ğŸ§ ğŸ’¡"
      ]
    }
  ]
}