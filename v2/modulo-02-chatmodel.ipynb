{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatmodels-langchain-guth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¤– ChatModels no LangChain: Seu Primeiro Amigo Virtual de IA!\n\n*Pedro Nunes Guth - MÃ³dulo 2 do Curso LangChain v0.3*\n\n---\n\nBora comeÃ§ar nossa jornada de verdade no LangChain! Se no mÃ³dulo passado entendemos **o que** Ã© o LangChain, agora vamos colocar a mÃ£o na massa com o **coraÃ§Ã£o** de qualquer aplicaÃ§Ã£o de IA: os **ChatModels**!\n\nPensa assim: se o LangChain fosse um carro, os ChatModels seriam o motor. Sem eles, nÃ£o saÃ­mos do lugar! ğŸš—ğŸ’¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ O que vocÃª vai aprender neste mÃ³dulo:\n\n- O que diabos Ã© um ChatModel e por que ele Ã© TÃƒO importante\n- Como usar o Gemini 2.0 Flash (nosso queridinho do curso)\n- Outras opÃ§Ãµes de ChatModels (OpenAI, Claude, Ollama)\n- Como trocar de modelo sem quebrar o cÃ³digo\n- ParÃ¢metros que fazem toda a diferenÃ§a\n- Streaming (receber respostas em tempo real)\n\n**Dica!** Guarda esse notebook como ouro! VocÃª vai usar esses conceitos em TODOS os prÃ³ximos mÃ³dulos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” TÃ¡, mas o que Ã© um ChatModel?\n\nImagina que vocÃª tem um amigo super inteligente que:\n- Entende qualquer pergunta que vocÃª faz\n- Responde de forma natural e conversacional\n- Lembra do contexto da conversa\n- Pode ser especialista em qualquer assunto\n\nEsse amigo seria um **ChatModel**! ğŸ§ âœ¨\n\nTecnicamente falando, um ChatModel Ã©:\n> Uma interface padronizada do LangChain que permite conversar com diferentes modelos de linguagem (LLMs) de forma consistente\n\nA mÃ¡gica Ã© que vocÃª aprende a usar UM modelo, e consegue usar TODOS os outros com a mesma sintaxe!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ—ï¸ Arquitetura dos ChatModels\n\n```mermaid\ngraph TD\n    A[Sua AplicaÃ§Ã£o] --> B[LangChain ChatModel]\n    B --> C[Gemini 2.0 Flash]\n    B --> D[OpenAI GPT-4]\n    B --> E[Claude]\n    B --> F[Ollama Local]\n    \n    C --> G[Resposta Padronizada]\n    D --> G\n    E --> G\n    F --> G\n    \n    G --> A\n```\n\nViu sÃ³? Uma interface, mÃºltiplas possibilidades! Ã‰ como ter um controle universal para todos os modelos de IA do mundo! ğŸ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora instalar tudo que vamos precisar!\n",
        "# Se vocÃª tÃ¡ no Colab, rode isso aqui primeiro\n",
        "\n",
        "!pip install langchain langchain-google-genai langchain-openai langchain-anthropic python-dotenv\n",
        "\n",
        "print(\"Liiindo! Tudo instalado! ğŸš€\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports bÃ¡sicos - nossos melhores amigos daqui pra frente!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ChatModels que vamos usar\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Para trabalhar com mensagens\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Carrega as variÃ¡veis de ambiente (suas chaves de API)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Imports carregados! Vamos comeÃ§ar a brincadeira! ğŸ‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”‘ Configurando suas APIs\n\nÃ“, antes de continuar, vocÃª precisa ter pelo menos a API do Google (Gemini). Ã‰ de graÃ§a e muito boa!\n\n**Como conseguir:**\n1. Vai no [Google AI Studio](https://aistudio.google.com/)\n2. Clica em \"Get API Key\"\n3. Copia a chave\n4. Cola no cÃ³digo abaixo\n\n**Dica!** Em produÃ§Ã£o, NUNCA deixe suas chaves no cÃ³digo! Use variÃ¡veis de ambiente sempre!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure suas chaves de API aqui\n",
        "# ATENÃ‡ÃƒO: Em produÃ§Ã£o, use variÃ¡veis de ambiente!\n",
        "\n",
        "# Se vocÃª tem as chaves, descomente e preencha:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_google_aqui\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\" \n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sua_chave_anthropic_aqui\"\n",
        "\n",
        "# Vamos verificar se pelo menos o Google tÃ¡ configurado\n",
        "if \"GOOGLE_API_KEY\" in os.environ:\n",
        "    print(\"âœ… Google API configurada! Partiu Gemini!\")\n",
        "else:\n",
        "    print(\"âŒ Opa! Precisa configurar a GOOGLE_API_KEY primeiro!\")\n",
        "    print(\"Vai no Google AI Studio e pega sua chave: https://aistudio.google.com/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Nosso Primeiro ChatModel: Gemini 2.0 Flash\n\nVamos comeÃ§ar com nosso protagonista do curso: o **Gemini 2.0 Flash**!\n\nPor que escolhi ele?\n- **RÃ¡pido** como um raio âš¡\n- **Gratuito** (atÃ© um limite generoso)\n- **Multimodal** (texto, imagem, Ã¡udio)\n- **Qualidade** excelente para a maioria dos casos\n\nÃ‰ como ter uma Ferrari que nÃ£o gasta gasolina! ğŸï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando nosso primeiro ChatModel!\n",
        "# Ã‰ mais simples do que vocÃª imagina\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",  # Nosso modelo escolhido\n",
        "    temperature=0.7,               # Criatividade (0 = robÃ³tico, 1 = criativo)\n",
        "    max_tokens=1000               # MÃ¡ximo de tokens na resposta\n",
        ")\n",
        "\n",
        "print(\"Gemini carregado e pronto pra conversar! ğŸ¤–âœ¨\")\n",
        "print(f\"Modelo: {gemini.model_name}\")\n",
        "print(f\"Temperature: {gemini.temperature}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeira conversa com o Gemini!\n",
        "# Vamos mandar uma mensagem simples\n",
        "\n",
        "mensagem = \"Oi! Me explica em uma frase o que Ã© inteligÃªncia artificial.\"\n",
        "\n",
        "# Invoca o modelo (essa Ã© a mÃ¡gica!)\n",
        "resposta = gemini.invoke(mensagem)\n",
        "\n",
        "print(\"ğŸ¤– Gemini respondeu:\")\n",
        "print(resposta.content)\n",
        "print(\"\\nğŸ’¡ Tipo da resposta:\", type(resposta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Tipos de Mensagens: A Linguagem dos ChatModels\n\nTÃ¡, mas o que acabou de acontecer ali em cima? \n\nO LangChain tem **3 tipos principais** de mensagens:\n\n1. **HumanMessage** - Suas mensagens (vocÃª, o usuÃ¡rio)\n2. **AIMessage** - Respostas da IA\n3. **SystemMessage** - InstruÃ§Ãµes para a IA (como ela deve se comportar)\n\nÃ‰ como se fosse um protocolo de conversa! ğŸ’¬\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-02_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos usar mensagens estruturadas agora\n",
        "# Muito mais poderoso que texto simples!\n",
        "\n",
        "mensagens = [\n",
        "    SystemMessage(content=\"VocÃª Ã© um especialista em explicar tecnologia de forma simples e divertida, usando analogias do cotidiano brasileiro.\"),\n",
        "    HumanMessage(content=\"Me explica o que Ã© machine learning usando uma analogia com futebol.\")\n",
        "]\n",
        "\n",
        "resposta = gemini.invoke(mensagens)\n",
        "\n",
        "print(\"ğŸ¤– Gemini com personalidade:\")\n",
        "print(resposta.content)\n",
        "\n",
        "# Vamos ver os metadados da resposta\n",
        "print(\"\\nğŸ“Š Metadados da resposta:\")\n",
        "print(f\"Tokens usados: {resposta.usage_metadata if hasattr(resposta, 'usage_metadata') else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ›ï¸ ParÃ¢metros que Fazem a DiferenÃ§a\n\nAgora vamos entender os **parÃ¢metros mais importantes** dos ChatModels:\n\n### Temperature ğŸŒ¡ï¸\n- **0.0**: RobÃ³tico, sempre a mesma resposta\n- **0.5**: Balanceado (recomendado para a maioria)\n- **1.0**: Criativo, respostas mais variadas\n\n### Max Tokens ğŸ“\n- Controla o tamanho mÃ¡ximo da resposta\n- 1 token â‰ˆ 0.75 palavras em inglÃªs\n- 1 token â‰ˆ 0.5 palavras em portuguÃªs\n\n**Dica!** Temperature Ã© como o tempero da comida: pouco fica sem graÃ§a, muito fica ruim! ğŸ§‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando diferentes temperatures\n",
        "# Vamos ver a diferenÃ§a na prÃ¡tica!\n",
        "\n",
        "pergunta = \"Me conte uma piada sobre programaÃ§Ã£o.\"\n",
        "\n",
        "temperatures = [0.0, 0.5, 1.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nğŸŒ¡ï¸ Temperature {temp}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Cria um modelo com temperature especÃ­fica\n",
        "    modelo_temp = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        temperature=temp\n",
        "    )\n",
        "    \n",
        "    resposta = modelo_temp.invoke(pergunta)\n",
        "    print(resposta.content[:200] + \"...\" if len(resposta.content) > 200 else resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Outras OpÃ§Ãµes de ChatModels\n\nBeleza, o Gemini Ã© nosso queridinho, mas vamos conhecer a famÃ­lia toda!\n\n### ğŸ¤– OpenAI (GPT-4)\n- **PrÃ³s**: Muito popular, documentaÃ§Ã£o excelente\n- **Contras**: Pago desde o primeiro uso\n\n### ğŸ§  Anthropic (Claude)\n- **PrÃ³s**: Muito bom para textos longos, seguro\n- **Contras**: Mais caro, menos APIs disponÃ­veis\n\n### ğŸ  Ollama (Local)\n- **PrÃ³s**: Roda na sua mÃ¡quina, privacidade total\n- **Contras**: Precisa de hardware potente\n\nÃ‰ como escolher entre Netflix, Amazon Prime e Globoplay - cada um tem seus pontos fortes! ğŸ“º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar diferentes ChatModels\n",
        "# (SÃ³ vai funcionar se vocÃª tiver as APIs configuradas)\n",
        "\n",
        "modelos = {}\n",
        "\n",
        "# Gemini (nosso favorito)\n",
        "modelos['gemini'] = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# OpenAI GPT-4 (se vocÃª tem a API)\n",
        "if \"OPENAI_API_KEY\" in os.environ:\n",
        "    modelos['gpt4'] = ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(\"âœ… GPT-4 configurado!\")\n",
        "else:\n",
        "    print(\"âŒ OpenAI API nÃ£o configurada\")\n",
        "\n",
        "# Claude (se vocÃª tem a API)\n",
        "if \"ANTHROPIC_API_KEY\" in os.environ:\n",
        "    modelos['claude'] = ChatAnthropic(\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(\"âœ… Claude configurado!\")\n",
        "else:\n",
        "    print(\"âŒ Anthropic API nÃ£o configurada\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Total de modelos disponÃ­veis: {len(modelos)}\")\n",
        "print(f\"ğŸ“‹ Modelos: {list(modelos.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando todos os modelos com a mesma pergunta\n",
        "# Vamos ver as diferenÃ§as!\n",
        "\n",
        "pergunta_teste = \"Explique em 2 frases o que Ã© LangChain.\"\n",
        "\n",
        "for nome, modelo in modelos.items():\n",
        "    print(f\"\\nğŸ¤– {nome.upper()}:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        resposta = modelo.invoke(pergunta_teste)\n",
        "        print(resposta.content)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro: {e}\")\n",
        "        print(\"(Provavelmente API nÃ£o configurada)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒŠ Streaming: Recebendo Respostas em Tempo Real\n\nSabe quando vocÃª tÃ¡ no ChatGPT e vÃª a resposta aparecendo palavra por palavra? Isso Ã© **streaming**! ğŸ¬\n\nÃ‰ muito Ãºtil para:\n- **UX melhor**: UsuÃ¡rio vÃª que algo tÃ¡ acontecendo\n- **Respostas longas**: NÃ£o fica esperando 30 segundos\n- **AplicaÃ§Ãµes reais**: Streamlit, chatbots, etc.\n\nA fÃ³rmula matemÃ¡tica do streaming Ã© simples:\n$$\\text{ExperiÃªncia do UsuÃ¡rio} = \\frac{\\text{InformaÃ§Ã£o Ãštil}}{\\text{Tempo de Espera}}$$\n\nCom streaming, diminuÃ­mos o tempo de espera percebido! ğŸ“ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming com Gemini\n",
        "# Vamos ver a mÃ¡gica acontecer!\n",
        "\n",
        "import time\n",
        "\n",
        "pergunta_longa = \"Me conte uma histÃ³ria sobre um robÃ´ que aprende a programar em Python. FaÃ§a uma histÃ³ria de pelo menos 3 parÃ¡grafos.\"\n",
        "\n",
        "print(\"ğŸŒŠ Streaming iniciado...\")\n",
        "print(\"ğŸ“ Resposta:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# O mÃ©todo stream() retorna pedaÃ§os da resposta\n",
        "resposta_completa = \"\"\n",
        "for chunk in gemini.stream(pergunta_longa):\n",
        "    # Cada chunk tem um pedaÃ§o do texto\n",
        "    if hasattr(chunk, 'content'):\n",
        "        print(chunk.content, end='', flush=True)\n",
        "        resposta_completa += chunk.content\n",
        "        time.sleep(0.05)  # SÃ³ pra ficar mais visual\n",
        "\n",
        "print(\"\\n\\nâœ… Streaming finalizado!\")\n",
        "print(f\"ğŸ“Š Total de caracteres: {len(resposta_completa)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas\n\nAgora que jÃ¡ sabemos o bÃ¡sico, vamos para as configuraÃ§Ãµes que fazem vocÃª parecer um **profissional**! ğŸ’¼\n\n### Top-p (Nucleus Sampling)\n- Controla a diversidade das respostas\n- Valores entre 0.1 e 1.0\n- 0.9 Ã© um bom padrÃ£o\n\n### Stop Sequences\n- Palavras que fazem o modelo parar de gerar\n- Ãštil para formatar saÃ­das\n\n### Max Tokens vs Max Output Tokens\n- Max tokens: limite total (entrada + saÃ­da)\n- Max output tokens: sÃ³ a resposta\n\n**Dica!** Essas configuraÃ§Ãµes sÃ£o como os temperos de uma receita - cada uma tem seu momento certo! ğŸ‘¨â€ğŸ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChatModel com configuraÃ§Ãµes avanÃ§adas\n",
        "# Vamos criar um modelo bem configuradinho!\n",
        "\n",
        "gemini_avancado = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.8,           # Um pouco mais criativo\n",
        "    max_tokens=500,           # Respostas mais concisas\n",
        "    top_p=0.9,               # Diversidade controlada\n",
        "    stop=[\"\\n\\n\\n\", \"FIM\"]    # Para quando encontrar isso\n",
        ")\n",
        "\n",
        "# Teste com um prompt especÃ­fico\n",
        "prompt_teste = \"\"\"\n",
        "Crie uma lista de 3 dicas para aprender programaÃ§Ã£o:\n",
        "1.\n",
        "\"\"\"\n",
        "\n",
        "resposta = gemini_avancado.invoke(prompt_teste)\n",
        "\n",
        "print(\"ğŸ¯ Resposta com configuraÃ§Ãµes avanÃ§adas:\")\n",
        "print(resposta.content)\n",
        "\n",
        "print(\"\\nâš™ï¸ ConfiguraÃ§Ãµes usadas:\")\n",
        "print(f\"Temperature: {gemini_avancado.temperature}\")\n",
        "print(f\"Max tokens: {gemini_avancado.max_tokens}\")\n",
        "print(f\"Top-p: {gemini_avancado.top_p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Visualizando Performance dos Modelos\n\nVamos criar um grÃ¡fico para comparar nossos modelos! Ã‰ sempre bom ter dados visuais para tomar decisÃµes. ğŸ“ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Vamos medir a velocidade de resposta dos modelos\n",
        "pergunta_padrao = \"O que Ã© inteligÃªncia artificial?\"\n",
        "velocidades = {}\n",
        "qualidade_subjetiva = {}  # Nota de 1 a 10 (subjetiva)\n",
        "\n",
        "# Teste sÃ³ com Gemini (jÃ¡ que Ã© o que temos certeza que funciona)\n",
        "print(\"â±ï¸ Testando velocidade do Gemini...\")\n",
        "\n",
        "inicio = time.time()\n",
        "resposta_gemini = gemini.invoke(pergunta_padrao)\n",
        "tempo_gemini = time.time() - inicio\n",
        "\n",
        "velocidades['Gemini 2.0'] = tempo_gemini\n",
        "qualidade_subjetiva['Gemini 2.0'] = 8.5  # Nota subjetiva\n",
        "\n",
        "print(f\"âœ… Gemini: {tempo_gemini:.2f} segundos\")\n",
        "print(f\"ğŸ“ Resposta: {resposta_gemini.content[:100]}...\")\n",
        "\n",
        "# Se tiver outros modelos, teste tambÃ©m\n",
        "if 'gpt4' in modelos:\n",
        "    print(\"\\nâ±ï¸ Testando GPT-4...\")\n",
        "    inicio = time.time()\n",
        "    try:\n",
        "        resposta_gpt4 = modelos['gpt4'].invoke(pergunta_padrao)\n",
        "        tempo_gpt4 = time.time() - inicio\n",
        "        velocidades['GPT-4'] = tempo_gpt4\n",
        "        qualidade_subjetiva['GPT-4'] = 9.0\n",
        "        print(f\"âœ… GPT-4: {tempo_gpt4:.2f} segundos\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro no GPT-4: {e}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Resultados coletados para {len(velocidades)} modelo(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando grÃ¡fico de comparaÃ§Ã£o\n",
        "# VisualizaÃ§Ã£o sempre ajuda na tomada de decisÃ£o!\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico 1: Velocidade\n",
        "modelos_nomes = list(velocidades.keys())\n",
        "tempos = list(velocidades.values())\n",
        "\n",
        "bars1 = ax1.bar(modelos_nomes, tempos, color=['#4285F4', '#FF6B6B', '#4ECDC4'][:len(modelos_nomes)])\n",
        "ax1.set_title('âš¡ Velocidade de Resposta (segundos)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Tempo (s)')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, tempo in zip(bars1, tempos):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "             f'{tempo:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# GrÃ¡fico 2: Qualidade (subjetiva)\n",
        "qualidades = [qualidade_subjetiva[modelo] for modelo in modelos_nomes]\n",
        "\n",
        "bars2 = ax2.bar(modelos_nomes, qualidades, color=['#4285F4', '#FF6B6B', '#4ECDC4'][:len(modelos_nomes)])\n",
        "ax2.set_title('â­ Qualidade Subjetiva (1-10)', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Nota')\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, qual in zip(bars2, qualidades):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{qual}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š AnÃ¡lise dos dados:\")\n",
        "print(f\"ğŸ† Modelo mais rÃ¡pido: {min(velocidades, key=velocidades.get)}\")\n",
        "print(f\"â­ Maior qualidade: {max(qualidade_subjetiva, key=qualidade_subjetiva.get)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Switching Between Models: A MÃ¡gica do LangChain\n\nAqui estÃ¡ a **beleza** do LangChain: vocÃª pode trocar de modelo sem mudar o resto do cÃ³digo! ğŸª„\n\nImagina que vocÃª desenvolveu uma aplicaÃ§Ã£o inteira com GPT-4, mas depois descobriu que o Gemini faz o mesmo trabalho por menos dinheiro. Com LangChain, vocÃª muda **uma linha** e pronto!\n\nÃ‰ como trocar o motor do carro sem precisar trocar o carro todo! ğŸš—â¡ï¸ğŸï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo prÃ¡tico: FunÃ§Ã£o que funciona com qualquer modelo\n",
        "# Essa Ã© a mÃ¡gica do LangChain!\n",
        "\n",
        "def analisador_sentimentos(texto, modelo_chat):\n",
        "    \"\"\"\n",
        "    Analisa o sentimento de um texto usando qualquer ChatModel\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analise o sentimento do seguinte texto e responda apenas com:\n",
        "    - POSITIVO\n",
        "    - NEGATIVO  \n",
        "    - NEUTRO\n",
        "    \n",
        "    Texto: \"{texto}\"\n",
        "    \n",
        "    Sentimento:\n",
        "    \"\"\"\n",
        "    \n",
        "    resposta = modelo_chat.invoke(prompt)\n",
        "    return resposta.content.strip()\n",
        "\n",
        "# Testando com diferentes textos\n",
        "textos_teste = [\n",
        "    \"Eu amo programar em Python!\",\n",
        "    \"Esse cÃ³digo estÃ¡ horrÃ­vel e cheio de bugs.\",\n",
        "    \"O tempo hoje estÃ¡ nublado.\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ­ AnÃ¡lise de Sentimentos com Gemini:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, texto in enumerate(textos_teste, 1):\n",
        "    sentimento = analisador_sentimentos(texto, gemini)\n",
        "    print(f\"{i}. \\\"{texto}\\\" â†’ {sentimento}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ A mesma funÃ§Ã£o funciona com qualquer ChatModel!\")\n",
        "print(\"Troque sÃ³ o parÃ¢metro 'gemini' por outro modelo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cio PrÃ¡tico: Seu Primeiro Chatbot\n\nBora colocar a mÃ£o na massa! Vamos criar um chatbot simples que:\n1. MantÃ©m histÃ³rico da conversa\n2. Tem uma personalidade definida\n3. Pode ser usado com qualquer ChatModel\n\n**Desafio:** Complete o cÃ³digo abaixo para fazer o chatbot funcionar! ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO: Complete este chatbot!\n",
        "# Siga as instruÃ§Ãµes nos comentÃ¡rios\n",
        "\n",
        "class ChatbotSimples:\n",
        "    def __init__(self, modelo_chat, personalidade=\"VocÃª Ã© um assistente Ãºtil e amigÃ¡vel.\"):\n",
        "        self.modelo = modelo_chat\n",
        "        self.historico = [SystemMessage(content=personalidade)]\n",
        "        \n",
        "    def conversar(self, mensagem_usuario):\n",
        "        # TODO: Adicione a mensagem do usuÃ¡rio ao histÃ³rico\n",
        "        # Use HumanMessage(content=mensagem_usuario)\n",
        "        self.historico.append(HumanMessage(content=mensagem_usuario))\n",
        "        \n",
        "        # TODO: Envie o histÃ³rico completo para o modelo\n",
        "        resposta = self.modelo.invoke(self.historico)\n",
        "        \n",
        "        # TODO: Adicione a resposta da IA ao histÃ³rico\n",
        "        # Use AIMessage(content=resposta.content)\n",
        "        self.historico.append(AIMessage(content=resposta.content))\n",
        "        \n",
        "        return resposta.content\n",
        "    \n",
        "    def limpar_historico(self):\n",
        "        # MantÃ©m sÃ³ a mensagem de sistema (personalidade)\n",
        "        self.historico = self.historico[:1]\n",
        "\n",
        "# Criando um chatbot especializado\n",
        "personalidade = \"\"\"\n",
        "VocÃª Ã© o Pedro Bot, um assistente especializado em ensinar programaÃ§Ã£o.\n",
        "Responda sempre de forma didÃ¡tica, com exemplos prÃ¡ticos e um toque de humor brasileiro.\n",
        "Use expressÃµes como \"Liiindo!\", \"Bora!\", \"Dica!\" nas suas respostas.\n",
        "\"\"\"\n",
        "\n",
        "pedro_bot = ChatbotSimples(gemini, personalidade)\n",
        "\n",
        "print(\"ğŸ¤– Pedro Bot criado com sucesso!\")\n",
        "print(\"ğŸ’¬ Comece a conversar digitando suas perguntas\")\n",
        "print(\"ğŸ“ Digite 'sair' para encerrar\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o Pedro Bot!\n",
        "# Vamos fazer algumas perguntas\n",
        "\n",
        "perguntas_teste = [\n",
        "    \"Oi Pedro Bot! Como vocÃª estÃ¡?\",\n",
        "    \"Me explica o que Ã© uma variÃ¡vel em Python?\",\n",
        "    \"E uma funÃ§Ã£o? DÃ¡ um exemplo prÃ¡tico.\",\n",
        "    \"Obrigado pelas explicaÃ§Ãµes!\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ­ Simulando conversa com Pedro Bot:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for pergunta in perguntas_teste:\n",
        "    print(f\"\\nğŸ‘¤ UsuÃ¡rio: {pergunta}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    resposta = pedro_bot.conversar(pergunta)\n",
        "    print(f\"ğŸ¤– Pedro Bot: {resposta}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(f\"\\nğŸ“Š Total de mensagens no histÃ³rico: {len(pedro_bot.historico)}\")\n",
        "print(\"ğŸ’¡ O bot lembra de toda a conversa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”® Preparando para os PrÃ³ximos MÃ³dulos\n\nLiiindo! VocÃª jÃ¡ domina os ChatModels! ğŸ‰\n\nMas espera que vem mais coisa boa! Nos prÃ³ximos mÃ³dulos vamos ver:\n\n### ğŸ“œ MÃ³dulo 3 - Runnables e LCEL\n- Como conectar ChatModels com outros componentes\n- A linguagem de expressÃµes do LangChain\n- Pipelines de processamento\n\n### ğŸ¯ MÃ³dulo 4 - Prompt Templates\n- Como criar prompts dinÃ¢micos e reutilizÃ¡veis\n- Templates que se adaptam a diferentes contextos\n- Prompts profissionais que realmente funcionam\n\n**Spoiler:** Os ChatModels que vocÃª aprendeu aqui vÃ£o ser usados em TODOS os prÃ³ximos mÃ³dulos! ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Resumo do MÃ³dulo: O que Aprendemos\n\nCara, que jornada! Vamos recapitular o que vocÃª virou expert hoje:\n\n### âœ… Conceitos Fundamentais\n- **ChatModels**: Interface padronizada para conversar com IAs\n- **Tipos de mensagem**: Human, AI e System\n- **ParÃ¢metros cruciais**: Temperature, max_tokens, top_p\n\n### âœ… Modelos que Dominamos\n- **Gemini 2.0 Flash**: Nosso protagonista (rÃ¡pido e gratuito)\n- **OpenAI GPT-4**: O clÃ¡ssico (pago mas poderoso)\n- **Claude**: O cuidadoso (bom para textos longos)\n\n### âœ… TÃ©cnicas AvanÃ§adas\n- **Streaming**: Respostas em tempo real\n- **Model Switching**: Trocar modelos sem quebrar cÃ³digo\n- **Chatbots com memÃ³ria**: HistÃ³rico de conversas\n\n### ğŸ¯ PrÃ³ximos Passos\nAgora vocÃª tem a **base sÃ³lida** para os prÃ³ximos mÃ³dulos! Os ChatModels sÃ£o o coraÃ§Ã£o de tudo que vamos construir.\n\n**Dica Final!** Pratique bastante com diferentes temperaturas e prompts. A prÃ¡tica leva Ã  perfeiÃ§Ã£o! ğŸ’ª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ† Desafio Extra: Para os Corajosos!\n\nSe vocÃª chegou atÃ© aqui, Ã© porque estÃ¡ **realmente** comprometido! Aqui vai um desafio extra:\n\n**MissÃ£o:** Crie um sistema que:\n1. Testa uma pergunta em TODOS os modelos disponÃ­veis\n2. Compara as respostas lado a lado\n3. Mede tempo de resposta e tokens usados\n4. Gera um relatÃ³rio final\n\nUse tudo que vocÃª aprendeu no mÃ³dulo! ğŸš€\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-02_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DESAFIO EXTRA: Sistema de ComparaÃ§Ã£o de Modelos\n",
        "# Complete este cÃ³digo para fazer um comparador completo!\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def comparar_modelos(pergunta, modelos_dict):\n",
        "    \"\"\"\n",
        "    Compara diferentes modelos com a mesma pergunta\n",
        "    \"\"\"\n",
        "    resultados = {}\n",
        "    \n",
        "    print(f\"ğŸ” Comparando modelos com: \\\"{pergunta}\\\"\\n\")\n",
        "    \n",
        "    for nome, modelo in modelos_dict.items():\n",
        "        print(f\"â±ï¸ Testando {nome}...\")\n",
        "        \n",
        "        try:\n",
        "            # Mede tempo de resposta\n",
        "            inicio = time.time()\n",
        "            resposta = modelo.invoke(pergunta)\n",
        "            tempo = time.time() - inicio\n",
        "            \n",
        "            # Armazena resultados\n",
        "            resultados[nome] = {\n",
        "                'resposta': resposta.content,\n",
        "                'tempo': tempo,\n",
        "                'tokens': len(resposta.content.split()),  # AproximaÃ§Ã£o\n",
        "                'caracteres': len(resposta.content)\n",
        "            }\n",
        "            \n",
        "            print(f\"âœ… {nome}: {tempo:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {nome}: Erro - {e}\")\n",
        "            resultados[nome] = {'erro': str(e)}\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "# Teste o sistema!\n",
        "pergunta_desafio = \"Crie um cÃ³digo Python que calcule a sequÃªncia de Fibonacci atÃ© o 10Âº termo.\"\n",
        "\n",
        "# Use todos os modelos que vocÃª tem disponÃ­vel\n",
        "resultados_comparacao = comparar_modelos(pergunta_desafio, {'Gemini': gemini})\n",
        "\n",
        "# TODO: Adicione mais modelos se vocÃª tiver as APIs!\n",
        "# TODO: Crie um relatÃ³rio mais detalhado!\n",
        "# TODO: Adicione visualizaÃ§Ãµes!\n",
        "\n",
        "print(\"\\nğŸ¯ ComparaÃ§Ã£o concluÃ­da!\")\n",
        "print(f\"ğŸ“Š Modelos testados: {len(resultados_comparacao)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# ğŸ‰ ParabÃ©ns! VocÃª Domina ChatModels!\n\nCara, que orgulho! VocÃª acabou de dar um **passo gigante** no mundo do LangChain! ğŸš€\n\n### O que vocÃª conquistou hoje:\n- âœ… Entendeu o que sÃ£o ChatModels e por que sÃ£o importantes\n- âœ… Configurou e usou o Gemini 2.0 Flash\n- âœ… Aprendeu sobre outros modelos (OpenAI, Claude)\n- âœ… Dominou parÃ¢metros como temperature e tokens\n- âœ… Implementou streaming para melhor UX\n- âœ… Criou seu primeiro chatbot com memÃ³ria\n- âœ… Aprendeu a trocar modelos sem quebrar cÃ³digo\n\n**Agora vocÃª tem a base sÃ³lida para construir qualquer aplicaÃ§Ã£o de IA!** ğŸ’ª\n\n### ğŸ¯ PrÃ³ximo mÃ³dulo: **Runnables e LCEL**\nVamos aprender a conectar ChatModels com outros componentes e criar pipelines poderosos!\n\n**Dica final:** Salve este notebook e use como referÃªncia. VocÃª vai voltar aqui MUITAS vezes! ğŸ“š\n\nNos vemos no prÃ³ximo mÃ³dulo! Bora que tÃ¡ sÃ³ comeÃ§ando! ğŸš€âœ¨\n\n*Pedro Nunes Guth - Expert em IA e AWS*"
      ]
    }
  ]
}