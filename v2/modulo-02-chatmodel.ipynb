{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatmodels-langchain-guth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🤖 ChatModels no LangChain: Seu Primeiro Amigo Virtual de IA!\n\n*Pedro Nunes Guth - Módulo 2 do Curso LangChain v0.3*\n\n---\n\nBora começar nossa jornada de verdade no LangChain! Se no módulo passado entendemos **o que** é o LangChain, agora vamos colocar a mão na massa com o **coração** de qualquer aplicação de IA: os **ChatModels**!\n\nPensa assim: se o LangChain fosse um carro, os ChatModels seriam o motor. Sem eles, não saímos do lugar! 🚗💨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 O que você vai aprender neste módulo:\n\n- O que diabos é um ChatModel e por que ele é TÃO importante\n- Como usar o Gemini 2.0 Flash (nosso queridinho do curso)\n- Outras opções de ChatModels (OpenAI, Claude, Ollama)\n- Como trocar de modelo sem quebrar o código\n- Parâmetros que fazem toda a diferença\n- Streaming (receber respostas em tempo real)\n\n**Dica!** Guarda esse notebook como ouro! Você vai usar esses conceitos em TODOS os próximos módulos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Tá, mas o que é um ChatModel?\n\nImagina que você tem um amigo super inteligente que:\n- Entende qualquer pergunta que você faz\n- Responde de forma natural e conversacional\n- Lembra do contexto da conversa\n- Pode ser especialista em qualquer assunto\n\nEsse amigo seria um **ChatModel**! 🧠✨\n\nTecnicamente falando, um ChatModel é:\n> Uma interface padronizada do LangChain que permite conversar com diferentes modelos de linguagem (LLMs) de forma consistente\n\nA mágica é que você aprende a usar UM modelo, e consegue usar TODOS os outros com a mesma sintaxe!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏗️ Arquitetura dos ChatModels\n\n```mermaid\ngraph TD\n    A[Sua Aplicação] --> B[LangChain ChatModel]\n    B --> C[Gemini 2.0 Flash]\n    B --> D[OpenAI GPT-4]\n    B --> E[Claude]\n    B --> F[Ollama Local]\n    \n    C --> G[Resposta Padronizada]\n    D --> G\n    E --> G\n    F --> G\n    \n    G --> A\n```\n\nViu só? Uma interface, múltiplas possibilidades! É como ter um controle universal para todos os modelos de IA do mundo! 🎮"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bora instalar tudo que vamos precisar!\n",
        "# Se você tá no Colab, rode isso aqui primeiro\n",
        "\n",
        "!pip install langchain langchain-google-genai langchain-openai langchain-anthropic python-dotenv\n",
        "\n",
        "print(\"Liiindo! Tudo instalado! 🚀\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports básicos - nossos melhores amigos daqui pra frente!\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ChatModels que vamos usar\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Para trabalhar com mensagens\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Carrega as variáveis de ambiente (suas chaves de API)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Imports carregados! Vamos começar a brincadeira! 🎉\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔑 Configurando suas APIs\n\nÓ, antes de continuar, você precisa ter pelo menos a API do Google (Gemini). É de graça e muito boa!\n\n**Como conseguir:**\n1. Vai no [Google AI Studio](https://aistudio.google.com/)\n2. Clica em \"Get API Key\"\n3. Copia a chave\n4. Cola no código abaixo\n\n**Dica!** Em produção, NUNCA deixe suas chaves no código! Use variáveis de ambiente sempre!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure suas chaves de API aqui\n",
        "# ATENÇÃO: Em produção, use variáveis de ambiente!\n",
        "\n",
        "# Se você tem as chaves, descomente e preencha:\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"sua_chave_google_aqui\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sua_chave_openai_aqui\" \n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sua_chave_anthropic_aqui\"\n",
        "\n",
        "# Vamos verificar se pelo menos o Google tá configurado\n",
        "if \"GOOGLE_API_KEY\" in os.environ:\n",
        "    print(\"✅ Google API configurada! Partiu Gemini!\")\n",
        "else:\n",
        "    print(\"❌ Opa! Precisa configurar a GOOGLE_API_KEY primeiro!\")\n",
        "    print(\"Vai no Google AI Studio e pega sua chave: https://aistudio.google.com/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Nosso Primeiro ChatModel: Gemini 2.0 Flash\n\nVamos começar com nosso protagonista do curso: o **Gemini 2.0 Flash**!\n\nPor que escolhi ele?\n- **Rápido** como um raio ⚡\n- **Gratuito** (até um limite generoso)\n- **Multimodal** (texto, imagem, áudio)\n- **Qualidade** excelente para a maioria dos casos\n\nÉ como ter uma Ferrari que não gasta gasolina! 🏎️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando nosso primeiro ChatModel!\n",
        "# É mais simples do que você imagina\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",  # Nosso modelo escolhido\n",
        "    temperature=0.7,               # Criatividade (0 = robótico, 1 = criativo)\n",
        "    max_tokens=1000               # Máximo de tokens na resposta\n",
        ")\n",
        "\n",
        "print(\"Gemini carregado e pronto pra conversar! 🤖✨\")\n",
        "print(f\"Modelo: {gemini.model_name}\")\n",
        "print(f\"Temperature: {gemini.temperature}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeira conversa com o Gemini!\n",
        "# Vamos mandar uma mensagem simples\n",
        "\n",
        "mensagem = \"Oi! Me explica em uma frase o que é inteligência artificial.\"\n",
        "\n",
        "# Invoca o modelo (essa é a mágica!)\n",
        "resposta = gemini.invoke(mensagem)\n",
        "\n",
        "print(\"🤖 Gemini respondeu:\")\n",
        "print(resposta.content)\n",
        "print(\"\\n💡 Tipo da resposta:\", type(resposta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 Tipos de Mensagens: A Linguagem dos ChatModels\n\nTá, mas o que acabou de acontecer ali em cima? \n\nO LangChain tem **3 tipos principais** de mensagens:\n\n1. **HumanMessage** - Suas mensagens (você, o usuário)\n2. **AIMessage** - Respostas da IA\n3. **SystemMessage** - Instruções para a IA (como ela deve se comportar)\n\nÉ como se fosse um protocolo de conversa! 💬\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-02_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos usar mensagens estruturadas agora\n",
        "# Muito mais poderoso que texto simples!\n",
        "\n",
        "mensagens = [\n",
        "    SystemMessage(content=\"Você é um especialista em explicar tecnologia de forma simples e divertida, usando analogias do cotidiano brasileiro.\"),\n",
        "    HumanMessage(content=\"Me explica o que é machine learning usando uma analogia com futebol.\")\n",
        "]\n",
        "\n",
        "resposta = gemini.invoke(mensagens)\n",
        "\n",
        "print(\"🤖 Gemini com personalidade:\")\n",
        "print(resposta.content)\n",
        "\n",
        "# Vamos ver os metadados da resposta\n",
        "print(\"\\n📊 Metadados da resposta:\")\n",
        "print(f\"Tokens usados: {resposta.usage_metadata if hasattr(resposta, 'usage_metadata') else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎛️ Parâmetros que Fazem a Diferença\n\nAgora vamos entender os **parâmetros mais importantes** dos ChatModels:\n\n### Temperature 🌡️\n- **0.0**: Robótico, sempre a mesma resposta\n- **0.5**: Balanceado (recomendado para a maioria)\n- **1.0**: Criativo, respostas mais variadas\n\n### Max Tokens 📏\n- Controla o tamanho máximo da resposta\n- 1 token ≈ 0.75 palavras em inglês\n- 1 token ≈ 0.5 palavras em português\n\n**Dica!** Temperature é como o tempero da comida: pouco fica sem graça, muito fica ruim! 🧂"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando diferentes temperatures\n",
        "# Vamos ver a diferença na prática!\n",
        "\n",
        "pergunta = \"Me conte uma piada sobre programação.\"\n",
        "\n",
        "temperatures = [0.0, 0.5, 1.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n🌡️ Temperature {temp}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Cria um modelo com temperature específica\n",
        "    modelo_temp = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        temperature=temp\n",
        "    )\n",
        "    \n",
        "    resposta = modelo_temp.invoke(pergunta)\n",
        "    print(resposta.content[:200] + \"...\" if len(resposta.content) > 200 else resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Outras Opções de ChatModels\n\nBeleza, o Gemini é nosso queridinho, mas vamos conhecer a família toda!\n\n### 🤖 OpenAI (GPT-4)\n- **Prós**: Muito popular, documentação excelente\n- **Contras**: Pago desde o primeiro uso\n\n### 🧠 Anthropic (Claude)\n- **Prós**: Muito bom para textos longos, seguro\n- **Contras**: Mais caro, menos APIs disponíveis\n\n### 🏠 Ollama (Local)\n- **Prós**: Roda na sua máquina, privacidade total\n- **Contras**: Precisa de hardware potente\n\nÉ como escolher entre Netflix, Amazon Prime e Globoplay - cada um tem seus pontos fortes! 📺"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar diferentes ChatModels\n",
        "# (Só vai funcionar se você tiver as APIs configuradas)\n",
        "\n",
        "modelos = {}\n",
        "\n",
        "# Gemini (nosso favorito)\n",
        "modelos['gemini'] = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# OpenAI GPT-4 (se você tem a API)\n",
        "if \"OPENAI_API_KEY\" in os.environ:\n",
        "    modelos['gpt4'] = ChatOpenAI(\n",
        "        model=\"gpt-4\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(\"✅ GPT-4 configurado!\")\n",
        "else:\n",
        "    print(\"❌ OpenAI API não configurada\")\n",
        "\n",
        "# Claude (se você tem a API)\n",
        "if \"ANTHROPIC_API_KEY\" in os.environ:\n",
        "    modelos['claude'] = ChatAnthropic(\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "    print(\"✅ Claude configurado!\")\n",
        "else:\n",
        "    print(\"❌ Anthropic API não configurada\")\n",
        "\n",
        "print(f\"\\n🎯 Total de modelos disponíveis: {len(modelos)}\")\n",
        "print(f\"📋 Modelos: {list(modelos.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando todos os modelos com a mesma pergunta\n",
        "# Vamos ver as diferenças!\n",
        "\n",
        "pergunta_teste = \"Explique em 2 frases o que é LangChain.\"\n",
        "\n",
        "for nome, modelo in modelos.items():\n",
        "    print(f\"\\n🤖 {nome.upper()}:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        resposta = modelo.invoke(pergunta_teste)\n",
        "        print(resposta.content)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro: {e}\")\n",
        "        print(\"(Provavelmente API não configurada)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌊 Streaming: Recebendo Respostas em Tempo Real\n\nSabe quando você tá no ChatGPT e vê a resposta aparecendo palavra por palavra? Isso é **streaming**! 🎬\n\nÉ muito útil para:\n- **UX melhor**: Usuário vê que algo tá acontecendo\n- **Respostas longas**: Não fica esperando 30 segundos\n- **Aplicações reais**: Streamlit, chatbots, etc.\n\nA fórmula matemática do streaming é simples:\n$$\\text{Experiência do Usuário} = \\frac{\\text{Informação Útil}}{\\text{Tempo de Espera}}$$\n\nCom streaming, diminuímos o tempo de espera percebido! 📈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming com Gemini\n",
        "# Vamos ver a mágica acontecer!\n",
        "\n",
        "import time\n",
        "\n",
        "pergunta_longa = \"Me conte uma história sobre um robô que aprende a programar em Python. Faça uma história de pelo menos 3 parágrafos.\"\n",
        "\n",
        "print(\"🌊 Streaming iniciado...\")\n",
        "print(\"📝 Resposta:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# O método stream() retorna pedaços da resposta\n",
        "resposta_completa = \"\"\n",
        "for chunk in gemini.stream(pergunta_longa):\n",
        "    # Cada chunk tem um pedaço do texto\n",
        "    if hasattr(chunk, 'content'):\n",
        "        print(chunk.content, end='', flush=True)\n",
        "        resposta_completa += chunk.content\n",
        "        time.sleep(0.05)  # Só pra ficar mais visual\n",
        "\n",
        "print(\"\\n\\n✅ Streaming finalizado!\")\n",
        "print(f\"📊 Total de caracteres: {len(resposta_completa)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Configurações Avançadas\n\nAgora que já sabemos o básico, vamos para as configurações que fazem você parecer um **profissional**! 💼\n\n### Top-p (Nucleus Sampling)\n- Controla a diversidade das respostas\n- Valores entre 0.1 e 1.0\n- 0.9 é um bom padrão\n\n### Stop Sequences\n- Palavras que fazem o modelo parar de gerar\n- Útil para formatar saídas\n\n### Max Tokens vs Max Output Tokens\n- Max tokens: limite total (entrada + saída)\n- Max output tokens: só a resposta\n\n**Dica!** Essas configurações são como os temperos de uma receita - cada uma tem seu momento certo! 👨‍🍳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChatModel com configurações avançadas\n",
        "# Vamos criar um modelo bem configuradinho!\n",
        "\n",
        "gemini_avancado = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.8,           # Um pouco mais criativo\n",
        "    max_tokens=500,           # Respostas mais concisas\n",
        "    top_p=0.9,               # Diversidade controlada\n",
        "    stop=[\"\\n\\n\\n\", \"FIM\"]    # Para quando encontrar isso\n",
        ")\n",
        "\n",
        "# Teste com um prompt específico\n",
        "prompt_teste = \"\"\"\n",
        "Crie uma lista de 3 dicas para aprender programação:\n",
        "1.\n",
        "\"\"\"\n",
        "\n",
        "resposta = gemini_avancado.invoke(prompt_teste)\n",
        "\n",
        "print(\"🎯 Resposta com configurações avançadas:\")\n",
        "print(resposta.content)\n",
        "\n",
        "print(\"\\n⚙️ Configurações usadas:\")\n",
        "print(f\"Temperature: {gemini_avancado.temperature}\")\n",
        "print(f\"Max tokens: {gemini_avancado.max_tokens}\")\n",
        "print(f\"Top-p: {gemini_avancado.top_p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Visualizando Performance dos Modelos\n\nVamos criar um gráfico para comparar nossos modelos! É sempre bom ter dados visuais para tomar decisões. 📈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Vamos medir a velocidade de resposta dos modelos\n",
        "pergunta_padrao = \"O que é inteligência artificial?\"\n",
        "velocidades = {}\n",
        "qualidade_subjetiva = {}  # Nota de 1 a 10 (subjetiva)\n",
        "\n",
        "# Teste só com Gemini (já que é o que temos certeza que funciona)\n",
        "print(\"⏱️ Testando velocidade do Gemini...\")\n",
        "\n",
        "inicio = time.time()\n",
        "resposta_gemini = gemini.invoke(pergunta_padrao)\n",
        "tempo_gemini = time.time() - inicio\n",
        "\n",
        "velocidades['Gemini 2.0'] = tempo_gemini\n",
        "qualidade_subjetiva['Gemini 2.0'] = 8.5  # Nota subjetiva\n",
        "\n",
        "print(f\"✅ Gemini: {tempo_gemini:.2f} segundos\")\n",
        "print(f\"📝 Resposta: {resposta_gemini.content[:100]}...\")\n",
        "\n",
        "# Se tiver outros modelos, teste também\n",
        "if 'gpt4' in modelos:\n",
        "    print(\"\\n⏱️ Testando GPT-4...\")\n",
        "    inicio = time.time()\n",
        "    try:\n",
        "        resposta_gpt4 = modelos['gpt4'].invoke(pergunta_padrao)\n",
        "        tempo_gpt4 = time.time() - inicio\n",
        "        velocidades['GPT-4'] = tempo_gpt4\n",
        "        qualidade_subjetiva['GPT-4'] = 9.0\n",
        "        print(f\"✅ GPT-4: {tempo_gpt4:.2f} segundos\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro no GPT-4: {e}\")\n",
        "\n",
        "print(f\"\\n📊 Resultados coletados para {len(velocidades)} modelo(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando gráfico de comparação\n",
        "# Visualização sempre ajuda na tomada de decisão!\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico 1: Velocidade\n",
        "modelos_nomes = list(velocidades.keys())\n",
        "tempos = list(velocidades.values())\n",
        "\n",
        "bars1 = ax1.bar(modelos_nomes, tempos, color=['#4285F4', '#FF6B6B', '#4ECDC4'][:len(modelos_nomes)])\n",
        "ax1.set_title('⚡ Velocidade de Resposta (segundos)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Tempo (s)')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, tempo in zip(bars1, tempos):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "             f'{tempo:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gráfico 2: Qualidade (subjetiva)\n",
        "qualidades = [qualidade_subjetiva[modelo] for modelo in modelos_nomes]\n",
        "\n",
        "bars2 = ax2.bar(modelos_nomes, qualidades, color=['#4285F4', '#FF6B6B', '#4ECDC4'][:len(modelos_nomes)])\n",
        "ax2.set_title('⭐ Qualidade Subjetiva (1-10)', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Nota')\n",
        "ax2.set_ylim(0, 10)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adiciona valores nas barras\n",
        "for bar, qual in zip(bars2, qualidades):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{qual}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 Análise dos dados:\")\n",
        "print(f\"🏆 Modelo mais rápido: {min(velocidades, key=velocidades.get)}\")\n",
        "print(f\"⭐ Maior qualidade: {max(qualidade_subjetiva, key=qualidade_subjetiva.get)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Switching Between Models: A Mágica do LangChain\n\nAqui está a **beleza** do LangChain: você pode trocar de modelo sem mudar o resto do código! 🪄\n\nImagina que você desenvolveu uma aplicação inteira com GPT-4, mas depois descobriu que o Gemini faz o mesmo trabalho por menos dinheiro. Com LangChain, você muda **uma linha** e pronto!\n\nÉ como trocar o motor do carro sem precisar trocar o carro todo! 🚗➡️🏎️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo prático: Função que funciona com qualquer modelo\n",
        "# Essa é a mágica do LangChain!\n",
        "\n",
        "def analisador_sentimentos(texto, modelo_chat):\n",
        "    \"\"\"\n",
        "    Analisa o sentimento de um texto usando qualquer ChatModel\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analise o sentimento do seguinte texto e responda apenas com:\n",
        "    - POSITIVO\n",
        "    - NEGATIVO  \n",
        "    - NEUTRO\n",
        "    \n",
        "    Texto: \"{texto}\"\n",
        "    \n",
        "    Sentimento:\n",
        "    \"\"\"\n",
        "    \n",
        "    resposta = modelo_chat.invoke(prompt)\n",
        "    return resposta.content.strip()\n",
        "\n",
        "# Testando com diferentes textos\n",
        "textos_teste = [\n",
        "    \"Eu amo programar em Python!\",\n",
        "    \"Esse código está horrível e cheio de bugs.\",\n",
        "    \"O tempo hoje está nublado.\"\n",
        "]\n",
        "\n",
        "print(\"🎭 Análise de Sentimentos com Gemini:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, texto in enumerate(textos_teste, 1):\n",
        "    sentimento = analisador_sentimentos(texto, gemini)\n",
        "    print(f\"{i}. \\\"{texto}\\\" → {sentimento}\")\n",
        "\n",
        "print(\"\\n💡 A mesma função funciona com qualquer ChatModel!\")\n",
        "print(\"Troque só o parâmetro 'gemini' por outro modelo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Exercício Prático: Seu Primeiro Chatbot\n\nBora colocar a mão na massa! Vamos criar um chatbot simples que:\n1. Mantém histórico da conversa\n2. Tem uma personalidade definida\n3. Pode ser usado com qualquer ChatModel\n\n**Desafio:** Complete o código abaixo para fazer o chatbot funcionar! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO: Complete este chatbot!\n",
        "# Siga as instruções nos comentários\n",
        "\n",
        "class ChatbotSimples:\n",
        "    def __init__(self, modelo_chat, personalidade=\"Você é um assistente útil e amigável.\"):\n",
        "        self.modelo = modelo_chat\n",
        "        self.historico = [SystemMessage(content=personalidade)]\n",
        "        \n",
        "    def conversar(self, mensagem_usuario):\n",
        "        # TODO: Adicione a mensagem do usuário ao histórico\n",
        "        # Use HumanMessage(content=mensagem_usuario)\n",
        "        self.historico.append(HumanMessage(content=mensagem_usuario))\n",
        "        \n",
        "        # TODO: Envie o histórico completo para o modelo\n",
        "        resposta = self.modelo.invoke(self.historico)\n",
        "        \n",
        "        # TODO: Adicione a resposta da IA ao histórico\n",
        "        # Use AIMessage(content=resposta.content)\n",
        "        self.historico.append(AIMessage(content=resposta.content))\n",
        "        \n",
        "        return resposta.content\n",
        "    \n",
        "    def limpar_historico(self):\n",
        "        # Mantém só a mensagem de sistema (personalidade)\n",
        "        self.historico = self.historico[:1]\n",
        "\n",
        "# Criando um chatbot especializado\n",
        "personalidade = \"\"\"\n",
        "Você é o Pedro Bot, um assistente especializado em ensinar programação.\n",
        "Responda sempre de forma didática, com exemplos práticos e um toque de humor brasileiro.\n",
        "Use expressões como \"Liiindo!\", \"Bora!\", \"Dica!\" nas suas respostas.\n",
        "\"\"\"\n",
        "\n",
        "pedro_bot = ChatbotSimples(gemini, personalidade)\n",
        "\n",
        "print(\"🤖 Pedro Bot criado com sucesso!\")\n",
        "print(\"💬 Comece a conversar digitando suas perguntas\")\n",
        "print(\"📝 Digite 'sair' para encerrar\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testando o Pedro Bot!\n",
        "# Vamos fazer algumas perguntas\n",
        "\n",
        "perguntas_teste = [\n",
        "    \"Oi Pedro Bot! Como você está?\",\n",
        "    \"Me explica o que é uma variável em Python?\",\n",
        "    \"E uma função? Dá um exemplo prático.\",\n",
        "    \"Obrigado pelas explicações!\"\n",
        "]\n",
        "\n",
        "print(\"🎭 Simulando conversa com Pedro Bot:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for pergunta in perguntas_teste:\n",
        "    print(f\"\\n👤 Usuário: {pergunta}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    resposta = pedro_bot.conversar(pergunta)\n",
        "    print(f\"🤖 Pedro Bot: {resposta}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(f\"\\n📊 Total de mensagens no histórico: {len(pedro_bot.historico)}\")\n",
        "print(\"💡 O bot lembra de toda a conversa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔮 Preparando para os Próximos Módulos\n\nLiiindo! Você já domina os ChatModels! 🎉\n\nMas espera que vem mais coisa boa! Nos próximos módulos vamos ver:\n\n### 📜 Módulo 3 - Runnables e LCEL\n- Como conectar ChatModels com outros componentes\n- A linguagem de expressões do LangChain\n- Pipelines de processamento\n\n### 🎯 Módulo 4 - Prompt Templates\n- Como criar prompts dinâmicos e reutilizáveis\n- Templates que se adaptam a diferentes contextos\n- Prompts profissionais que realmente funcionam\n\n**Spoiler:** Os ChatModels que você aprendeu aqui vão ser usados em TODOS os próximos módulos! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Resumo do Módulo: O que Aprendemos\n\nCara, que jornada! Vamos recapitular o que você virou expert hoje:\n\n### ✅ Conceitos Fundamentais\n- **ChatModels**: Interface padronizada para conversar com IAs\n- **Tipos de mensagem**: Human, AI e System\n- **Parâmetros cruciais**: Temperature, max_tokens, top_p\n\n### ✅ Modelos que Dominamos\n- **Gemini 2.0 Flash**: Nosso protagonista (rápido e gratuito)\n- **OpenAI GPT-4**: O clássico (pago mas poderoso)\n- **Claude**: O cuidadoso (bom para textos longos)\n\n### ✅ Técnicas Avançadas\n- **Streaming**: Respostas em tempo real\n- **Model Switching**: Trocar modelos sem quebrar código\n- **Chatbots com memória**: Histórico de conversas\n\n### 🎯 Próximos Passos\nAgora você tem a **base sólida** para os próximos módulos! Os ChatModels são o coração de tudo que vamos construir.\n\n**Dica Final!** Pratique bastante com diferentes temperaturas e prompts. A prática leva à perfeição! 💪"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏆 Desafio Extra: Para os Corajosos!\n\nSe você chegou até aqui, é porque está **realmente** comprometido! Aqui vai um desafio extra:\n\n**Missão:** Crie um sistema que:\n1. Testa uma pergunta em TODOS os modelos disponíveis\n2. Compara as respostas lado a lado\n3. Mede tempo de resposta e tokens usados\n4. Gera um relatório final\n\nUse tudo que você aprendeu no módulo! 🚀\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/notebooks/imagens/langchain-modulo-02_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DESAFIO EXTRA: Sistema de Comparação de Modelos\n",
        "# Complete este código para fazer um comparador completo!\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def comparar_modelos(pergunta, modelos_dict):\n",
        "    \"\"\"\n",
        "    Compara diferentes modelos com a mesma pergunta\n",
        "    \"\"\"\n",
        "    resultados = {}\n",
        "    \n",
        "    print(f\"🔍 Comparando modelos com: \\\"{pergunta}\\\"\\n\")\n",
        "    \n",
        "    for nome, modelo in modelos_dict.items():\n",
        "        print(f\"⏱️ Testando {nome}...\")\n",
        "        \n",
        "        try:\n",
        "            # Mede tempo de resposta\n",
        "            inicio = time.time()\n",
        "            resposta = modelo.invoke(pergunta)\n",
        "            tempo = time.time() - inicio\n",
        "            \n",
        "            # Armazena resultados\n",
        "            resultados[nome] = {\n",
        "                'resposta': resposta.content,\n",
        "                'tempo': tempo,\n",
        "                'tokens': len(resposta.content.split()),  # Aproximação\n",
        "                'caracteres': len(resposta.content)\n",
        "            }\n",
        "            \n",
        "            print(f\"✅ {nome}: {tempo:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ {nome}: Erro - {e}\")\n",
        "            resultados[nome] = {'erro': str(e)}\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "# Teste o sistema!\n",
        "pergunta_desafio = \"Crie um código Python que calcule a sequência de Fibonacci até o 10º termo.\"\n",
        "\n",
        "# Use todos os modelos que você tem disponível\n",
        "resultados_comparacao = comparar_modelos(pergunta_desafio, {'Gemini': gemini})\n",
        "\n",
        "# TODO: Adicione mais modelos se você tiver as APIs!\n",
        "# TODO: Crie um relatório mais detalhado!\n",
        "# TODO: Adicione visualizações!\n",
        "\n",
        "print(\"\\n🎯 Comparação concluída!\")\n",
        "print(f\"📊 Modelos testados: {len(resultados_comparacao)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# 🎉 Parabéns! Você Domina ChatModels!\n\nCara, que orgulho! Você acabou de dar um **passo gigante** no mundo do LangChain! 🚀\n\n### O que você conquistou hoje:\n- ✅ Entendeu o que são ChatModels e por que são importantes\n- ✅ Configurou e usou o Gemini 2.0 Flash\n- ✅ Aprendeu sobre outros modelos (OpenAI, Claude)\n- ✅ Dominou parâmetros como temperature e tokens\n- ✅ Implementou streaming para melhor UX\n- ✅ Criou seu primeiro chatbot com memória\n- ✅ Aprendeu a trocar modelos sem quebrar código\n\n**Agora você tem a base sólida para construir qualquer aplicação de IA!** 💪\n\n### 🎯 Próximo módulo: **Runnables e LCEL**\nVamos aprender a conectar ChatModels com outros componentes e criar pipelines poderosos!\n\n**Dica final:** Salve este notebook e use como referência. Você vai voltar aqui MUITAS vezes! 📚\n\nNos vemos no próximo módulo! Bora que tá só começando! 🚀✨\n\n*Pedro Nunes Guth - Expert em IA e AWS*"
      ]
    }
  ]
}