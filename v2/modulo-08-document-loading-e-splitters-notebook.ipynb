{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modulo_8_Document_Loading_Splitters.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Document Loading e Splitters - Preparando Dados para RAG\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/books/imagens/langchain-modulo-08_img_01.png)\n\n**M√≥dulo 8 de 17 - LangChain v0.3**\n\nFala pessoal! T√¥ aqui de novo com voc√™s pra mais um m√≥dulo do nosso curso de LangChain! üöÄ\n\nT√°, mas o que diabos √© Document Loading e Splitters? Imagina que voc√™ tem uma biblioteca gigante e precisa encontrar uma informa√ß√£o espec√≠fica. Voc√™ n√£o vai ler todos os livros n√©? Voc√™ vai direto no √≠ndice, nas p√°ginas certas!\n\n√â exatamente isso que fazemos com documentos em IA. Pegamos textos enormes e dividimos em pedacinhos menores que fazem sentido. √â tipo picar uma pizza - cada peda√ßo tem que ter tudo que precisa pra ser saboroso! üçï\n\n**Por que isso √© importante?**\n- Nos pr√≥ximos m√≥dulos vamos implementar RAG (Retrieval Augmented Generation)\n- RAG precisa de documentos bem organizados e divididos\n- √â a base para sistemas de busca inteligente\n\nBora come√ßar!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Setup Inicial - Instalando as Depend√™ncias\n\nAntes de come√ßar, vamos instalar tudo que precisamos. √â tipo preparar os ingredientes antes de cozinhar!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instalando as bibliotecas necess√°rias\n",
        "!pip install langchain langchain-community python-docx pypdf pymupdf beautifulsoup4 requests\n",
        "\n",
        "# Imports b√°sicos\n",
        "import os\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìñ O que s√£o Document Loaders?\n\nT√°, mas o que s√£o esses Document Loaders? Simples!\n\nImagina que voc√™ tem documentos em v√°rios formatos:\n- PDF (tipo aquele manual chato da TV)\n- TXT (arquivo simples de texto)\n- P√°ginas web (sites, blogs)\n- Word, Excel, PowerPoint\n\nCada formato tem sua \"linguagem\" pr√≥pria. Os Document Loaders s√£o tipo tradutores universais que pegam qualquer formato e transformam em algo que o LangChain entende!\n\n### Estrutura de um Document no LangChain\n\nTodo documento no LangChain tem duas partes principais:\n- **page_content**: O texto em si\n- **metadata**: Informa√ß√µes sobre o documento (t√≠tulo, fonte, p√°gina, etc.)\n\n√â tipo um envelope: tem a carta (conte√∫do) e as informa√ß√µes do envelope (metadados)!\n\n**Dica!** Sempre guarde metadados! Eles s√£o super √∫teis para rastrear de onde veio cada peda√ßo de informa√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vamos criar um documento simples para entender a estrutura\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Criando um documento b√°sico\n",
        "documento_exemplo = Document(\n",
        "    page_content=\"Este √© o conte√∫do do meu documento sobre IA. A intelig√™ncia artificial est√° revolucionando o mundo!\",\n",
        "    metadata={\n",
        "        \"titulo\": \"Introdu√ß√£o √† IA\",\n",
        "        \"autor\": \"Pedro Guth\",\n",
        "        \"data\": \"2024-01-15\",\n",
        "        \"categoria\": \"tecnologia\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Conte√∫do do documento:\")\n",
        "print(documento_exemplo.page_content)\n",
        "print(\"\\nMetadados:\")\n",
        "print(documento_exemplo.metadata)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Carregando Diferentes Tipos de Documentos\n\nAgora vamos ver como carregar documentos de diferentes fontes. √â tipo ter chaves para abrir diferentes tipos de portas!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Carregando arquivo de texto simples\n",
        "# Primeiro vamos criar um arquivo de exemplo\n",
        "texto_exemplo = \"\"\"\n",
        "LangChain √© uma biblioteca incr√≠vel para desenvolvimento de aplica√ß√µes com IA.\n",
        "Ela facilita muito a integra√ß√£o com diferentes modelos de linguagem.\n",
        "Nos m√≥dulos anteriores, aprendemos sobre ChatModels, Prompts e Chains.\n",
        "Agora estamos preparando dados para implementar RAG nos pr√≥ximos m√≥dulos.\n",
        "O RAG (Retrieval Augmented Generation) √© uma t√©cnica poderosa que combina busca e gera√ß√£o.\n",
        "\"\"\"\n",
        "\n",
        "# Salvando o arquivo\n",
        "with open(\"exemplo.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(texto_exemplo)\n",
        "\n",
        "# Carregando com TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader_texto = TextLoader(\"exemplo.txt\", encoding=\"utf-8\")\n",
        "documentos_texto = loader_texto.load()\n",
        "\n",
        "print(\"Documento carregado do arquivo TXT:\")\n",
        "print(f\"N√∫mero de documentos: {len(documentos_texto)}\")\n",
        "print(f\"Conte√∫do: {documentos_texto[0].page_content[:100]}...\")\n",
        "print(f\"Metadados: {documentos_texto[0].metadata}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Carregando conte√∫do de p√°ginas web\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Carregando uma p√°gina web (vamos usar uma p√°gina simples)\n",
        "try:\n",
        "    loader_web = WebBaseLoader(\"https://httpbin.org/html\")\n",
        "    documentos_web = loader_web.load()\n",
        "    \n",
        "    print(\"\\nDocumento carregado da web:\")\n",
        "    print(f\"N√∫mero de documentos: {len(documentos_web)}\")\n",
        "    print(f\"Conte√∫do (primeiros 200 chars): {documentos_web[0].page_content[:200]}...\")\n",
        "    print(f\"Metadados: {documentos_web[0].metadata}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar p√°gina web: {e}\")\n",
        "    print(\"Sem problemas! Vamos criar um documento web simulado:\")\n",
        "    \n",
        "    # Simulando conte√∫do web\n",
        "    documento_web_simulado = Document(\n",
        "        page_content=\"Este √© o conte√∫do de uma p√°gina web sobre LangChain. A p√°gina cont√©m informa√ß√µes sobre como usar document loaders para processar diferentes tipos de arquivo.\",\n",
        "        metadata={\"source\": \"https://exemplo.com/langchain\", \"title\": \"Guia LangChain\"}\n",
        "    )\n",
        "    print(f\"Conte√∫do simulado: {documento_web_simulado.page_content}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÇÔ∏è Text Splitters - Dividindo para Conquistar\n\nAgora vem a parte mais importante! T√°, mas por que dividir os textos?\n\nImagina que voc√™ tem um livro de 500 p√°ginas e quer encontrar informa√ß√£o sobre \"como fazer brigadeiro\". Voc√™ n√£o vai dar o livro inteiro para algu√©m ler n√©? Voc√™ vai direto no cap√≠tulo de doces!\n\n√â exatamente isso que os Text Splitters fazem:\n- Dividem textos grandes em peda√ßos menores\n- Mant√™m o contexto (n√£o cortam no meio de uma frase)\n- Facilitam a busca e processamento\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/books/imagens/langchain-modulo-08_img_02.png)\n\n### Por que isso √© importante para RAG?\n\nLembra que nos pr√≥ximos m√≥dulos vamos implementar RAG? Ent√£o:\n- RAG precisa encontrar informa√ß√µes relevantes rapidamente\n- Textos muito grandes s√£o dif√≠ceis de processar\n- Peda√ßos pequenos s√£o mais f√°ceis de buscar e comparar\n\n**Dica!** O tamanho ideal dos chunks (peda√ßos) depende do seu caso de uso. Para perguntas e respostas: 200-500 caracteres. Para an√°lise profunda: 1000-2000 caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vamos criar um texto mais longo para demonstrar os splitters\n",
        "texto_longo = \"\"\"\n",
        "O LangChain √© uma biblioteca poderosa que facilita o desenvolvimento de aplica√ß√µes com IA. \n",
        "Ela oferece uma abstra√ß√£o elegante para trabalhar com diferentes modelos de linguagem.\n",
        "\n",
        "Nos m√≥dulos anteriores, aprendemos sobre ChatModels que nos permitem conversar com IAs como GPT e Gemini.\n",
        "Tamb√©m vimos Prompt Templates que padronizam nossas conversas com a IA.\n",
        "As Chains nos ajudam a criar fluxos complexos de processamento.\n",
        "\n",
        "Agora estamos preparando dados para RAG (Retrieval Augmented Generation).\n",
        "O RAG √© uma t√©cnica que combina busca em documentos com gera√ß√£o de texto.\n",
        "√â muito √∫til para criar assistentes que podem responder baseados em documentos espec√≠ficos.\n",
        "\n",
        "Para implementar RAG, precisamos:\n",
        "1. Carregar documentos de diferentes fontes\n",
        "2. Dividir os textos em peda√ßos menores\n",
        "3. Criar embeddings (vamos ver no pr√≥ximo m√≥dulo)\n",
        "4. Armazenar em um vector store\n",
        "5. Implementar a busca e gera√ß√£o\n",
        "\n",
        "Os Document Loaders e Text Splitters s√£o o primeiro passo desta jornada.\n",
        "Eles preparam nossos dados para os pr√≥ximos passos do processo RAG.\n",
        "\"\"\"\n",
        "\n",
        "# Criando um documento com este texto\n",
        "documento_longo = Document(\n",
        "    page_content=texto_longo,\n",
        "    metadata={\"fonte\": \"curso_langchain\", \"modulo\": 8}\n",
        ")\n",
        "\n",
        "print(f\"Texto original tem {len(texto_longo)} caracteres\")\n",
        "print(f\"Vamos dividir este texto em peda√ßos menores!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß CharacterTextSplitter - O Divisor Simples\n\nO CharacterTextSplitter √© o mais b√°sico. Ele divide o texto baseado em um caractere espec√≠fico (tipo \\n para quebra de linha).\n\n√â tipo cortar uma pizza com uma r√©gua - funciona, mas n√£o √© o mais inteligente!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Usando CharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Configurando o splitter\n",
        "splitter_simples = CharacterTextSplitter(\n",
        "    chunk_size=300,      # Tamanho m√°ximo de cada peda√ßo\n",
        "    chunk_overlap=50,    # Sobreposi√ß√£o entre peda√ßos (importante para manter contexto!)\n",
        "    separator=\"\\n\\n\"      # Divide onde tem duas quebras de linha\n",
        ")\n",
        "\n",
        "# Dividindo o documento\n",
        "chunks_simples = splitter_simples.split_documents([documento_longo])\n",
        "\n",
        "print(f\"Documento dividido em {len(chunks_simples)} peda√ßos:\")\n",
        "print()\n",
        "\n",
        "# Mostrando cada peda√ßo\n",
        "for i, chunk in enumerate(chunks_simples):\n",
        "    print(f\"--- Peda√ßo {i+1} ({len(chunk.page_content)} chars) ---\")\n",
        "    print(chunk.page_content[:200] + \"...\" if len(chunk.page_content) > 200 else chunk.page_content)\n",
        "    print(f\"Metadados: {chunk.metadata}\")\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† RecursiveCharacterTextSplitter - O Inteligente\n\nAgora sim! Este √© o splitter mais usado e inteligente. Por qu√™?\n\nEle tenta dividir o texto seguindo uma hierarquia:\n1. Primeiro tenta dividir por par√°grafos (\\n\\n)\n2. Se ainda estiver grande, divide por frases (.)\n3. Se ainda estiver grande, divide por palavras ( )\n4. Por √∫ltimo, divide por caracteres\n\n√â tipo um chef experiente cortando ingredientes - ele sabe onde cortar para n√£o estragar o sabor!\n\n**Dica!** Este √© o splitter que voc√™ vai usar 90% das vezes. Ele √© inteligente o suficiente para manter o contexto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Usando RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configurando o splitter inteligente\n",
        "splitter_inteligente = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,        # Tamanho m√°ximo de cada peda√ßo\n",
        "    chunk_overlap=100,     # Sobreposi√ß√£o entre peda√ßos\n",
        "    length_function=len,   # Fun√ß√£o para medir o tamanho\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Hierarquia de separadores\n",
        ")\n",
        "\n",
        "# Dividindo o documento\n",
        "chunks_inteligentes = splitter_inteligente.split_documents([documento_longo])\n",
        "\n",
        "print(f\"Documento dividido em {len(chunks_inteligentes)} peda√ßos com RecursiveCharacterTextSplitter:\")\n",
        "print()\n",
        "\n",
        "# Mostrando cada peda√ßo\n",
        "for i, chunk in enumerate(chunks_inteligentes):\n",
        "    print(f\"--- Peda√ßo Inteligente {i+1} ({len(chunk.page_content)} chars) ---\")\n",
        "    print(chunk.page_content)\n",
        "    print(f\"Metadados: {chunk.metadata}\")\n",
        "    print(\"-\" * 50)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizando a Diferen√ßa entre Splitters\n\nVamos criar um gr√°fico para visualizar como cada splitter se comporta. √â sempre bom ver os dados!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Coletando dados dos splitters\n",
        "tamanhos_simples = [len(chunk.page_content) for chunk in chunks_simples]\n",
        "tamanhos_inteligentes = [len(chunk.page_content) for chunk in chunks_inteligentes]\n",
        "\n",
        "# Criando o gr√°fico\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Compara√ß√£o de tamanhos\n",
        "ax1.bar(range(len(tamanhos_simples)), tamanhos_simples, \n",
        "        alpha=0.7, label='Character Splitter', color='skyblue')\n",
        "ax1.bar(range(len(tamanhos_inteligentes)), tamanhos_inteligentes, \n",
        "        alpha=0.7, label='Recursive Splitter', color='lightcoral')\n",
        "ax1.set_xlabel('N√∫mero do Chunk')\n",
        "ax1.set_ylabel('Tamanho (caracteres)')\n",
        "ax1.set_title('Compara√ß√£o de Tamanhos dos Chunks')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 2: Distribui√ß√£o de tamanhos\n",
        "ax2.hist(tamanhos_simples, bins=5, alpha=0.7, label='Character Splitter', color='skyblue')\n",
        "ax2.hist(tamanhos_inteligentes, bins=5, alpha=0.7, label='Recursive Splitter', color='lightcoral')\n",
        "ax2.set_xlabel('Tamanho do Chunk (caracteres)')\n",
        "ax2.set_ylabel('Frequ√™ncia')\n",
        "ax2.set_title('Distribui√ß√£o dos Tamanhos')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nEstat√≠sticas:\")\n",
        "print(f\"Character Splitter: {len(chunks_simples)} chunks, tamanho m√©dio: {np.mean(tamanhos_simples):.1f}\")\n",
        "print(f\"Recursive Splitter: {len(chunks_inteligentes)} chunks, tamanho m√©dio: {np.mean(tamanhos_inteligentes):.1f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Par√¢metros Importantes dos Splitters\n\nT√°, mas quais s√£o os par√¢metros mais importantes? Vamos entender cada um:\n\n### chunk_size\n- Tamanho m√°ximo de cada peda√ßo\n- **Pequeno (100-300)**: Boa precis√£o, mas pode perder contexto\n- **M√©dio (300-800)**: Equil√≠brio entre precis√£o e contexto ‚≠ê\n- **Grande (800+)**: Muito contexto, mas busca menos precisa\n\n### chunk_overlap\n- Quantos caracteres se repetem entre chunks\n- **Importante**: Evita que informa√ß√µes sejam cortadas no meio\n- **Recomenda√ß√£o**: 10-20% do chunk_size\n\n### separators\n- Define onde o texto pode ser cortado\n- **Padr√£o**: [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n- **Para c√≥digo**: [\"\\n\\n\", \"\\n\", \";\", \"{\", \"}\", \" \"]\n\n**Dica!** Para RAG, comece com chunk_size=500 e chunk_overlap=100. Depois ajuste baseado nos seus testes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testando diferentes configura√ß√µes\n",
        "configura√ß√µes = [\n",
        "    {\"nome\": \"Pequenos\", \"chunk_size\": 200, \"chunk_overlap\": 50},\n",
        "    {\"nome\": \"M√©dios\", \"chunk_size\": 400, \"chunk_overlap\": 80},\n",
        "    {\"nome\": \"Grandes\", \"chunk_size\": 800, \"chunk_overlap\": 150}\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for config in configura√ß√µes:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunk_size\"],\n",
        "        chunk_overlap=config[\"chunk_overlap\"]\n",
        "    )\n",
        "    \n",
        "    chunks = splitter.split_documents([documento_longo])\n",
        "    \n",
        "    resultado = {\n",
        "        \"nome\": config[\"nome\"],\n",
        "        \"num_chunks\": len(chunks),\n",
        "        \"tamanho_medio\": np.mean([len(chunk.page_content) for chunk in chunks]),\n",
        "        \"chunk_size_config\": config[\"chunk_size\"]\n",
        "    }\n",
        "    \n",
        "    resultados.append(resultado)\n",
        "    \n",
        "    print(f\"{config['nome']}: {len(chunks)} chunks, tamanho m√©dio: {resultado['tamanho_medio']:.1f}\")\n",
        "\n",
        "print(\"\\nQual configura√ß√£o escolher?\")\n",
        "print(\"üìç Pequenos: √ìtimo para busca precisa\")\n",
        "print(\"üéØ M√©dios: Equilibrio ideal para a maioria dos casos\")\n",
        "print(\"üìö Grandes: Bom quando voc√™ precisa de muito contexto\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Exemplo Pr√°tico - Processando Conte√∫do Web\n\nVamos fazer um exemplo mais pr√≥ximo da realidade: carregar conte√∫do de uma p√°gina web e preparar para RAG!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulando conte√∫do de um blog sobre IA\n",
        "conteudo_blog = \"\"\"\n",
        "# Intelig√™ncia Artificial no Brasil: O Futuro √© Agora\n",
        "\n",
        "A intelig√™ncia artificial est√° transformando o mercado brasileiro de forma acelerada. \n",
        "Empresas de todos os tamanhos est√£o adotando solu√ß√µes de IA para melhorar seus processos.\n",
        "\n",
        "## Principais Aplica√ß√µes\n",
        "\n",
        "### Atendimento ao Cliente\n",
        "Chatbots inteligentes est√£o revolucionando o atendimento. Eles conseguem resolver 80% dos problemas sem interven√ß√£o humana.\n",
        "Empresas como Magazine Luiza e Bradesco j√° implementaram solu√ß√µes avan√ßadas.\n",
        "\n",
        "### An√°lise de Dados\n",
        "Machine Learning est√° sendo usado para analisar padr√µes de consumo e prever tend√™ncias.\n",
        "O setor financeiro √© pioneiro nesta √°rea, usando IA para detectar fraudes e avaliar riscos.\n",
        "\n",
        "### Automa√ß√£o de Processos\n",
        "RPA (Robotic Process Automation) combinado com IA est√° automatizando tarefas repetitivas.\n",
        "Isso libera funcion√°rios para atividades mais estrat√©gicas e criativas.\n",
        "\n",
        "## Desafios e Oportunidades\n",
        "\n",
        "O maior desafio √© a capacita√ß√£o profissional. O mercado demanda profissionais qualificados em IA.\n",
        "Por outro lado, isso cria oportunidades enormes para quem se especializar na √°rea.\n",
        "\n",
        "## Conclus√£o\n",
        "\n",
        "A IA n√£o √© mais coisa do futuro - √© realidade do presente.\n",
        "Empresas que n√£o se adaptarem ficar√£o para tr√°s na competi√ß√£o.\n",
        "\"\"\"\n",
        "\n",
        "# Criando documento simulando conte√∫do web\n",
        "documento_blog = Document(\n",
        "    page_content=conteudo_blog,\n",
        "    metadata={\n",
        "        \"source\": \"https://blog-ia-brasil.com/futuro-agora\",\n",
        "        \"title\": \"IA no Brasil: O Futuro √© Agora\",\n",
        "        \"author\": \"Tech Blog Brasil\",\n",
        "        \"date\": \"2024-01-15\",\n",
        "        \"category\": \"tecnologia\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Documento do blog carregado!\")\n",
        "print(f\"Tamanho: {len(conteudo_blog)} caracteres\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Processando o conte√∫do do blog para RAG\n",
        "splitter_blog = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks_blog = splitter_blog.split_documents([documento_blog])\n",
        "\n",
        "print(f\"Blog dividido em {len(chunks_blog)} chunks para RAG:\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "for i, chunk in enumerate(chunks_blog):\n",
        "    print(f\"\\nChunk {i+1} ({len(chunk.page_content)} chars):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(chunk.page_content.strip())\n",
        "    print(f\"Metadados: {chunk.metadata}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Liiindo! Agora temos os dados prontos para o pr√≥ximo m√≥dulo (Vector Store)!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Fluxo Completo - Da Fonte ao Chunk\n\nVamos visualizar todo o processo que acabamos de aprender:\n\n```mermaid\ngraph TD\n    A[Documento Original] --> B[Document Loader]\n    B --> C[Document Object]\n    C --> D[Text Splitter]\n    D --> E[Chunks Menores]\n    E --> F[Prontos para Vector Store]\n    \n    B1[PDF] --> B\n    B2[TXT] --> B\n    B3[Web] --> B\n    B4[Word] --> B\n    \n    D1[CharacterTextSplitter] --> D\n    D2[RecursiveCharacterTextSplitter] --> D\n```\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/books/imagens/langchain-modulo-08_img_03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Dicas Avan√ßadas e Boas Pr√°ticas\n\nAgora que voc√™ j√° sabe o b√°sico, vou compartilhar algumas dicas de quem j√° quebrou a cabe√ßa com isso:\n\n### 1. Preservando Metadados Importantes\n- Sempre mantenha informa√ß√µes de fonte\n- Adicione timestamps quando relevante\n- Use metadados para filtrar buscas\n\n### 2. Tamanho dos Chunks para Diferentes Casos\n- **FAQ/Q&A**: 100-300 caracteres\n- **Documenta√ß√£o t√©cnica**: 400-800 caracteres\n- **Artigos longos**: 600-1200 caracteres\n- **C√≥digo**: 200-500 caracteres\n\n### 3. Overlap Estrat√©gico\n- Para textos t√©cnicos: 20% do chunk_size\n- Para narrativas: 15% do chunk_size\n- Para listas/FAQ: 10% do chunk_size\n\n**Dica!** No pr√≥ximo m√≥dulo (Vector Store), vamos ver como esses chunks s√£o transformados em embeddings para busca sem√¢ntica. √â a√≠ que a m√°gica do RAG realmente acontece!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exemplo de preserva√ß√£o de metadados estrat√©gicos\n",
        "def processar_documento_completo(conteudo, fonte, categoria):\n",
        "    \"\"\"Fun√ß√£o para processar documento mantendo metadados importantes\"\"\"\n",
        "    \n",
        "    # Criando documento com metadados ricos\n",
        "    documento = Document(\n",
        "        page_content=conteudo,\n",
        "        metadata={\n",
        "            \"fonte\": fonte,\n",
        "            \"categoria\": categoria,\n",
        "            \"tamanho_original\": len(conteudo),\n",
        "            \"processado_em\": \"2024-01-15\",\n",
        "            \"versao_langchain\": \"0.3\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Configurando splitter otimizado\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=80,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Dividindo e enriquecendo metadados\n",
        "    chunks = splitter.split_documents([documento])\n",
        "    \n",
        "    # Adicionando n√∫mero do chunk nos metadados\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk.metadata[\"chunk_id\"] = i\n",
        "        chunk.metadata[\"total_chunks\"] = len(chunks)\n",
        "        chunk.metadata[\"chunk_size\"] = len(chunk.page_content)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Testando a fun√ß√£o\n",
        "chunks_processados = processar_documento_completo(\n",
        "    conteudo=conteudo_blog,\n",
        "    fonte=\"blog_ia_brasil\",\n",
        "    categoria=\"tecnologia\"\n",
        ")\n",
        "\n",
        "print(f\"Processamento completo: {len(chunks_processados)} chunks gerados\")\n",
        "print(f\"\\nExemplo de metadados enriquecidos:\")\n",
        "print(chunks_processados[0].metadata)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cios Pr√°ticos\n\nAgora √© sua vez de praticar! Bora colocar a m√£o na massa!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exerc√≠cio 1: Processando Diferentes Tipos de Conte√∫do\n\nCrie uma fun√ß√£o que processe diferentes tipos de documentos e encontre a configura√ß√£o ideal de chunk para cada tipo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EXERC√çCIO 1: Complete o c√≥digo abaixo\n",
        "\n",
        "# Documentos de exemplo de diferentes tipos\n",
        "doc_faq = \"\"\"\n",
        "P: Como instalar o LangChain?\n",
        "R: Use pip install langchain\n",
        "\n",
        "P: O que √© um Document Loader?\n",
        "R: √â uma ferramenta para carregar documentos de diferentes fontes\n",
        "\n",
        "P: Qual a diferen√ßa entre splitters?\n",
        "R: RecursiveCharacterTextSplitter √© mais inteligente que CharacterTextSplitter\n",
        "\"\"\"\n",
        "\n",
        "doc_tecnico = \"\"\"\n",
        "A arquitetura do LangChain √© baseada em componentes modulares que permitem flexibilidade m√°xima.\n",
        "Os Document Loaders s√£o respons√°veis por converter diferentes formatos em objetos Document padronizados.\n",
        "Este processo de normaliza√ß√£o √© fundamental para o funcionamento correto dos Text Splitters.\n",
        "Os Text Splitters implementam algoritmos sofisticados para dividir texto mantendo contexto sem√¢ntico.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Complete a fun√ß√£o abaixo\n",
        "def encontrar_melhor_config(documento, tipo_doc):\n",
        "    \"\"\"Encontra a melhor configura√ß√£o de chunk para cada tipo de documento\"\"\"\n",
        "    \n",
        "    configs = {\n",
        "        \"faq\": {\"chunk_size\": 150, \"chunk_overlap\": 20},\n",
        "        \"tecnico\": {\"chunk_size\": 400, \"chunk_overlap\": 80}\n",
        "    }\n",
        "    \n",
        "    # TODO: Implemente a l√≥gica aqui\n",
        "    # 1. Pegar a config do tipo de documento\n",
        "    # 2. Criar o splitter\n",
        "    # 3. Processar o documento\n",
        "    # 4. Retornar os chunks\n",
        "    \n",
        "    config = configs.get(tipo_doc, {\"chunk_size\": 300, \"chunk_overlap\": 50})\n",
        "    \n",
        "    # Seu c√≥digo aqui...\n",
        "    \n",
        "    return chunks  # Remova esta linha e implemente!\n",
        "\n",
        "# Teste sua fun√ß√£o\n",
        "# chunks_faq = encontrar_melhor_config(doc_faq, \"faq\")\n",
        "# chunks_tecnico = encontrar_melhor_config(doc_tecnico, \"tecnico\")\n",
        "\n",
        "print(\"üí° Dica: Pense em como cada tipo de documento deve ser dividido!\")\n",
        "print(\"FAQ: peda√ßos pequenos, uma pergunta por chunk\")\n",
        "print(\"T√©cnico: peda√ßos maiores, mantendo contexto completo\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exerc√≠cio 2: An√°lise de Qualidade dos Chunks\n\nCrie uma fun√ß√£o que analise a qualidade dos chunks gerados e sugira melhorias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EXERC√çCIO 2: Complete a fun√ß√£o de an√°lise\n",
        "\n",
        "def analisar_qualidade_chunks(chunks):\n",
        "    \"\"\"Analisa a qualidade dos chunks e sugere melhorias\"\"\"\n",
        "    \n",
        "    # TODO: Implemente as an√°lises abaixo\n",
        "    \n",
        "    # 1. Calcular estat√≠sticas b√°sicas\n",
        "    tamanhos = [len(chunk.page_content) for chunk in chunks]\n",
        "    \n",
        "    # 2. Verificar chunks muito pequenos (< 50 chars)\n",
        "    chunks_pequenos = 0  # TODO: contar chunks pequenos\n",
        "    \n",
        "    # 3. Verificar chunks muito grandes (> 1000 chars)  \n",
        "    chunks_grandes = 0   # TODO: contar chunks grandes\n",
        "    \n",
        "    # 4. Verificar chunks que terminam no meio de palavra\n",
        "    chunks_mal_cortados = 0  # TODO: contar chunks mal cortados\n",
        "    \n",
        "    # Seu c√≥digo aqui...\n",
        "    \n",
        "    # Relat√≥rio\n",
        "    print(\"üìä An√°lise de Qualidade dos Chunks:\")\n",
        "    print(f\"Total de chunks: {len(chunks)}\")\n",
        "    print(f\"Tamanho m√©dio: {np.mean(tamanhos):.1f} caracteres\")\n",
        "    print(f\"Chunks muito pequenos: {chunks_pequenos}\")\n",
        "    print(f\"Chunks muito grandes: {chunks_grandes}\")\n",
        "    print(f\"Chunks mal cortados: {chunks_mal_cortados}\")\n",
        "    \n",
        "    # TODO: Adicionar sugest√µes de melhoria\n",
        "    print(\"\\nüí° Sugest√µes:\")\n",
        "    if chunks_pequenos > 0:\n",
        "        print(f\"- Considere aumentar chunk_size (muitos chunks pequenos)\")\n",
        "    # Adicione mais sugest√µes...\n",
        "\n",
        "# Teste com os chunks que criamos\n",
        "print(\"An√°lise dos chunks do blog:\")\n",
        "analisar_qualidade_chunks(chunks_blog)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Preparando para os Pr√≥ximos M√≥dulos\n\nLiiindo! Voc√™ chegou at√© aqui e j√° sabe como:\n- Carregar documentos de diferentes fontes\n- Dividir textos de forma inteligente\n- Configurar splitters para diferentes casos de uso\n- Manter metadados importantes\n\n### O que vem pela frente?\n\n**M√≥dulo 9 - Vector Store e Embeddings**:\n- Vamos transformar nossos chunks em embeddings (vetores)\n- Aprender sobre diferentes vector stores\n- Implementar busca sem√¢ntica\n\n**M√≥dulo 10 - RAG Implementation**:\n- Juntar tudo: chunks + embeddings + LLM\n- Criar um sistema de pergunta e resposta\n- Otimizar a recupera√ß√£o de informa√ß√µes\n\n![](https://s3.us-east-1.amazonaws.com/turing.education/books/imagens/langchain-modulo-08_img_04.png)\n\n**Dica!** Guarde bem os conceitos deste m√≥dulo! Eles s√£o a base de tudo que vem pela frente no RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Resumo do M√≥dulo\n\nCara, que jornada incr√≠vel! Vamos recapitular o que aprendemos:\n\n### üîë Conceitos Principais\n\n1. **Document Loaders**: Carregam documentos de qualquer formato\n2. **Text Splitters**: Dividem textos mantendo contexto\n3. **Chunks**: Peda√ßos pequenos e contextualizados de texto\n4. **Metadados**: Informa√ß√µes sobre os documentos\n\n### üõ†Ô∏è Ferramentas que Dominamos\n\n- `TextLoader`: Para arquivos de texto\n- `WebBaseLoader`: Para p√°ginas web\n- `CharacterTextSplitter`: Divis√£o simples\n- `RecursiveCharacterTextSplitter`: Divis√£o inteligente ‚≠ê\n\n### üìè Configura√ß√µes Importantes\n\n- **chunk_size**: 300-500 para a maioria dos casos\n- **chunk_overlap**: 10-20% do chunk_size\n- **separators**: Use a hierarquia padr√£o\n\n### üöÄ Pr√≥ximos Passos\n\nAgora que nossos dados est√£o prontos, vamos transform√°-los em embeddings e implementar RAG!\n\n**Exerc√≠cio para casa**: Pense em um documento que voc√™ gostaria de processar (PDF, site, arquivo). No pr√≥ximo m√≥dulo vamos transformar ele em um sistema de busca inteligente!\n\nNos vemos no M√≥dulo 9! Bora para Vector Stores! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C√≥digo final - Limpeza dos arquivos criados\n",
        "import os\n",
        "\n",
        "# Removendo arquivo tempor√°rio\n",
        "if os.path.exists(\"exemplo.txt\"):\n",
        "    os.remove(\"exemplo.txt\")\n",
        "    print(\"‚úÖ Arquivo tempor√°rio removido\")\n",
        "\n",
        "print(\"\\nüéØ M√≥dulo 8 completo!\")\n",
        "print(\"üìö Voc√™ agora sabe preparar dados para RAG\")\n",
        "print(\"üöÄ Pr√≥ximo m√≥dulo: Vector Store e Embeddings\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Pedro Guth - LangChain v0.3 Course\")\n",
        "print(\"M√≥dulo 8/17: Document Loading e Splitters\")\n",
        "print(\"=\"*50)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
