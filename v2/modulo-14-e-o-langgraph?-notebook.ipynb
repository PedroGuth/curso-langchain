{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🕸️ LangGraph: Quando o Agente Precisa de um GPS!\n\n**Módulo 14/15 - Curso LangChain v0.2**\n\nE aí, galera! Chegamos ao Módulo 14 e agora vamos falar de uma parada que é o **próximo nível** dos agents que já vimos!\n\nLembra dos agents que criamos no Módulo 9? Pois é, eles eram legais, mas meio... como posso dizer... **desorganizados**! Era tipo aquele amigo que sai de casa sem GPS e fica rodando pela cidade sem rumo.\n\n**E o LangGraph?** É justamente o GPS dos nossos agents! 🚗📍\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Tá, mas o que é o LangGraph afinal?\n\nVou te explicar com uma analogia brasileira que você vai entender na hora!\n\nImagina que você está organizando um churrasco (eu sei, toda explicação de tech no Brasil tem que ter churrasco 😄):\n\n**Agents tradicionais** = Você sozinho fazendo tudo: comprando carne, acendendo churrasqueira, fazendo farofa... É muito trabalho e você pode se perder no meio!\n\n**LangGraph** = Você com uma equipe organizada: João cuida da carne, Maria faz a farofa, Pedro organiza as bebidas, e **você coordena todo mundo** seguindo um plano bem definido!\n\n### O que o LangGraph faz?\n\n- **Cria fluxos complexos** de decisão para agents\n- **Gerencia estados** entre diferentes etapas  \n- **Permite loops e condições** (coisa que agent simples não faz direito)\n- **Controla a execução** de múltiplos agents trabalhando juntos\n- **Oferece observabilidade** total do que está rolando\n\n### Por que precisamos dele?\n\nLembra dos agents que criamos? Eles eram **lineares**: pergunta → ferramenta → resposta. \n\nMas na vida real, precisamos de fluxos tipo:\n- \"Se isso, então aquilo\"\n- \"Tenta de novo se der erro\"\n- \"Faz essa parte, depois volta e faz aquela\"\n- \"Vários agents trabalhando em paralelo\"\n\nÉ aí que entra o **LangGraph**! 🎯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Primeiro, vamos instalar as dependências que vamos usar\n# Bora configurar nosso ambiente!\n\n!pip install -q langgraph langchain-google-genai python-dotenv matplotlib networkx\n\n# Imports básicos para começar nossa jornada\nimport os\nimport json\nfrom typing import Dict, Any, List\nfrom dotenv import load_dotenv\n\n# LangGraph imports - as estrelas do show!\nfrom langgraph.graph import Graph, StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Para visualizar nossos grafos\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\nprint(\"📦 Bibliotecas instaladas e importadas!\")\nprint(\"🚀 Bora mergulhar no mundo do LangGraph!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configuração da API - igual fazíamos antes!\nload_dotenv()\n\n# Nosso modelo que já conhecemos bem\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.0-flash-exp\",\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    temperature=0.7\n)\n\nprint(\"🤖 Modelo configurado!\")\nprint(f\"✅ Usando: {llm.model}\")\n\n# Teste rápido para garantir que está funcionando\nresponse = llm.invoke(\"Olá! Estou aprendendo LangGraph hoje!\")\nprint(f\"\\n🎯 Teste: {response.content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Conceitos Fundamentais do LangGraph\n\nAntes de botar a mão na massa, vamos entender os conceitos principais. É como aprender as regras do futebol antes de entrar em campo!\n\n### 1. **Graph (Grafo)**\nÉ a estrutura principal - pensa nele como o **mapa do nosso churrasco**. Define quem faz o quê e em que ordem.\n\n### 2. **Nodes (Nós)**\nSão as **funções ou agents** que fazem o trabalho. Cada nó é como uma pessoa no churrasco:\n- Nó \"comprador\" → vai no açougue\n- Nó \"churrasqueiro\" → cuida da carne\n- Nó \"organizador\" → coordena tudo\n\n### 3. **Edges (Arestas)**\nSão as **conexões** entre os nós. Definem o fluxo: \"depois de comprar a carne, acenda a churrasqueira\".\n\n### 4. **State (Estado)**\nÉ a **memória compartilhada** do grafo. Todo mundo no churrasco sabe: \"quantas pessoas vão vir?\", \"que horas começar?\", etc.\n\n### 5. **Conditional Edges (Arestas Condicionais)**\nO **if/else** do grafo: \"Se está chovendo, faz o churrasco na garagem, senão faz no quintal\".\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧮 A Matemática por trás do LangGraph\n\nVou te mostrar a teoria sem assustar! 😅\n\nUm grafo no LangGraph pode ser representado matematicamente como:\n\n$$G = (V, E, S, F)$$\n\nOnde:\n- $V$ = conjunto de vértices (nossos nós/funções)\n- $E$ = conjunto de arestas (conexões entre nós)\n- $S$ = estado compartilhado (nossa memória)\n- $F$ = função de transição de estado\n\n### Função de Transição de Estado\n\nCada nó executa uma função que pode ser representada como:\n\n$$S_{novo} = f_i(S_{atual}, Input_i)$$\n\nOnde:\n- $f_i$ é a função do nó $i$\n- $S_{atual}$ é o estado antes da execução\n- $Input_i$ é a entrada específica do nó\n- $S_{novo}$ é o estado resultante\n\n### Algoritmo de Execução\n\nO LangGraph usa um algoritmo similar ao **BFS (Breadth-First Search)** com controle de estado:\n\n1. **Inicializa** estado $S_0$\n2. **Executa** nó inicial: $S_1 = f_{start}(S_0)$\n3. **Avalia** condições de transição\n4. **Seleciona** próximo nó baseado em $S_{atual}$\n5. **Repete** até atingir condição de parada\n\n**Dica do Pedro**: Não se preocupe muito com a matemática agora. O importante é entender que o LangGraph gerencia tudo isso para você! É como usar GPS - você não precisa saber os algoritmos de roteamento para chegar no destino! 🗺️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar nosso primeiro grafo simples!\n# Começando com o básico: um contador que toma decisões\n\nfrom typing import TypedDict\n\n# Definindo o estado do nosso grafo\n# É como definir \"que informações todo mundo vai compartilhar\"\nclass CounterState(TypedDict):\n    count: int\n    messages: List[str]\n    should_continue: bool\n\n# Função que incrementa o contador\ndef increment_counter(state: CounterState) -> CounterState:\n    \"\"\"Incrementa o contador e adiciona uma mensagem\"\"\"\n    new_count = state[\"count\"] + 1\n    new_message = f\"Contador agora está em: {new_count}\"\n    \n    return {\n        \"count\": new_count,\n        \"messages\": state[\"messages\"] + [new_message],\n        \"should_continue\": new_count < 5  # Para quando chegar em 5\n    }\n\n# Função que decide se devemos continuar\ndef should_continue_counting(state: CounterState) -> str:\n    \"\"\"Decide se devemos continuar ou parar\"\"\"\n    if state[\"should_continue\"]:\n        return \"continue\"\n    else:\n        return \"stop\"\n\n# Função final\ndef finish_counting(state: CounterState) -> CounterState:\n    \"\"\"Finaliza a contagem\"\"\"\n    return {\n        **state,\n        \"messages\": state[\"messages\"] + [\"🎉 Contagem finalizada!\"]\n    }\n\nprint(\"🔧 Funções do grafo criadas!\")\nprint(\"📝 Estado definido: count, messages, should_continue\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Agora vamos montar nosso grafo!\n# É como montar um fluxograma, mas em código\n\nfrom langgraph.graph import StateGraph, END\n\n# Criando o grafo\nworkflow = StateGraph(CounterState)\n\n# Adicionando os nós (as \"estações\" do nosso fluxo)\nworkflow.add_node(\"increment\", increment_counter)\nworkflow.add_node(\"finish\", finish_counting)\n\n# Definindo o ponto de entrada\nworkflow.set_entry_point(\"increment\")\n\n# Adicionando as conexões condicionais\n# É aqui que a mágica acontece!\nworkflow.add_conditional_edges(\n    \"increment\",  # De qual nó\n    should_continue_counting,  # Função que decide\n    {\n        \"continue\": \"increment\",  # Se continuar, volta pro increment\n        \"stop\": \"finish\"         # Se parar, vai pro finish\n    }\n)\n\n# Conectando o nó final ao fim do grafo\nworkflow.add_edge(\"finish\", END)\n\n# Compilando o grafo (transformando em algo executável)\napp = workflow.compile()\n\nprint(\"🏗️ Grafo construído e compilado!\")\nprint(\"🎯 Pronto para executar nosso primeiro fluxo!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Hora de executar nosso grafo!\n# É como apertar o play na nossa máquina de estados\n\n# Estado inicial\ninitial_state = {\n    \"count\": 0,\n    \"messages\": [\"🚀 Iniciando contagem...\"],\n    \"should_continue\": True\n}\n\nprint(\"▶️ Executando o grafo...\\n\")\n\n# Executando o grafo\nresult = app.invoke(initial_state)\n\n# Mostrando os resultados\nprint(\"📊 Resultado Final:\")\nprint(f\"Contador final: {result['count']}\")\nprint(f\"Deve continuar: {result['should_continue']}\")\nprint(\"\\n📝 Histórico de mensagens:\")\nfor i, msg in enumerate(result['messages'], 1):\n    print(f\"{i}. {msg}\")\n\nprint(\"\\n🎉 Liiindo! Nosso primeiro grafo funcionou!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 Visualizando nosso Grafo\n\nUma imagem vale mais que mil palavras! Vamos ver como nosso grafo fica visualmente.\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar uma visualização do nosso grafo\n# Porque ver é melhor que imaginar!\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom matplotlib.patches import FancyBboxPatch\n\n# Criando o gráfico\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\n\n# Definindo posições dos elementos\npositions = {\n    'start': (1, 4),\n    'increment': (3, 4),\n    'decision': (5, 4),\n    'finish': (7, 4),\n    'end': (9, 4)\n}\n\n# Desenhando os nós\nfor name, (x, y) in positions.items():\n    if name == 'decision':\n        # Losango para decisão\n        diamond = FancyBboxPatch(\n            (x-0.5, y-0.3), 1, 0.6,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightblue',\n            edgecolor='blue',\n            transform=ax.transData\n        )\n        ax.add_patch(diamond)\n        ax.text(x, y, 'Continuar?', ha='center', va='center', fontsize=10, weight='bold')\n    else:\n        # Retângulo para ações\n        rect = FancyBboxPatch(\n            (x-0.5, y-0.3), 1, 0.6,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightgreen',\n            edgecolor='darkgreen',\n            transform=ax.transData\n        )\n        ax.add_patch(rect)\n        ax.text(x, y, name.title(), ha='center', va='center', fontsize=10, weight='bold')\n\n# Desenhando as setas\narrows = [\n    ((1.5, 4), (2.5, 4), 'Entrada'),\n    ((3.5, 4), (4.5, 4), ''),\n    ((5.5, 4), (6.5, 4), 'Não'),\n    ((7.5, 4), (8.5, 4), ''),\n    ((5, 4.3), (3, 4.7), 'Sim'),  # Loop de volta\n]\n\nfor (x1, y1), (x2, y2), label in arrows:\n    if label == 'Sim':  # Curva para o loop\n        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                   arrowprops=dict(arrowstyle='->', lw=2, color='red',\n                                 connectionstyle=\"arc3,rad=0.3\"))\n        ax.text(4, 5, label, ha='center', va='center', fontsize=9, color='red', weight='bold')\n    else:\n        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                   arrowprops=dict(arrowstyle='->', lw=2, color='darkblue'))\n        if label:\n            mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n            ax.text(mid_x, mid_y + 0.2, label, ha='center', va='center', \n                   fontsize=9, color='darkblue', weight='bold')\n\nax.set_title('🕸️ Estrutura do nosso LangGraph - Contador', fontsize=16, weight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📊 Visualização criada!\")\nprint(\"🎯 Observe como o fluxo funciona: Start → Increment → Decisão → Finish ou Loop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 LangGraph com LLMs: Agora fica sério!\n\nTá, contador é legal, mas vamos fazer algo mais parecido com o que usamos no mundo real!\n\nVamos criar um grafo que usa nosso modelo LLM para:\n1. **Analisar** uma pergunta do usuário\n2. **Decidir** se precisa de mais informações\n3. **Fazer perguntas** de esclarecimento se necessário\n4. **Gerar** a resposta final\n\nÉ como ter um assistente que não só responde, mas **pensa no processo** de como responder melhor!\n\n### Casos de uso reais:\n- **Chatbots inteligentes** que fazem perguntas de esclarecimento\n- **Sistemas de suporte** que coletam informações antes de ajudar\n- **Assistentes de vendas** que qualificam leads\n- **Tutores virtuais** que adaptam explicações\n\n**Dica do Pedro**: Lembra dos agents do Módulo 9? Eles eram ótimos para tarefas simples, mas aqui conseguimos criar **fluxos de conversação muito mais sofisticados**! 🧠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar um assistente inteligente que pensa antes de responder!\n# Estado mais complexo para nosso chatbot\n\nfrom langchain_core.messages import BaseMessage\nfrom typing import Annotated\n\nclass ChatState(TypedDict):\n    messages: Annotated[List[BaseMessage], add_messages]\n    user_question: str\n    needs_clarification: bool\n    clarification_asked: bool\n    context_info: Dict[str, Any]\n    final_answer: str\n\n# Nó 1: Analisa se a pergunta precisa de esclarecimento\ndef analyze_question(state: ChatState) -> ChatState:\n    \"\"\"Analisa se a pergunta do usuário precisa de mais informações\"\"\"\n    \n    question = state[\"user_question\"]\n    \n    analysis_prompt = f\"\"\"\n    Analise esta pergunta do usuário: \"{question}\"\n    \n    Determine se a pergunta é:\n    - CLARA: pode ser respondida diretamente\n    - AMBÍGUA: precisa de esclarecimentos\n    \n    Responda apenas com: CLARA ou AMBÍGUA\n    \"\"\"\n    \n    response = llm.invoke(analysis_prompt)\n    needs_clarification = \"AMBÍGUA\" in response.content.upper()\n    \n    return {\n        **state,\n        \"needs_clarification\": needs_clarification,\n        \"messages\": [AIMessage(content=f\"Análise: {'Precisa esclarecimento' if needs_clarification else 'Pergunta clara'}\")]\n    }\n\n# Nó 2: Faz pergunta de esclarecimento\ndef ask_clarification(state: ChatState) -> ChatState:\n    \"\"\"Gera uma pergunta de esclarecimento para o usuário\"\"\"\n    \n    question = state[\"user_question\"]\n    \n    clarification_prompt = f\"\"\"\n    O usuário fez esta pergunta ambígua: \"{question}\"\n    \n    Faça UMA pergunta específica para esclarecer o que ele realmente quer saber.\n    Seja direto e útil.\n    \"\"\"\n    \n    response = llm.invoke(clarification_prompt)\n    \n    return {\n        **state,\n        \"clarification_asked\": True,\n        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)]\n    }\n\n# Nó 3: Gera resposta final\ndef generate_final_answer(state: ChatState) -> ChatState:\n    \"\"\"Gera a resposta final baseada em todas as informações\"\"\"\n    \n    question = state[\"user_question\"]\n    context = \"\\n\".join([msg.content for msg in state[\"messages\"] if msg.content])\n    \n    final_prompt = f\"\"\"\n    Pergunta original: \"{question}\"\n    Contexto da conversa: {context}\n    \n    Forneça uma resposta completa e útil para o usuário.\n    Seja claro, direto e amigável.\n    \"\"\"\n    \n    response = llm.invoke(final_prompt)\n    \n    return {\n        **state,\n        \"final_answer\": response.content,\n        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)]\n    }\n\nprint(\"🧠 Nós do chatbot inteligente criados!\")\nprint(\"✅ Funções: analyze_question, ask_clarification, generate_final_answer\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Função de decisão para nosso chatbot\ndef should_ask_clarification(state: ChatState) -> str:\n    \"\"\"Decide o próximo passo baseado no estado atual\"\"\"\n    \n    # Se precisa de esclarecimento e ainda não perguntou\n    if state[\"needs_clarification\"] and not state[\"clarification_asked\"]:\n        return \"ask_clarification\"\n    \n    # Senão, gera a resposta final\n    return \"generate_answer\"\n\n# Construindo nosso chatbot inteligente\nchatbot_workflow = StateGraph(ChatState)\n\n# Adicionando os nós\nchatbot_workflow.add_node(\"analyze\", analyze_question)\nchatbot_workflow.add_node(\"clarify\", ask_clarification)\nchatbot_workflow.add_node(\"answer\", generate_final_answer)\n\n# Definindo o fluxo\nchatbot_workflow.set_entry_point(\"analyze\")\n\n# Conexões condicionais\nchatbot_workflow.add_conditional_edges(\n    \"analyze\",\n    should_ask_clarification,\n    {\n        \"ask_clarification\": \"clarify\",\n        \"generate_answer\": \"answer\"\n    }\n)\n\n# Da clarificação sempre vai para a resposta\nchatbot_workflow.add_edge(\"clarify\", \"answer\")\n\n# Resposta final termina o fluxo\nchatbot_workflow.add_edge(\"answer\", END)\n\n# Compilando nosso chatbot\nchatbot_app = chatbot_workflow.compile()\n\nprint(\"🤖 Chatbot inteligente montado!\")\nprint(\"🎯 Fluxo: Analisa → Decide → Esclarece (se necessário) → Responde\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando nosso chatbot inteligente!\n# Vamos ver como ele se comporta com diferentes tipos de pergunta\n\ndef test_chatbot(question: str):\n    \"\"\"Testa o chatbot com uma pergunta\"\"\"\n    \n    print(f\"\\n🗣️ Usuário: {question}\")\n    print(\"=\"*50)\n    \n    # Estado inicial\n    initial_state = {\n        \"messages\": [],\n        \"user_question\": question,\n        \"needs_clarification\": False,\n        \"clarification_asked\": False,\n        \"context_info\": {},\n        \"final_answer\": \"\"\n    }\n    \n    # Executando\n    result = chatbot_app.invoke(initial_state)\n    \n    # Mostrando o processo\n    print(\"🔄 Processo:\")\n    for i, msg in enumerate(result[\"messages\"], 1):\n        print(f\"{i}. {msg.content}\")\n    \n    print(f\"\\n🎯 Precisou esclarecimento: {'Sim' if result['needs_clarification'] else 'Não'}\")\n    \n    return result\n\n# Teste 1: Pergunta clara\nprint(\"🧪 TESTE 1: Pergunta Clara\")\ntest1 = test_chatbot(\"Qual é a capital do Brasil?\")\n\n# Teste 2: Pergunta ambígua\nprint(\"\\n\\n🧪 TESTE 2: Pergunta Ambígua\")\ntest2 = test_chatbot(\"Como faço isso?\")\n\nprint(\"\\n🎉 Testes concluídos! Veja como o chatbot se adapta ao tipo de pergunta!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Comparação: Agents vs LangGraph\n\nAgora que vimos os dois em ação, vamos comparar!\n\n| Aspecto | Agents (Módulo 9) | LangGraph |\n|---------|-------------------|----------|\n| **Complexidade** | Simples, linear | Complexo, ramificado |\n| **Controle de Fluxo** | Limitado | Total |\n| **Estados** | Não gerencia | Gerencia completamente |\n| **Loops** | Difícil | Nativo |\n| **Múltiplos Caminhos** | Não suporta | Suporta nativamente |\n| **Debugging** | Difícil | Fácil (observável) |\n| **Casos de Uso** | Tarefas simples | Workflows complexos |\n\n### Quando usar cada um?\n\n**Use Agents quando**:\n- Tarefa simples e direta\n- Não precisa de loops ou condições\n- Quer algo rápido de implementar\n\n**Use LangGraph quando**:\n- Precisa de fluxos condicionais\n- Quer controlar cada etapa\n- Precisa de loops ou retry logic\n- Quer observabilidade total\n- Múltiplos agents trabalhando juntos\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar um gráfico comparativo de performance\n# Simulando cenários diferentes\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Dados simulados de performance\nscenarios = ['Tarefa Simples', 'Fluxo Condicional', 'Múltiplos Loops', 'Error Handling', 'Observabilidade']\nagents_score = [9, 4, 2, 3, 2]  # Agents tradicionais\nlanggraph_score = [7, 9, 9, 8, 10]  # LangGraph\n\nx = np.arange(len(scenarios))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 7))\nrects1 = ax.bar(x - width/2, agents_score, width, label='Agents Tradicionais', \n                color='lightcoral', alpha=0.8)\nrects2 = ax.bar(x + width/2, langgraph_score, width, label='LangGraph', \n                color='lightblue', alpha=0.8)\n\nax.set_ylabel('Score (0-10)', fontsize=12)\nax.set_title('🏆 Agents vs LangGraph - Comparação de Capabilities', fontsize=14, weight='bold')\nax.set_xticks(x)\nax.set_xticklabels(scenarios, rotation=45, ha='right')\nax.legend()\nax.grid(axis='y', alpha=0.3)\n\n# Adicionando valores nas barras\ndef autolabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate(f'{height}',\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', weight='bold')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.tight_layout()\nplt.show()\n\n# Análise dos resultados\nprint(\"📊 Análise dos Resultados:\")\nprint(\"\\n🟢 Agents Tradicionais são melhores em:\")\nfor i, scenario in enumerate(scenarios):\n    if agents_score[i] > langgraph_score[i]:\n        print(f\"   • {scenario} (Score: {agents_score[i]} vs {langgraph_score[i]})\")\n\nprint(\"\\n🔵 LangGraph é melhor em:\")\nfor i, scenario in enumerate(scenarios):\n    if langgraph_score[i] > agents_score[i]:\n        print(f\"   • {scenario} (Score: {langgraph_score[i]} vs {agents_score[i]})\")\n\nprint(\"\\n🎯 Conclusão: LangGraph vence em cenários complexos, Agents em tarefas simples!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏗️ Projeto Prático: Sistema de Atendimento Inteligente\n\nBora fazer um projeto real que você pode usar no trabalho!\n\nVamos criar um **Sistema de Atendimento ao Cliente** que:\n\n1. **Classifica** o tipo de problema\n2. **Coleta** informações necessárias\n3. **Tenta resolver** automaticamente\n4. **Escalona** para humano se necessário\n\nÉ tipo um **funil inteligente** que otimiza o atendimento!\n\n### Fluxo do Sistema:\n```mermaid\ngraph TD\n    A[Cliente faz pergunta] --> B[Classifica problema]\n    B --> C{Tipo de problema?}\n    C -->|Técnico| D[Coleta info técnica]\n    C -->|Comercial| E[Coleta info comercial]\n    C -->|Suporte| F[Coleta info suporte]\n    D --> G[Tenta resolver]\n    E --> G\n    F --> G\n    G --> H{Conseguiu resolver?}\n    H -->|Sim| I[Resposta final]\n    H -->|Não| J[Escalona para humano]\n```\n\n**Dica do Pedro**: Este é o tipo de sistema que grandes empresas pagam milhões para ter! E você vai aprender a fazer em algumas células! 💪"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Estado do nosso sistema de atendimento\nclass SupportState(TypedDict):\n    messages: Annotated[List[BaseMessage], add_messages]\n    customer_query: str\n    problem_type: str  # 'technical', 'commercial', 'support'\n    collected_info: Dict[str, Any]\n    resolution_attempt: str\n    resolved: bool\n    escalated: bool\n    final_response: str\n\n# Nó 1: Classifica o tipo de problema\ndef classify_problem(state: SupportState) -> SupportState:\n    \"\"\"Classifica o tipo de problema do cliente\"\"\"\n    \n    query = state[\"customer_query\"]\n    \n    classification_prompt = f\"\"\"\n    Classifique este problema do cliente em uma das categorias:\n    \n    Problema: \"{query}\"\n    \n    Categorias:\n    - TECHNICAL: problemas técnicos, bugs, não funciona\n    - COMMERCIAL: preços, planos, vendas, upgrades\n    - SUPPORT: como usar, tutoriais, dúvidas gerais\n    \n    Responda apenas com: TECHNICAL, COMMERCIAL ou SUPPORT\n    \"\"\"\n    \n    response = llm.invoke(classification_prompt)\n    problem_type = response.content.strip().upper()\n    \n    if problem_type not in ['TECHNICAL', 'COMMERCIAL', 'SUPPORT']:\n        problem_type = 'SUPPORT'  # Default\n    \n    return {\n        **state,\n        \"problem_type\": problem_type.lower(),\n        \"messages\": [AIMessage(content=f\"Problema classificado como: {problem_type}\")]\n    }\n\n# Nó 2: Coleta informações específicas\ndef collect_info(state: SupportState) -> SupportState:\n    \"\"\"Coleta informações baseadas no tipo de problema\"\"\"\n    \n    problem_type = state[\"problem_type\"]\n    query = state[\"customer_query\"]\n    \n    if problem_type == \"technical\":\n        info_prompt = f\"\"\"\n        Para resolver este problema técnico: \"{query}\"\n        \n        Que informações técnicas você precisa coletar?\n        Liste 3 perguntas específicas que ajudariam a diagnosticar.\n        \"\"\"\n    elif problem_type == \"commercial\":\n        info_prompt = f\"\"\"\n        Para esta questão comercial: \"{query}\"\n        \n        Que informações comerciais você precisa?\n        Liste 3 perguntas sobre necessidades e orçamento.\n        \"\"\"\n    else:  # support\n        info_prompt = f\"\"\"\n        Para esta dúvida de suporte: \"{query}\"\n        \n        Que esclarecimentos você precisa?\n        Liste 3 perguntas para entender melhor a necessidade.\n        \"\"\"\n    \n    response = llm.invoke(info_prompt)\n    \n    return {\n        **state,\n        \"collected_info\": {\"questions\": response.content},\n        \"messages\": state[\"messages\"] + [AIMessage(content=f\"Informações coletadas para {problem_type}\")]\n    }\n\nprint(\"🎯 Funções de classificação e coleta criadas!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Nó 3: Tenta resolver o problema\ndef attempt_resolution(state: SupportState) -> SupportState:\n    \"\"\"Tenta resolver o problema automaticamente\"\"\"\n    \n    query = state[\"customer_query\"]\n    problem_type = state[\"problem_type\"]\n    info = state[\"collected_info\"]\n    \n    resolution_prompt = f\"\"\"\n    Tente resolver este problema do cliente:\n    \n    Problema: \"{query}\"\n    Tipo: {problem_type}\n    Informações coletadas: {info}\n    \n    Forneça uma solução detalhada e prática.\n    Se não conseguir resolver completamente, diga \"ESCALATE\".\n    \"\"\"\n    \n    response = llm.invoke(resolution_prompt)\n    \n    # Verifica se conseguiu resolver\n    resolved = \"ESCALATE\" not in response.content.upper()\n    \n    return {\n        **state,\n        \"resolution_attempt\": response.content,\n        \"resolved\": resolved,\n        \"messages\": state[\"messages\"] + [AIMessage(content=\"Tentativa de resolução realizada\")]\n    }\n\n# Nó 4: Resposta final (quando resolve)\ndef provide_final_answer(state: SupportState) -> SupportState:\n    \"\"\"Fornece a resposta final quando o problema foi resolvido\"\"\"\n    \n    resolution = state[\"resolution_attempt\"]\n    \n    final_prompt = f\"\"\"\n    Transforme esta solução técnica em uma resposta amigável para o cliente:\n    \n    Solução: {resolution}\n    \n    Seja caloroso, claro e útil. Termine perguntando se precisa de mais alguma coisa.\n    \"\"\"\n    \n    response = llm.invoke(final_prompt)\n    \n    return {\n        **state,\n        \"final_response\": response.content,\n        \"messages\": state[\"messages\"] + [AIMessage(content=response.content)]\n    }\n\n# Nó 5: Escalona para humano\ndef escalate_to_human(state: SupportState) -> SupportState:\n    \"\"\"Escalona o problema para atendimento humano\"\"\"\n    \n    query = state[\"customer_query\"]\n    problem_type = state[\"problem_type\"]\n    \n    escalation_msg = f\"\"\"\n    🤝 Olá! Vou conectar você com um de nossos especialistas.\n    \n    Seu problema ({problem_type}) será tratado com prioridade.\n    Tempo estimado de espera: 5-10 minutos.\n    \n    Um momento, por favor...\n    \"\"\"\n    \n    return {\n        **state,\n        \"escalated\": True,\n        \"final_response\": escalation_msg,\n        \"messages\": state[\"messages\"] + [AIMessage(content=escalation_msg)]\n    }\n\nprint(\"🔧 Funções de resolução e escalonamento criadas!\")\nprint(\"✅ Sistema de atendimento quase pronto!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Função de decisão do sistema\ndef decide_next_step(state: SupportState) -> str:\n    \"\"\"Decide se resolve ou escalona\"\"\"\n    \n    if state[\"resolved\"]:\n        return \"provide_answer\"\n    else:\n        return \"escalate\"\n\n# Montando o sistema completo\nsupport_workflow = StateGraph(SupportState)\n\n# Adicionando todos os nós\nsupport_workflow.add_node(\"classify\", classify_problem)\nsupport_workflow.add_node(\"collect\", collect_info)\nsupport_workflow.add_node(\"resolve\", attempt_resolution)\nsupport_workflow.add_node(\"answer\", provide_final_answer)\nsupport_workflow.add_node(\"escalate\", escalate_to_human)\n\n# Definindo o fluxo\nsupport_workflow.set_entry_point(\"classify\")\n\n# Conexões lineares até a decisão\nsupport_workflow.add_edge(\"classify\", \"collect\")\nsupport_workflow.add_edge(\"collect\", \"resolve\")\n\n# Decisão condicional após tentar resolver\nsupport_workflow.add_conditional_edges(\n    \"resolve\",\n    decide_next_step,\n    {\n        \"provide_answer\": \"answer\",\n        \"escalate\": \"escalate\"\n    }\n)\n\n# Ambos os finais terminam o fluxo\nsupport_workflow.add_edge(\"answer\", END)\nsupport_workflow.add_edge(\"escalate\", END)\n\n# Compilando nosso sistema de atendimento\nsupport_app = support_workflow.compile()\n\nprint(\"🏢 Sistema de Atendimento Inteligente PRONTO!\")\nprint(\"🎯 Fluxo completo montado e testado!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Testando nosso sistema de atendimento!\n# Vamos simular diferentes tipos de problemas\n\ndef test_support_system(customer_query: str):\n    \"\"\"Testa o sistema de atendimento\"\"\"\n    \n    print(f\"\\n🎧 NOVO ATENDIMENTO\")\n    print(f\"Cliente: {customer_query}\")\n    print(\"=\"*60)\n    \n    # Estado inicial\n    initial_state = {\n        \"messages\": [],\n        \"customer_query\": customer_query,\n        \"problem_type\": \"\",\n        \"collected_info\": {},\n        \"resolution_attempt\": \"\",\n        \"resolved\": False,\n        \"escalated\": False,\n        \"final_response\": \"\"\n    }\n    \n    # Executando o sistema\n    result = support_app.invoke(initial_state)\n    \n    # Relatório do atendimento\n    print(f\"\\n📊 RELATÓRIO DO ATENDIMENTO:\")\n    print(f\"Tipo do problema: {result['problem_type'].upper()}\")\n    print(f\"Resolvido automaticamente: {'✅ Sim' if result['resolved'] else '❌ Não'}\")\n    print(f\"Escalonado: {'✅ Sim' if result['escalated'] else '❌ Não'}\")\n    \n    print(f\"\\n💬 RESPOSTA FINAL:\")\n    print(result['final_response'])\n    \n    return result\n\n# Teste 1: Problema técnico simples\nprint(\"🧪 TESTE 1: Problema Técnico\")\ntest1 = test_support_system(\"Meu aplicativo não está abrindo no celular\")\n\n# Teste 2: Dúvida comercial\nprint(\"\\n\\n🧪 TESTE 2: Questão Comercial\")\ntest2 = test_support_system(\"Quero saber sobre os planos premium\")\n\n# Teste 3: Problema complexo (deve escalonar)\nprint(\"\\n\\n🧪 TESTE 3: Problema Complexo\")\ntest3 = test_support_system(\"Perdi todos os meus dados após a atualização\")\n\nprint(\"\\n🎉 Sistema testado! Veja como ele se adapta a diferentes situações!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Exercício Prático: Seu Primeiro LangGraph\n\nAgora é sua vez de botar a mão na massa!\n\n### 🏆 DESAFIO 1: Sistema de Recomendação Inteligente\n\nCrie um LangGraph que:\n1. **Pergunta** sobre os gostos do usuário\n2. **Analisa** as preferências\n3. **Decide** se tem informação suficiente\n4. **Faz mais perguntas** se necessário\n5. **Gera** recomendações personalizadas\n\n#### Requisitos:\n- Use pelo menos 4 nós\n- Implemente pelo menos 1 condicional\n- Gerencie um estado com múltiplas variáveis\n- Teste com diferentes cenários\n\n#### Dica do Pedro:\nComece simples! Defina o estado, crie as funções, monte o grafo e teste. Depois você pode incrementar! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# SEU CÓDIGO AQUI! 🚀\n# Implemente o sistema de recomendação\n\n# 1. Defina o estado (RecommendationState)\n# 2. Crie as funções dos nós\n# 3. Monte o grafo\n# 4. Teste!\n\n# Estrutura sugerida:\n# class RecommendationState(TypedDict):\n#     user_preferences: Dict[str, Any]\n#     questions_asked: int\n#     enough_info: bool\n#     recommendations: List[str]\n#     # ... outros campos que você achar necessário\n\n# Dica: Comece definindo que tipo de recomendação vai fazer\n# (filmes, restaurantes, livros, etc.)\n\nprint(\"💪 Seu desafio está aqui!\")\nprint(\"🎯 Implemente um sistema de recomendação com LangGraph!\")\n\n# APAGUE ESTE COMENTÁRIO E IMPLEMENTE SUA SOLUÇÃO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🏅 Exercício Avançado: Multi-Agent System\n\n### 🚀 DESAFIO 2: Sistema de Análise de Texto Multi-Agente\n\nPara quem quer ir além! Crie um sistema onde **múltiplos especialistas** analisam um texto:\n\n1. **Agent Análise Sentimento** - Analisa se é positivo/negativo\n2. **Agent Extração Tópicos** - Identifica temas principais  \n3. **Agent Resumo** - Cria um resumo\n4. **Agent Coordenador** - Combina todos os resultados\n\n#### Fluxo:\n```mermaid\ngraph TD\n    A[Texto de entrada] --> B[Análise Sentimento]\n    A --> C[Extração Tópicos]\n    A --> D[Resumo]\n    B --> E[Coordenador]\n    C --> E\n    D --> E\n    E --> F[Relatório Final]\n```\n\n#### Requisitos Avançados:\n- Processamento em paralelo (simule com sleep)\n- Agregação inteligente dos resultados\n- Sistema de pontuação de confiança\n- Tratamento de erros\n\n**Dica do Pedro**: Este é nível **ninja**! Se conseguir fazer, você já entendeu o poder real do LangGraph! 🥷"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# DESAFIO NINJA! 🥷\n# Sistema Multi-Agent de Análise de Texto\n\n# Sua missão (se aceitar):\n# 1. Criar múltiplos agents especializados\n# 2. Fazer eles trabalharem em paralelo\n# 3. Coordenar os resultados\n# 4. Gerar um relatório unificado\n\n# Estado sugerido:\n# class MultiAnalysisState(TypedDict):\n#     input_text: str\n#     sentiment_analysis: Dict[str, Any]\n#     topic_extraction: Dict[str, Any]\n#     summary: str\n#     confidence_scores: Dict[str, float]\n#     final_report: str\n\n# Dica: Use time.sleep() para simular processamento em paralelo\n# Dica 2: Cada agent pode ter sua própria função de \"confiança\"\n\nimport time\nimport random\n\nprint(\"🥷 DESAFIO NINJA ATIVADO!\")\nprint(\"🎯 Crie um sistema multi-agent para análise de texto!\")\nprint(\"💪 Boa sorte, padawan!\")\n\n# SEU CÓDIGO AQUI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔮 O Futuro do LangGraph e próximos passos\n\nBora falar sobre onde isso tudo vai dar e como você pode evoluir!\n\n### 🚀 Próximas evoluções do LangGraph:\n\n1. **Paralelização Nativa** - Execução real em paralelo\n2. **Persistência de Estado** - Estados que sobrevivem entre execuções\n3. **Streaming Real-time** - Execução em tempo real\n4. **Auto-otimização** - Grafos que se otimizam sozinhos\n5. **Integração com Deploy** - Deploy direto na nuvem\n\n### 💼 Casos de uso empresariais reais:\n\n- **Customer Journey Automation** - Jornadas personalizadas\n- **Content Moderation** - Moderação inteligente em múltiplas etapas  \n- **Sales Qualification** - Qualificação automática de leads\n- **Technical Support** - Suporte técnico multiníveis\n- **Data Processing Pipelines** - Pipelines de dados inteligentes\n\n### 🎯 Como continuar estudando:\n\n1. **Pratique os exemplos** deste notebook\n2. **Crie seus próprios grafos** para problemas reais\n3. **Combine com o que aprendeu** nos módulos anteriores\n4. **Experimente com diferentes LLMs**\n5. **Teste em produção** (próximo módulo: LangSmith!)\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_05.png)\n\n**Dica do Pedro**: LangGraph não é só uma ferramenta, é uma **nova forma de pensar** em IA! Em vez de \"o que minha IA faz?\", pergunte \"como minha IA pensa?\". Essa mudança de mindset é revolucionária! 🧠✨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Vamos criar uma visualização do \"futuro\" do LangGraph\n# Mostrando a evolução de complexidade que conseguimos atingir\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Dados da evolução de capacidades\ntechnologies = ['Scripts\\nSimples', 'Chains\\nLangChain', 'Agents\\nTradicionais', 'LangGraph\\nBásico', 'LangGraph\\nAvançado', 'Multi-Agent\\nSystems']\ncomplexity_score = [1, 3, 5, 7, 8, 10]\ncapabilities = [1, 4, 6, 8, 9, 10]\nmaintainability = [8, 6, 4, 7, 8, 9]\n\nx = np.arange(len(technologies))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(14, 8))\n\n# Barras para cada métrica\nrects1 = ax.bar(x - width, complexity_score, width, label='Complexidade de Problemas', \n                color='#FF6B6B', alpha=0.8)\nrects2 = ax.bar(x, capabilities, width, label='Capacidades', \n                color='#4ECDC4', alpha=0.8)\nrects3 = ax.bar(x + width, maintainability, width, label='Manutenibilidade', \n                color='#45B7D1', alpha=0.8)\n\n# Configuração do gráfico\nax.set_xlabel('Tecnologias', fontsize=12, weight='bold')\nax.set_ylabel('Score (1-10)', fontsize=12, weight='bold')\nax.set_title('🚀 Evolução das Tecnologias de IA - De Scripts a Multi-Agents', \n             fontsize=14, weight='bold', pad=20)\nax.set_xticks(x)\nax.set_xticklabels(technologies, rotation=0, ha='center')\nax.legend(loc='upper left')\nax.grid(axis='y', alpha=0.3)\nax.set_ylim(0, 11)\n\n# Adicionando anotações especiais\nax.annotate('🎯 Estamos aqui!', \n            xy=(4, 8.5), xytext=(4.5, 9.5),\n            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n            fontsize=12, weight='bold', color='red')\n\nax.annotate('🔮 O futuro!', \n            xy=(5, 10), xytext=(4.2, 10.5),\n            arrowprops=dict(arrowstyle='->', color='purple', lw=2),\n            fontsize=12, weight='bold', color='purple')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"📈 Evolução das capacidades:\")\nprint(\"🔥 LangGraph representa um salto qualitativo!\")\nprint(\"💪 Você agora domina uma das tecnologias mais avançadas de IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Resumo do Módulo - O que aprendemos?\n\nCaraca, que jornada! Vamos recapitular tudo que vimos:\n\n### ✅ **Conceitos Fundamentais**\n- **LangGraph** = GPS para agents inteligentes\n- **Grafos dirigidos** para controle de fluxo\n- **Estados compartilhados** entre nós\n- **Execução condicional** baseada em lógica\n\n### ✅ **Implementações Práticas**\n- Sistema de **contador com loops**\n- **Chatbot inteligente** com análise contextual\n- **Sistema de atendimento** empresarial completo\n- **Fluxos condicionais** e decisões automáticas\n\n### ✅ **Comparações e Insights**\n- **Agents vs LangGraph** - quando usar cada um\n- **Casos de uso reais** em empresas\n- **Visualizações** de fluxos e performance\n- **Próximos passos** na evolução da tecnologia\n\n### 🎯 **Principais Diferenças do que vimos antes:**\n\n| Módulo Anterior | LangGraph | \n|-----------------|----------|\n| Agents lineares (Módulo 9) | **Fluxos complexos e condicionais** |\n| Memory isolada (Módulo 5) | **Estado compartilhado inteligente** |\n| Chains simples (Módulo 4) | **Workflows sofisticados** |\n\n### 🚀 **Preparação para o Módulo 15:**\nNo próximo módulo vamos aprender sobre **LangSmith** - a ferramenta que vai nos ajudar a:\n- **Monitorar** nossos LangGraphs em produção\n- **Debugar** fluxos complexos\n- **Otimizar** performance \n- **Analisar** comportamentos\n\n### 💡 **Dica Final do Pedro:**\nLangGraph não é só sobre fazer IA mais complexa - é sobre fazer IA **mais inteligente e controlável**. Você agora tem o poder de criar sistemas que **realmente pensam** antes de agir!\n\nE lembra: **toda expertise começa com curiosidade**. Continue experimentando, criando e se divertindo com o que aprendeu! 🎉\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/langchain---usando-versão-v0.2-modulo-14_img_06.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Parabéns! Você chegou ao final do Módulo 14! 🎉\n# Vamos criar um \"certificado\" visual do que você domina agora\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, FancyBboxPatch\nimport matplotlib.patches as mpatches\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\n\n# Fundo do certificado\ncert_bg = FancyBboxPatch(\n    (0.5, 1), 9, 8,\n    boxstyle=\"round,pad=0.1\",\n    facecolor='#f0f8ff',\n    edgecolor='#4169e1',\n    linewidth=3\n)\nax.add_patch(cert_bg)\n\n# Título\nax.text(5, 8.5, '🏆 CERTIFICADO DE CONCLUSÃO', \n        ha='center', va='center', fontsize=18, weight='bold', color='#4169e1')\nax.text(5, 8, 'Módulo 14: LangGraph Mastery', \n        ha='center', va='center', fontsize=14, weight='bold', color='#2e8b57')\n\n# Linha decorativa\nax.plot([1.5, 8.5], [7.5, 7.5], color='#4169e1', linewidth=2)\n\n# Competências adquiridas\nskills = [\n    '✅ Criação de Grafos Inteligentes',\n    '✅ Gerenciamento de Estados Complexos', \n    '✅ Fluxos Condicionais e Loops',\n    '✅ Sistemas Multi-Agent',\n    '✅ Integração com LLMs',\n    '✅ Debugging e Visualização'\n]\n\ny_start = 6.5\nfor i, skill in enumerate(skills):\n    ax.text(1.5, y_start - i*0.4, skill, \n            ha='left', va='center', fontsize=11, color='#2e8b57')\n\n# Estatísticas\nax.text(5, 3.5, '📊 SUAS ESTATÍSTICAS:', \n        ha='center', va='center', fontsize=12, weight='bold', color='#4169e1')\n\nstats = [\n    '🧠 Conceitos Aprendidos: 10+',\n    '💻 Códigos Executados: 20+', \n    '🏗️ Projetos Criados: 3',\n    '🎯 Nível Alcançado: Avançado'\n]\n\ny_start = 3\nfor i, stat in enumerate(stats):\n    ax.text(5, y_start - i*0.3, stat, \n            ha='center', va='center', fontsize=10, color='#2e8b57')\n\n# Assinatura do Pedro\nax.text(5, 1.5, '👨‍💻 Pedro Nunes Guth', \n        ha='center', va='center', fontsize=12, weight='bold', color='#4169e1')\nax.text(5, 1.2, 'Instrutor Expert em IA & LangChain', \n        ha='center', va='center', fontsize=10, style='italic', color='#666')\n\nplt.title('🎓 PARABÉNS! VOCÊ DOMINA LANGGRAPH!', \n          fontsize=16, weight='bold', pad=20, color='#4169e1')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"🎉 PARABÉNS! Você completou o Módulo 14!\")\nprint(\"🚀 Agora você domina LangGraph e pode criar sistemas de IA complexos!\")\nprint(\"📚 Próximo desafio: Módulo 15 - LangSmith para produção!\")\nprint(\"\\n💪 Continue assim, você está arrasando!\")"
      ]
    }
  ]
}