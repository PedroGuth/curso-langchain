{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“š Document Loading e Splitters: Transformando Documentos em Conhecimento Ãštil para IA\n\n",
        "## MÃ³dulo 6 - LangChain v0.2 ğŸš€\n\n",
        "---\n\n",
        "**EaÃ­, galera!** Chegamos no mÃ³dulo 6 do nosso curso de LangChain e agora vamos falar sobre uma das partes mais importantes quando trabalhamos com IA: **como carregar e dividir documentos**!\n\n",
        "JÃ¡ vimos nos mÃ³dulos anteriores como usar ChatModels, Prompt Templates, Chains e Memory Systems. Agora vamos aprender como alimentar nossa IA com conhecimento de documentos reais - PDFs, textos, pÃ¡ginas web e muito mais!\n\n",
        "**Bora entender como transformar qualquer documento em conhecimento Ãºtil para nossa IA!** ğŸ¤–ğŸ“–"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ O que vamos aprender hoje?\n\n",
        "- âœ… **Document Loaders**: Como carregar diferentes tipos de documentos\n",
        "- âœ… **Text Splitters**: Como dividir textos grandes em pedaÃ§os Ãºteis\n",
        "- âœ… **EstratÃ©gias de chunking**: Diferentes formas de dividir textos\n",
        "- âœ… **PreparaÃ§Ã£o para RAG**: Como isso se conecta com o que vem pela frente\n",
        "- âœ… **Casos prÃ¡ticos**: PDFs, websites, CSVs e mais!\n\n",
        "**Dica do Pedro**: Pense nos Document Loaders como \"garÃ§ons\" que trazem a comida (documentos) da cozinha, e os Splitters como \"chefs\" que cortam tudo em pedaÃ§os do tamanho certo para vocÃª comer (processar)! ğŸ½ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos instalar as dependÃªncias necessÃ¡rias\n",
        "!pip install langchain langchain-community langchain-google-genai\n",
        "!pip install pypdf python-docx beautifulsoup4 requests\n",
        "!pip install matplotlib seaborn pandas numpy\n",
        "\n",
        "print(\"ğŸ“¦ Todas as dependÃªncias instaladas com sucesso!\")\n",
        "print(\"ğŸš€ Bora comeÃ§ar nossa jornada pelos Document Loaders!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports essenciais para nosso workshop\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# LangChain imports - os protagonistas de hoje!\n",
        "from langchain.document_loaders import (\n",
        "    TextLoader,\n",
        "    PyPDFLoader,\n",
        "    WebBaseLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter\n",
        ")\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "print(\"ğŸ‰ Imports carregados! Vamos comeÃ§ar a brincadeira!\")\n",
        "print(\"ğŸ“š Hoje vamos transformar documentos em conhecimento Ãºtil para IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤” TÃ¡, mas o que sÃ£o Document Loaders?\n\n",
        "**Document Loaders** sÃ£o como \"tradutores universais\" que conseguem ler qualquer tipo de documento e transformar em um formato que o LangChain entende.\n\n",
        "### Analogia do AÃ§ougue ğŸ¥©\n",
        "Imagina que vocÃª vai no aÃ§ougue e pede:\n",
        "- **Carne bovina** (PDF)\n",
        "- **Frango** (TXT)\n",
        "- **Peixe** (CSV)\n",
        "- **CamarÃ£o** (Website)\n\n",
        "O aÃ§ougueiro (Document Loader) pega todos esses diferentes tipos de \"proteÃ­na\" e entrega tudo **embalado da mesma forma** para vocÃª levar para casa!\n\n",
        "### Estrutura de um Document no LangChain\n",
        "Todo documento carregado vira um objeto `Document` com:\n",
        "- **page_content**: O texto em si\n",
        "- **metadata**: InformaÃ§Ãµes extras (nome do arquivo, pÃ¡gina, etc.)\n\n",
        "```python\n",
        "Document(\n",
        "    page_content=\"Aqui fica o texto do documento...\",\n",
        "    metadata={\"source\": \"arquivo.pdf\", \"page\": 1}\n",
        ")\n",
        "```\n\n",
        "**Dica do Pedro**: Ã‰ como se cada documento virasse uma \"ficha\" padronizada, independente de onde veio! ğŸ“„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar alguns documentos de exemplo para brincar\n",
        "import tempfile\n",
        "\n",
        "# Criando um arquivo de texto de exemplo\n",
        "texto_exemplo = \"\"\"\n",
        "LangChain Ã© uma framework incrÃ­vel para desenvolvimento de aplicaÃ§Ãµes com IA.\n",
        "Ela permite integrar modelos de linguagem com diversas fontes de dados.\n",
        "Com Document Loaders, podemos carregar informaÃ§Ãµes de PDFs, websites, \n",
        "bancos de dados e muito mais!\n",
        "\n",
        "No Brasil, o uso de IA estÃ¡ crescendo exponencialmente.\n",
        "Empresas de todos os tamanhos estÃ£o adotando soluÃ§Ãµes inteligentes.\n",
        "O LangChain facilita muito esse processo de implementaÃ§Ã£o.\n",
        "\"\"\"\n",
        "\n",
        "# Salvando em um arquivo temporÃ¡rio\n",
        "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
        "    f.write(texto_exemplo)\n",
        "    arquivo_texto = f.name\n",
        "\n",
        "print(f\"ğŸ“ Arquivo de exemplo criado: {arquivo_texto}\")\n",
        "print(\"ğŸ¯ Agora vamos carregar este arquivo usando TextLoader!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nosso primeiro Document Loader em aÃ§Ã£o!\n",
        "loader = TextLoader(arquivo_texto, encoding='utf-8')\n",
        "documentos = loader.load()\n",
        "\n",
        "print(\"ğŸ‰ Documento carregado com sucesso!\")\n",
        "print(f\"ğŸ“Š NÃºmero de documentos: {len(documentos)}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ“„ CONTEÃšDO DO DOCUMENTO:\")\n",
        "print(\"=\"*50)\n",
        "print(documentos[0].page_content)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ·ï¸ METADATA:\")\n",
        "print(\"=\"*50)\n",
        "print(documentos[0].metadata)\n",
        "\n",
        "# Limpeza\n",
        "os.unlink(arquivo_texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒ Carregando ConteÃºdo da Web\n\n",
        "Agora vamos ver algo **liiindo**: carregar conteÃºdo diretamente de websites! \n",
        "\n",
        "O `WebBaseLoader` Ã© como ter um **\"assistente virtual\"** que vai lÃ¡ no site, lÃª tudo e traz o conteÃºdo organizadinho para vocÃª!\n\n",
        "### Como funciona por baixo dos panos:\n",
        "1. ğŸŒ Faz uma requisiÃ§Ã£o HTTP para a URL\n",
        "2. ğŸ§¹ Usa BeautifulSoup para \"limpar\" o HTML\n",
        "3. ğŸ“ Extrai apenas o texto Ãºtil\n",
        "4. ğŸ“¦ Empacota tudo em um Document\n\n",
        "**Dica do Pedro**: Ã‰ como ter um estagiÃ¡rio que vai na biblioteca, lÃª o livro inteiro e faz um resumo organizado para vocÃª! ğŸ“šğŸ‘¨â€ğŸ“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando conteÃºdo de uma pÃ¡gina web\n",
        "# Vamos usar uma pÃ¡gina do Wikipedia sobre IA\n",
        "url = \"https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial\"\n",
        "\n",
        "try:\n",
        "    web_loader = WebBaseLoader(url)\n",
        "    doc_web = web_loader.load()\n",
        "    \n",
        "    print(\"ğŸŒ PÃ¡gina web carregada com sucesso!\")\n",
        "    print(f\"ğŸ“Š NÃºmero de documentos: {len(doc_web)}\")\n",
        "    print(f\"ğŸ“ Tamanho do conteÃºdo: {len(doc_web[0].page_content)} caracteres\")\n",
        "    \n",
        "    # Mostrando os primeiros 500 caracteres\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“„ PREVIEW DO CONTEÃšDO:\")\n",
        "    print(\"=\"*50)\n",
        "    print(doc_web[0].page_content[:500] + \"...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"ğŸ·ï¸ METADATA:\")\n",
        "    print(\"=\"*30)\n",
        "    print(doc_web[0].metadata)\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"âŒ Erro ao carregar pÃ¡gina: {e}\")\n",
        "    print(\"ğŸ”„ Vamos continuar com um exemplo simulado...\")\n",
        "    \n",
        "    # Criando um documento simulado caso nÃ£o consiga acessar a web\n",
        "    doc_web = [Document(\n",
        "        page_content=\"InteligÃªncia artificial Ã© a simulaÃ§Ã£o de processos de inteligÃªncia humana por mÃ¡quinas, especialmente sistemas de computador. Estes processos incluem aprendizado, raciocÃ­nio e autocorreÃ§Ã£o.\",\n",
        "        metadata={\"source\": url, \"title\": \"InteligÃªncia Artificial\"}\n",
        "    )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar e carregar um CSV de exemplo\n",
        "import csv\n",
        "\n",
        "# Dados de exemplo sobre vendas de IA no Brasil\n",
        "dados_csv = [\n",
        "    [\"Empresa\", \"Produto\", \"Vendas_2023\", \"RegiÃ£o\"],\n",
        "    [\"TechAI\", \"Chatbot Corporativo\", \"1500000\", \"SÃ£o Paulo\"],\n",
        "    [\"BrasilBot\", \"Assistente Virtual\", \"800000\", \"Rio de Janeiro\"],\n",
        "    [\"SmartSul\", \"IA para Vendas\", \"1200000\", \"Porto Alegre\"],\n",
        "    [\"NordesteAI\", \"AutomaÃ§Ã£o Industrial\", \"950000\", \"Recife\"],\n",
        "    [\"CentroIA\", \"AnÃ¡lise Preditiva\", \"600000\", \"BrasÃ­lia\"]\n",
        "]\n",
        "\n",
        "# Criando arquivo CSV temporÃ¡rio\n",
        "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(dados_csv)\n",
        "    arquivo_csv = f.name\n",
        "\n",
        "print(f\"ğŸ“Š Arquivo CSV criado: {arquivo_csv}\")\n",
        "\n",
        "# Carregando o CSV\n",
        "csv_loader = CSVLoader(arquivo_csv, encoding='utf-8')\n",
        "docs_csv = csv_loader.load()\n",
        "\n",
        "print(f\"\\nâœ… CSV carregado com sucesso!\")\n",
        "print(f\"ğŸ“ˆ NÃºmero de documentos (linhas): {len(docs_csv)}\")\n",
        "\n",
        "# Mostrando os primeiros documentos\n",
        "for i, doc in enumerate(docs_csv[:3]):\n",
        "    print(f\"\\nğŸ“‹ DOCUMENTO {i+1}:\")\n",
        "    print(f\"ConteÃºdo: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Limpeza\n",
        "os.unlink(arquivo_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ‚ï¸ Text Splitters: A Arte de Dividir Textos\n\n",
        "Agora vem a parte **mais importante**: os **Text Splitters**! ğŸ¯\n\n",
        "### Por que precisamos dividir textos?\n",
        "\n",
        "Imagina que vocÃª tem um livro de 500 pÃ¡ginas sobre IA e quer que o ChatGPT responda perguntas sobre ele. O problema Ã©:\n",
        "- ğŸ§  **Modelos de IA tÃªm limite de tokens** (como ter uma \"boca pequena\")\n",
        "- ğŸ¯ **Textos muito grandes perdem foco** (Ã© difÃ­cil achar informaÃ§Ã£o especÃ­fica)\n",
        "- âš¡ **Processamento fica lento** (como mastigar um sanduÃ­che gigante)\n\n",
        "### Analogia da Pizza ğŸ•\n",
        "Os Text Splitters sÃ£o como **cortar uma pizza gigante**:\n",
        "- **RecursiveCharacterTextSplitter**: Corta seguindo as \"divisÃµes naturais\" (fatias iguais respeitando os ingredientes)\n",
        "- **CharacterTextSplitter**: Corta em um ponto especÃ­fico (como cortar sempre na borda do pepperoni)\n",
        "- **TokenTextSplitter**: Conta cada \"grÃ£o de queijo\" e corta quando chegar no limite\n\n",
        "### ParÃ¢metros Importantes:\n",
        "- **chunk_size**: Tamanho de cada \"fatia\"\n",
        "- **chunk_overlap**: Quantos \"ingredientes\" ficam repetidos entre fatias (para nÃ£o perder contexto)\n",
        "\n",
        "**Dica do Pedro**: O segredo estÃ¡ no **overlap**! Ã‰ como deixar um pedacinho da fatia anterior na prÃ³xima para manter o \"sabor\" da conversa! ğŸ§ âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um texto longo para testar nossos splitters\n",
        "texto_longo = \"\"\"\n",
        "A InteligÃªncia Artificial no Brasil estÃ¡ passando por uma revoluÃ§Ã£o extraordinÃ¡ria. \n",
        "Empresas de todos os setores estÃ£o adotando soluÃ§Ãµes baseadas em IA para otimizar processos, \n",
        "melhorar a experiÃªncia do cliente e aumentar a competitividade.\n",
        "\n",
        "No setor financeiro, bancos como ItaÃº, Bradesco e Nubank utilizam algoritmos de machine learning \n",
        "para anÃ¡lise de crÃ©dito, detecÃ§Ã£o de fraudes e personalizaÃ§Ã£o de produtos. \n",
        "Essas tecnologias permitem decisÃµes mais rÃ¡pidas e precisas.\n",
        "\n",
        "O varejo brasileiro tambÃ©m abraÃ§ou a IA. Empresas como Magazine Luiza e Via Varejo \n",
        "implementaram chatbots inteligentes, sistemas de recomendaÃ§Ã£o e anÃ¡lise preditiva de demanda. \n",
        "Isso resulta em melhor gestÃ£o de estoque e experiÃªncias de compra mais personalizadas.\n",
        "\n",
        "Na Ã¡rea da saÃºde, startups brasileiras desenvolvem soluÃ§Ãµes inovadoras. \n",
        "Sistemas de diagnÃ³stico por imagem, anÃ¡lise de exames laboratoriais e \n",
        "monitoramento de pacientes usando IA estÃ£o revolucionando o atendimento mÃ©dico.\n",
        "\n",
        "O agronegÃ³cio, setor vital da economia brasileira, tambÃ©m se beneficia enormemente da IA. \n",
        "AnÃ¡lise de imagens de satÃ©lite, previsÃ£o climÃ¡tica, otimizaÃ§Ã£o de plantio e \n",
        "monitoramento de pragas sÃ£o apenas alguns exemplos de aplicaÃ§Ã£o.\n",
        "\n",
        "LangChain facilita muito o desenvolvimento dessas soluÃ§Ãµes ao fornecer \n",
        "ferramentas padronizadas para integraÃ§Ã£o de modelos de linguagem com dados empresariais.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"ğŸ“ Texto criado com {len(texto_longo)} caracteres\")\n",
        "print(f\"ğŸ“Š Aproximadamente {len(texto_longo.split())} palavras\")\n",
        "print(\"\\nğŸ¯ Agora vamos ver diferentes formas de dividir este texto!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RecursiveCharacterTextSplitter - O mais inteligente!\n",
        "splitter_recursivo = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,  # Cada pedaÃ§o terÃ¡ atÃ© 300 caracteres\n",
        "    chunk_overlap=50,  # 50 caracteres de sobreposiÃ§Ã£o\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Ordem de preferÃªncia para dividir\n",
        ")\n",
        "\n",
        "chunks_recursivo = splitter_recursivo.split_text(texto_longo)\n",
        "\n",
        "print(\"ğŸ§  RECURSIVE CHARACTER TEXT SPLITTER\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“Š NÃºmero de chunks: {len(chunks_recursivo)}\")\n",
        "\n",
        "for i, chunk in enumerate(chunks_recursivo):\n",
        "    print(f\"\\nğŸ“„ CHUNK {i+1} ({len(chunk)} caracteres):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(chunk.strip())\n",
        "    if i >= 2:  # Mostrando apenas os primeiros 3\n",
        "        print(f\"\\n... (e mais {len(chunks_recursivo)-3} chunks)\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos comparar com Character TextSplitter\n",
        "splitter_simples = CharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\\n\"  # SÃ³ divide em parÃ¡grafos\n",
        ")\n",
        "\n",
        "chunks_simples = splitter_simples.split_text(texto_longo)\n",
        "\n",
        "print(\"âš¡ CHARACTER TEXT SPLITTER\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“Š NÃºmero de chunks: {len(chunks_simples)}\")\n",
        "\n",
        "for i, chunk in enumerate(chunks_simples[:2]):\n",
        "    print(f\"\\nğŸ“„ CHUNK {i+1} ({len(chunk)} caracteres):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(chunk.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando as diferenÃ§as entre os splitters\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Tamanhos dos chunks - Recursive\n",
        "tamanhos_recursivo = [len(chunk) for chunk in chunks_recursivo]\n",
        "ax1.bar(range(len(tamanhos_recursivo)), tamanhos_recursivo, \n",
        "        color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "ax1.set_title('ğŸ“Š Recursive Character Text Splitter\\nTamanho dos Chunks', fontsize=12, pad=20)\n",
        "ax1.set_xlabel('NÃºmero do Chunk')\n",
        "ax1.set_ylabel('Tamanho (caracteres)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Tamanhos dos chunks - Character\n",
        "tamanhos_simples = [len(chunk) for chunk in chunks_simples]\n",
        "ax2.bar(range(len(tamanhos_simples)), tamanhos_simples, \n",
        "        color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "ax2.set_title('ğŸ“Š Character Text Splitter\\nTamanho dos Chunks', fontsize=12, pad=20)\n",
        "ax2.set_xlabel('NÃºmero do Chunk')\n",
        "ax2.set_ylabel('Tamanho (caracteres)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ“ˆ ESTATÃSTICAS:\")\n",
        "print(f\"Recursive - Chunks: {len(chunks_recursivo)}, Tamanho mÃ©dio: {np.mean(tamanhos_recursivo):.1f}\")\n",
        "print(f\"Character - Chunks: {len(chunks_simples)}, Tamanho mÃ©dio: {np.mean(tamanhos_simples):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§® Token Text Splitter: Contando Cada \"Palavrinha\"\n\n",
        "O **TokenTextSplitter** Ã© o mais preciso de todos! Ele conta **tokens** (as \"palavrinhas\" que a IA entende) em vez de caracteres.\n\n",
        "### Por que tokens sÃ£o importantes?\n",
        "- ğŸ§  **Modelos de IA cobram por token** (como tÃ¡xi que cobra por quilÃ´metro)\n",
        "- ğŸ“ **Cada modelo tem limite de tokens** (como elevador com limite de peso)\n",
        "- ğŸ¯ **Token Ã© a \"moeda\" da IA** (1 token â‰ˆ 0.75 palavras em inglÃªs)\n\n",
        "### Regra de ouro:\n",
        "- **1 token** â‰ˆ 4 caracteres em inglÃªs\n",
        "- **1 token** â‰ˆ 0.75 palavras\n",
        "- **Em portuguÃªs pode variar** (palavras maiores = mais tokens)\n\n",
        "**Dica do Pedro**: Ã‰ como comprar carne no aÃ§ougue - vocÃª nÃ£o paga pelo tamanho da bandeja, mas pelo peso real da carne! ğŸ¥©âš–ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos tentar usar o TokenTextSplitter\n",
        "# Nota: Pode precisar de configuraÃ§Ã£o adicional dependendo do tokenizer\n",
        "\n",
        "try:\n",
        "    # Para usar tiktoken (tokenizer do OpenAI)\n",
        "    !pip install tiktoken -q\n",
        "    \n",
        "    from langchain.text_splitter import TokenTextSplitter\n",
        "    \n",
        "    token_splitter = TokenTextSplitter(\n",
        "        chunk_size=100,  # 100 tokens por chunk\n",
        "        chunk_overlap=20   # 20 tokens de overlap\n",
        "    )\n",
        "    \n",
        "    chunks_token = token_splitter.split_text(texto_longo)\n",
        "    \n",
        "    print(\"ğŸ§® TOKEN TEXT SPLITTER\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"ğŸ“Š NÃºmero de chunks: {len(chunks_token)}\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks_token[:3]):\n",
        "        # Estimativa de tokens (aproximada)\n",
        "        tokens_aprox = len(chunk) // 4\n",
        "        print(f\"\\nğŸ¯ CHUNK {i+1} (~{tokens_aprox} tokens, {len(chunk)} caracteres):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(chunk.strip())\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"âš ï¸ TokenTextSplitter precisa do tiktoken instalado\")\n",
        "    print(\"ğŸ“ Vamos simular o comportamento...\")\n",
        "    \n",
        "    # SimulaÃ§Ã£o simples baseada em palavras\n",
        "    palavras = texto_longo.split()\n",
        "    chunk_size_palavras = 75  # Aproximadamente 100 tokens\n",
        "    overlap_palavras = 15     # Aproximadamente 20 tokens\n",
        "    \n",
        "    chunks_token = []\n",
        "    for i in range(0, len(palavras), chunk_size_palavras - overlap_palavras):\n",
        "        chunk_palavras = palavras[i:i + chunk_size_palavras]\n",
        "        chunks_token.append(\" \".join(chunk_palavras))\n",
        "    \n",
        "    print(\"ğŸ§® TOKEN TEXT SPLITTER (Simulado)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"ğŸ“Š NÃºmero de chunks: {len(chunks_token)}\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks_token[:2]):\n",
        "        tokens_aprox = len(chunk.split())\n",
        "        print(f\"\\nğŸ¯ CHUNK {i+1} (~{tokens_aprox} palavras/tokens):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando todos os splitters em um grÃ¡fico Ã©pico!\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Dados para comparaÃ§Ã£o\n",
        "splitters_nomes = ['Recursive\\nCharacter', 'Character', 'Token\\n(Simulado)']\n",
        "num_chunks = [len(chunks_recursivo), len(chunks_simples), len(chunks_token)]\n",
        "cores = ['#3498db', '#e74c3c', '#f39c12']\n",
        "\n",
        "# Criando o grÃ¡fico de barras\n",
        "barras = ax.bar(splitters_nomes, num_chunks, color=cores, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for barra, valor in zip(barras, num_chunks):\n",
        "    altura = barra.get_height()\n",
        "    ax.text(barra.get_x() + barra.get_width()/2., altura + 0.1,\n",
        "            f'{valor} chunks', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "ax.set_title('ğŸ¥Š BATALHA DOS TEXT SPLITTERS\\nNÃºmero de Chunks Gerados', \n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('NÃºmero de Chunks', fontsize=12)\n",
        "ax.set_xlabel('Tipo de Splitter', fontsize=12)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Deixando o grÃ¡fico mais bonito\n",
        "ax.set_ylim(0, max(num_chunks) * 1.2)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ† RESULTADO DA BATALHA:\")\n",
        "print(f\"ğŸ¥‡ Recursive Character: {len(chunks_recursivo)} chunks (Mais inteligente!)\")\n",
        "print(f\"ğŸ¥ˆ Character: {len(chunks_simples)} chunks (Mais simples)\")\n",
        "print(f\"ğŸ¥‰ Token: {len(chunks_token)} chunks (Mais preciso para IA!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Workflow Completo: Loader + Splitter\n\n",
        "Agora vamos juntar tudo que aprendemos! **Document Loader + Text Splitter** = Combo perfeito! ğŸ¯\n\n",
        "### O Fluxo Completo:\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[ğŸ“„ Documento Original] --> B[ğŸ”„ Document Loader]\n",
        "    B --> C[ğŸ“ Document Object]\n",
        "    C --> D[âœ‚ï¸ Text Splitter]\n",
        "    D --> E[ğŸ“š Chunks Menores]\n",
        "    E --> F[ğŸ¤– Pronto para IA!]\n",
        "```\n\n",
        "### Por que esse workflow Ã© importante?\n",
        "- ğŸ¯ **Prepara dados para RAG** (prÃ³ximo mÃ³dulo!)\n",
        "- âš¡ **Otimiza performance** da IA\n",
        "- ğŸ’° **Economiza tokens** (e dinheiro!)\n",
        "- ğŸ§  **MantÃ©m contexto** com overlap\n\n",
        "**Dica do Pedro**: Ã‰ como preparar ingredientes antes de cozinhar - quanto melhor a preparaÃ§Ã£o, melhor o resultado final! ğŸ‘¨â€ğŸ³âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow completo: Criando documentos, carregando e dividindo\n",
        "def processar_documento_completo(texto, chunk_size=200, chunk_overlap=30):\n",
        "    \"\"\"\n",
        "    FunÃ§Ã£o que simula o workflow completo de processamento de documentos\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ INICIANDO WORKFLOW COMPLETO\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Etapa 1: Simular Document Loader\n",
        "    print(\"ğŸ“„ Etapa 1: Carregando documento...\")\n",
        "    documento = Document(\n",
        "        page_content=texto,\n",
        "        metadata={\"source\": \"documento_exemplo.txt\", \"tipo\": \"texto\"}\n",
        "    )\n",
        "    print(f\"âœ… Documento carregado: {len(documento.page_content)} caracteres\")\n",
        "    \n",
        "    # Etapa 2: Text Splitter\n",
        "    print(\"\\nâœ‚ï¸ Etapa 2: Dividindo em chunks...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    \n",
        "    # Dividindo o documento\n",
        "    chunks = splitter.split_documents([documento])\n",
        "    print(f\"âœ… Documento dividido em {len(chunks)} chunks\")\n",
        "    \n",
        "    # Etapa 3: AnÃ¡lise dos chunks\n",
        "    print(\"\\nğŸ“Š Etapa 3: Analisando resultados...\")\n",
        "    tamanhos = [len(chunk.page_content) for chunk in chunks]\n",
        "    \n",
        "    print(f\"ğŸ“ˆ Tamanho mÃ©dio dos chunks: {np.mean(tamanhos):.1f} caracteres\")\n",
        "    print(f\"ğŸ“ Menor chunk: {min(tamanhos)} caracteres\")\n",
        "    print(f\"ğŸ“ Maior chunk: {max(tamanhos)} caracteres\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Testando o workflow\n",
        "chunks_finais = processar_documento_completo(texto_longo, chunk_size=250, chunk_overlap=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando os chunks finais\n",
        "print(\"ğŸ¯ CHUNKS FINAIS PROCESSADOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, chunk in enumerate(chunks_finais):\n",
        "    print(f\"\\nğŸ“„ CHUNK {i+1}\")\n",
        "    print(f\"ğŸ“Š Tamanho: {len(chunk.page_content)} caracteres\")\n",
        "    print(f\"ğŸ·ï¸ Metadata: {chunk.metadata}\")\n",
        "    print(f\"ğŸ“ ConteÃºdo:\")\n",
        "    print(\"-\" * 40)\n",
        "    # Mostrando apenas os primeiros 150 caracteres para nÃ£o poluir\n",
        "    preview = chunk.page_content.strip()[:150]\n",
        "    print(preview + \"...\" if len(chunk.page_content) > 150 else preview)\n",
        "    \n",
        "    if i >= 3:  # Limitando a exibiÃ§Ã£o\n",
        "        print(f\"\\n... (e mais {len(chunks_finais)-4} chunks)\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nğŸ‰ Processo concluÃ­do! {len(chunks_finais)} chunks prontos para uso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ® EXERCÃCIO PRÃTICO 1: Criando seu Document Processor\n\n",
        "**Hora de colocar a mÃ£o na massa!** ğŸ’ª\n\n",
        "### ğŸ¯ Seu Desafio:\n",
        "Crie uma funÃ§Ã£o que:\n",
        "1. Recebe um texto longo\n",
        "2. Testa 3 estratÃ©gias diferentes de splitting\n",
        "3. Compara os resultados\n",
        "4. Retorna a melhor estratÃ©gia\n",
        "\n",
        "### ğŸ“‹ CritÃ©rios de \"melhor estratÃ©gia\":\n",
        "- Chunks com tamanhos mais uniformes\n",
        "- Menor nÃºmero total de chunks\n",
        "- Melhor preservaÃ§Ã£o de contexto\n",
        "\n",
        "**Dica do Pedro**: Pense como um chef que testa diferentes formas de cortar os ingredientes para ver qual fica melhor na receita! ğŸ‘¨â€ğŸ³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 1: Complete a funÃ§Ã£o abaixo\n",
        "def comparar_estrategias_splitting(texto, chunk_size=300):\n",
        "    \"\"\"\n",
        "    Compara diferentes estratÃ©gias de text splitting\n",
        "    \n",
        "    Args:\n",
        "        texto (str): Texto para dividir\n",
        "        chunk_size (int): Tamanho desejado dos chunks\n",
        "    \n",
        "    Returns:\n",
        "        dict: Resultados das comparaÃ§Ãµes\n",
        "    \"\"\"\n",
        "    resultados = {}\n",
        "    \n",
        "    # TODO: Implemente 3 estratÃ©gias diferentes\n",
        "    # EstratÃ©gia 1: RecursiveCharacterTextSplitter\n",
        "    # EstratÃ©gia 2: CharacterTextSplitter  \n",
        "    # EstratÃ©gia 3: Sua escolha criativa!\n",
        "    \n",
        "    # TODO: Para cada estratÃ©gia, calcule:\n",
        "    # - NÃºmero de chunks\n",
        "    # - Tamanho mÃ©dio dos chunks\n",
        "    # - Desvio padrÃ£o dos tamanhos (uniformidade)\n",
        "    \n",
        "    # TODO: Determine qual Ã© a \"melhor\" estratÃ©gia\n",
        "    \n",
        "    # CÃ“DIGO DE EXEMPLO (substitua pelo seu):\n",
        "    splitter1 = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
        "    chunks1 = splitter1.split_text(texto)\n",
        "    \n",
        "    resultados['recursive'] = {\n",
        "        'chunks': len(chunks1),\n",
        "        'tamanho_medio': np.mean([len(c) for c in chunks1]),\n",
        "        'desvio_padrao': np.std([len(c) for c in chunks1])\n",
        "    }\n",
        "    \n",
        "    # COMPLETE COM AS OUTRAS ESTRATÃ‰GIAS!\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "# Teste sua funÃ§Ã£o\n",
        "resultados_teste = comparar_estrategias_splitting(texto_longo)\n",
        "print(\"ğŸ§ª Resultados do seu teste:\")\n",
        "print(resultados_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Preparando para o PrÃ³ximo MÃ³dulo: RAG\n\n",
        "**Liiindo!** Agora que sabemos carregar e dividir documentos, vamos entender como isso se conecta com o **RAG (Retrieval-Augmented Generation)** ğŸ¯\n\n",
        "### O que vem pela frente no MÃ³dulo 7:\n",
        "- ğŸ§  **Vector Stores**: Como transformar nossos chunks em vetores\n",
        "- ğŸ” **Embeddings**: A \"impressÃ£o digital\" semÃ¢ntica dos textos\n",
        "- ğŸ¯ **Similarity Search**: Como achar informaÃ§Ãµes relevantes\n",
        "- ğŸ¤– **RAG Implementation**: Juntando tudo para criar IA com conhecimento especÃ­fico\n\n",
        "### Como os chunks se transformam em conhecimento:\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[ğŸ“š Chunks] --> B[ğŸ§® Embeddings]\n",
        "    B --> C[ğŸ—„ï¸ Vector Store]\n",
        "    C --> D[ğŸ” Search]\n",
        "    D --> E[ğŸ¤– RAG Response]\n",
        "```\n\n",
        "**Dica do Pedro**: Os chunks que criamos hoje sÃ£o como \"fichas de conhecimento\" que vamos usar para alimentar nossa IA no prÃ³ximo mÃ³dulo! Ã‰ como criar um \"catÃ¡logo inteligente\" de informaÃ§Ãµes! ğŸ“šğŸ§ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparando nossos chunks para o prÃ³ximo mÃ³dulo\n",
        "def preparar_chunks_para_rag(chunks, max_chunks=10):\n",
        "    \"\"\"\n",
        "    Prepara chunks para uso em RAG, otimizando tamanho e qualidade\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”„ PREPARANDO CHUNKS PARA RAG\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    # Filtrando chunks muito pequenos (pouco conteÃºdo Ãºtil)\n",
        "    chunks_filtrados = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 50]\n",
        "    \n",
        "    # Limitando nÃºmero de chunks se necessÃ¡rio\n",
        "    if len(chunks_filtrados) > max_chunks:\n",
        "        chunks_filtrados = chunks_filtrados[:max_chunks]\n",
        "    \n",
        "    print(f\"âœ… Chunks originais: {len(chunks)}\")\n",
        "    print(f\"âœ… Chunks filtrados: {len(chunks_filtrados)}\")\n",
        "    print(f\"âœ… Chunks finais: {len(chunks_filtrados)}\")\n",
        "    \n",
        "    # EstatÃ­sticas dos chunks finais\n",
        "    tamanhos = [len(chunk.page_content) for chunk in chunks_filtrados]\n",
        "    \n",
        "    print(f\"\\nğŸ“Š ESTATÃSTICAS FINAIS:\")\n",
        "    print(f\"ğŸ“ Tamanho mÃ©dio: {np.mean(tamanhos):.1f} caracteres\")\n",
        "    print(f\"ğŸ“ Desvio padrÃ£o: {np.std(tamanhos):.1f}\")\n",
        "    print(f\"ğŸ“ˆ Min/Max: {min(tamanhos)}/{max(tamanhos)} caracteres\")\n",
        "    \n",
        "    return chunks_filtrados\n",
        "\n",
        "# Preparando para RAG\n",
        "chunks_para_rag = preparar_chunks_para_rag(chunks_finais)\n",
        "\n",
        "print(\"\\nğŸ¯ Chunks prontos para o MÃ³dulo 7 - Vector Stores e RAG!\")\n",
        "print(\"No prÃ³ximo mÃ³dulo vamos transformar estes chunks em vetores semÃ¢nticos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ® EXERCÃCIO PRÃTICO 2: Document Processor AvanÃ§ado\n\n",
        "**Ãšltimo desafio do mÃ³dulo!** ğŸ†\n\n",
        "### ğŸ¯ MissÃ£o ImpossÃ­vel:\n",
        "Crie um **Document Processor Universal** que:\n",
        "1. Detecta automaticamente o tipo de conteÃºdo\n",
        "2. Escolhe a estratÃ©gia de splitting ideal\n",
        "3. Otimiza os chunks para diferentes casos de uso\n",
        "4. Gera relatÃ³rio de qualidade\n",
        "\n",
        "### ğŸ“‹ EspecificaÃ§Ãµes:\n",
        "- **Entrada**: Texto de qualquer tipo\n",
        "- **SaÃ­da**: Chunks otimizados + relatÃ³rio\n",
        "- **Bonus**: VisualizaÃ§Ã£o dos resultados\n",
        "\n",
        "**Dica do Pedro**: Pense como um \"sommelier de chunks\" - cada tipo de texto precisa de um tratamento especial! ğŸ·"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÃCIO 2: Document Processor AvanÃ§ado\n",
        "class DocumentProcessorAvancado:\n",
        "    def __init__(self):\n",
        "        self.estrategias = {\n",
        "            'narrativo': {'chunk_size': 400, 'overlap': 60},\n",
        "            'tecnico': {'chunk_size': 300, 'overlap': 50},\n",
        "            'listagem': {'chunk_size': 200, 'overlap': 30}\n",
        "        }\n",
        "    \n",
        "    def detectar_tipo_conteudo(self, texto):\n",
        "        \"\"\"\n",
        "        Detecta o tipo de conteÃºdo do texto\n",
        "        TODO: Implemente a lÃ³gica de detecÃ§Ã£o\n",
        "        \"\"\"\n",
        "        # DICAS:\n",
        "        # - Conte parÃ¡grafos longos vs curtos\n",
        "        # - Procure por listas (-, *, nÃºmeros)\n",
        "        # - Analise vocabulÃ¡rio tÃ©cnico\n",
        "        \n",
        "        # IMPLEMENTAÃ‡ÃƒO SIMPLES (vocÃª pode melhorar!):\n",
        "        paragrafos = texto.split('\\n\\n')\n",
        "        tamanho_medio_paragrafo = np.mean([len(p) for p in paragrafos if p.strip()])\n",
        "        \n",
        "        if tamanho_medio_paragrafo > 300:\n",
        "            return 'narrativo'\n",
        "        elif any(palavra in texto.lower() for palavra in ['sistema', 'algoritmo', 'funÃ§Ã£o', 'tecnologia']):\n",
        "            return 'tecnico'\n",
        "        else:\n",
        "            return 'listagem'\n",
        "    \n",
        "    def processar(self, texto):\n",
        "        \"\"\"\n",
        "        Processa o documento completo\n",
        "        TODO: Complete a implementaÃ§Ã£o\n",
        "        \"\"\"\n",
        "        print(\"ğŸ¤– DOCUMENT PROCESSOR AVANÃ‡ADO\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Etapa 1: Detectar tipo\n",
        "        tipo = self.detectar_tipo_conteudo(texto)\n",
        "        print(f\"ğŸ” Tipo detectado: {tipo}\")\n",
        "        \n",
        "        # Etapa 2: Aplicar estratÃ©gia\n",
        "        config = self.estrategias[tipo]\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config['chunk_size'],\n",
        "            chunk_overlap=config['overlap']\n",
        "        )\n",
        "        \n",
        "        chunks = splitter.split_text(texto)\n",
        "        print(f\"âœ‚ï¸ Chunks gerados: {len(chunks)}\")\n",
        "        \n",
        "        # TODO: Adicione mais anÃ¡lises e otimizaÃ§Ãµes\n",
        "        \n",
        "        return {\n",
        "            'tipo': tipo,\n",
        "            'chunks': chunks,\n",
        "            'config_usada': config,\n",
        "            'estatisticas': self._gerar_estatisticas(chunks)\n",
        "        }\n",
        "    \n",
        "    def _gerar_estatisticas(self, chunks):\n",
        "        tamanhos = [len(chunk) for chunk in chunks]\n",
        "        return {\n",
        "            'total_chunks': len(chunks),\n",
        "            'tamanho_medio': np.mean(tamanhos),\n",
        "            'desvio_padrao': np.std(tamanhos),\n",
        "            'min_max': (min(tamanhos), max(tamanhos))\n",
        "        }\n",
        "\n",
        "# Testando o processor avanÃ§ado\n",
        "processor = DocumentProcessorAvancado()\n",
        "resultado = processor.processar(texto_longo)\n",
        "\n",
        "print(f\"\\nğŸ“Š RELATÃ“RIO FINAL:\")\n",
        "print(f\"Tipo: {resultado['tipo']}\")\n",
        "print(f\"EstatÃ­sticas: {resultado['estatisticas']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VisualizaÃ§Ã£o final dos resultados\n",
        "def criar_dashboard_chunks(resultado):\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    chunks = resultado['chunks']\n",
        "    stats = resultado['estatisticas']\n",
        "    \n",
        "    # GrÃ¡fico 1: Tamanho dos chunks\n",
        "    tamanhos = [len(chunk) for chunk in chunks]\n",
        "    ax1.bar(range(len(tamanhos)), tamanhos, color='skyblue', alpha=0.7)\n",
        "    ax1.set_title('ğŸ“Š Tamanho dos Chunks')\n",
        "    ax1.set_xlabel('Chunk #')\n",
        "    ax1.set_ylabel('Caracteres')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # GrÃ¡fico 2: Histograma de tamanhos\n",
        "    ax2.hist(tamanhos, bins=10, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    ax2.set_title('ğŸ“ˆ DistribuiÃ§Ã£o de Tamanhos')\n",
        "    ax2.set_xlabel('Tamanho (caracteres)')\n",
        "    ax2.set_ylabel('FrequÃªncia')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # GrÃ¡fico 3: EstatÃ­sticas resumidas\n",
        "    stats_nomes = ['Total\\nChunks', 'Tamanho\\nMÃ©dio', 'Desvio\\nPadrÃ£o']\n",
        "    stats_valores = [stats['total_chunks'], stats['tamanho_medio'], stats['desvio_padrao']]\n",
        "    \n",
        "    cores_stats = ['#3498db', '#2ecc71', '#f39c12']\n",
        "    ax3.bar(stats_nomes, stats_valores, color=cores_stats, alpha=0.8)\n",
        "    ax3.set_title('ğŸ“‹ EstatÃ­sticas Resumidas')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # GrÃ¡fico 4: Qualidade dos chunks\n",
        "    labels = ['Chunks\\nPequenos', 'Chunks\\nIdeais', 'Chunks\\nGrandes']\n",
        "    pequenos = sum(1 for t in tamanhos if t < 150)\n",
        "    ideais = sum(1 for t in tamanhos if 150 <= t <= 400)\n",
        "    grandes = sum(1 for t in tamanhos if t > 400)\n",
        "    \n",
        "    valores_qualidade = [pequenos, ideais, grandes]\n",
        "    cores_qualidade = ['#e74c3c', '#2ecc71', '#f39c12']\n",
        "    \n",
        "    ax4.pie(valores_qualidade, labels=labels, colors=cores_qualidade, autopct='%1.1f%%')\n",
        "    ax4.set_title('ğŸ¯ Qualidade dos Chunks')\n",
        "    \n",
        "    plt.suptitle(f'ğŸ“š Dashboard - Document Processing ({resultado[\"tipo\"].upper()})', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'qualidade_score': (ideais / len(chunks)) * 100,\n",
        "        'uniformidade_score': 100 - (stats['desvio_padrao'] / stats['tamanho_medio']) * 100\n",
        "    }\n",
        "\n",
        "# Criando o dashboard\n",
        "scores = criar_dashboard_chunks(resultado)\n",
        "\n",
        "print(f\"\\nğŸ† SCORES DE QUALIDADE:\")\n",
        "print(f\"ğŸ“Š Qualidade dos Chunks: {scores['qualidade_score']:.1f}%\")\n",
        "print(f\"ğŸ“ Uniformidade: {scores['uniformidade_score']:.1f}%\")\n",
        "print(f\"\\nğŸ‰ Processamento concluÃ­do com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Resumo do MÃ³dulo: O que Aprendemos Hoje?\n\n",
        "**ParabÃ©ns!** VocÃª completou o MÃ³dulo 6 e agora Ã© um expert em Document Loading e Splitters! ğŸ‰\n\n",
        "### ğŸ† Conquistas Desbloqueadas:\n",
        "- âœ… **Document Loaders**: Aprendeu a carregar TXT, CSV, PDFs e pÃ¡ginas web\n",
        "- âœ… **Text Splitters**: Dominou 3 estratÃ©gias diferentes de divisÃ£o de texto\n",
        "- âœ… **Chunk Optimization**: Sabe otimizar chunks para diferentes casos de uso\n",
        "- âœ… **Workflow Completo**: Integrou loading + splitting em um processo eficiente\n",
        "- âœ… **PreparaÃ§Ã£o para RAG**: Chunks prontos para o prÃ³ximo mÃ³dulo!\n\n",
        "### ğŸ§  Conceitos-Chave Dominados:\n",
        "1. **Document** = `page_content` + `metadata`\n",
        "2. **RecursiveCharacterTextSplitter** = Mais inteligente\n",
        "3. **CharacterTextSplitter** = Mais simples\n",
        "4. **TokenTextSplitter** = Mais preciso para IA\n",
        "5. **Chunk Overlap** = MantÃ©m contexto entre pedaÃ§os\n",
        "\n",
        "### ğŸ”® PrÃ³ximos Passos (MÃ³dulo 7):\n",
        "- ğŸ§® **Embeddings**: Transformar chunks em vetores semÃ¢nticos\n",
        "- ğŸ—„ï¸ **Vector Stores**: Armazenar e buscar informaÃ§Ãµes similares\n",
        "- ğŸ¤– **RAG**: Criar IA com conhecimento especÃ­fico dos seus documentos\n",
        "\n",
        "**Dica do Pedro**: Agora vocÃª tem a \"matÃ©ria-prima\" (chunks) perfeitamente preparada. No prÃ³ximo mÃ³dulo vamos transformar essa matÃ©ria-prima em \"conhecimento searchÃ¡vel\" para nossa IA! ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Certificado de conclusÃ£o do mÃ³dulo! ğŸ†\n",
        "print(\"ğŸ“\" * 50)\n",
        "print(\"ğŸ“\" + \" \" * 48 + \"ğŸ“\")\n",
        "print(\"ğŸ“\" + \" \" * 15 + \"CERTIFICADO\" + \" \" * 15 + \"ğŸ“\")\n",
        "print(\"ğŸ“\" + \" \" * 48 + \"ğŸ“\")\n",
        "print(\"ğŸ“  MÃ³dulo 6: Document Loading e Splitters      ğŸ“\")\n",
        "print(\"ğŸ“\" + \" \" * 48 + \"ğŸ“\")\n",
        "print(\"ğŸ“  âœ… Document Loaders - DOMINADO             ğŸ“\")\n",
        "print(\"ğŸ“  âœ… Text Splitters - DOMINADO               ğŸ“\")\n",
        "print(\"ğŸ“  âœ… Chunk Optimization - DOMINADO           ğŸ“\")\n",
        "print(\"ğŸ“  âœ… Workflow Completo - DOMINADO            ğŸ“\")\n",
        "print(\"ğŸ“\" + \" \" * 48 + \"ğŸ“\")\n",
        "print(\"ğŸ“  ğŸš€ PRONTO PARA O MÃ“DULO 7: RAG!            ğŸ“\")\n",
        "print(\"ğŸ“\" + \" \" * 48 + \"ğŸ“\")\n",
        "print(\"ğŸ“\" * 50)\n",
        "\n",
        "print(\"\\nğŸ‰ ParabÃ©ns! VocÃª estÃ¡ cada vez mais perto de dominar o LangChain!\")\n",
        "print(\"ğŸ“š No prÃ³ximo mÃ³dulo: Vector Stores, Embeddings e RAG Implementation!\")\n",
        "print(\"ğŸ¤– Bora transformar esses chunks em conhecimento inteligente!\")\n",
        "\n",
        "# Preparando dados para o prÃ³ximo mÃ³dulo\n",
        "print(\"\\nğŸ’¾ Salvando progresso para o prÃ³ximo mÃ³dulo...\")\n",
        "progresso_modulo6 = {\n",
        "    'chunks_processados': len(chunks_finais),\n",
        "    'estrategias_testadas': 3,\n",
        "    'exercicios_concluidos': 2,\n",
        "    'status': 'CONCLUÃDO COM SUCESSO! ğŸ‰'\n",
        "}\n",
        "\n",
        "print(f\"âœ… Progresso salvo: {progresso_modulo6}\")\n",
        "print(\"\\nğŸš€ Nos vemos no MÃ³dulo 7! Bora fazer RAG de verdade!\")"
      ]
    }
  ]
}