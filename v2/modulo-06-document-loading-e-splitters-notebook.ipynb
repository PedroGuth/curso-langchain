{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📚 Document Loading e Splitters: Transformando Documentos em Conhecimento Útil para IA\n\n",
        "## Módulo 6 - LangChain v0.2 🚀\n\n",
        "---\n\n",
        "**Eaí, galera!** Chegamos no módulo 6 do nosso curso de LangChain e agora vamos falar sobre uma das partes mais importantes quando trabalhamos com IA: **como carregar e dividir documentos**!\n\n",
        "Já vimos nos módulos anteriores como usar ChatModels, Prompt Templates, Chains e Memory Systems. Agora vamos aprender como alimentar nossa IA com conhecimento de documentos reais - PDFs, textos, páginas web e muito mais!\n\n",
        "**Bora entender como transformar qualquer documento em conhecimento útil para nossa IA!** 🤖📖"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 O que vamos aprender hoje?\n\n",
        "- ✅ **Document Loaders**: Como carregar diferentes tipos de documentos\n",
        "- ✅ **Text Splitters**: Como dividir textos grandes em pedaços úteis\n",
        "- ✅ **Estratégias de chunking**: Diferentes formas de dividir textos\n",
        "- ✅ **Preparação para RAG**: Como isso se conecta com o que vem pela frente\n",
        "- ✅ **Casos práticos**: PDFs, websites, CSVs e mais!\n\n",
        "**Dica do Pedro**: Pense nos Document Loaders como \"garçons\" que trazem a comida (documentos) da cozinha, e os Splitters como \"chefs\" que cortam tudo em pedaços do tamanho certo para você comer (processar)! 🍽️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos instalar as dependências necessárias\n",
        "!pip install langchain langchain-community langchain-google-genai\n",
        "!pip install pypdf python-docx beautifulsoup4 requests\n",
        "!pip install matplotlib seaborn pandas numpy\n",
        "\n",
        "print(\"📦 Todas as dependências instaladas com sucesso!\")\n",
        "print(\"🚀 Bora começar nossa jornada pelos Document Loaders!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports essenciais para nosso workshop\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# LangChain imports - os protagonistas de hoje!\n",
        "from langchain.document_loaders import (\n",
        "    TextLoader,\n",
        "    PyPDFLoader,\n",
        "    WebBaseLoader,\n",
        "    CSVLoader\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter\n",
        ")\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "print(\"🎉 Imports carregados! Vamos começar a brincadeira!\")\n",
        "print(\"📚 Hoje vamos transformar documentos em conhecimento útil para IA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Tá, mas o que são Document Loaders?\n\n",
        "**Document Loaders** são como \"tradutores universais\" que conseguem ler qualquer tipo de documento e transformar em um formato que o LangChain entende.\n\n",
        "### Analogia do Açougue 🥩\n",
        "Imagina que você vai no açougue e pede:\n",
        "- **Carne bovina** (PDF)\n",
        "- **Frango** (TXT)\n",
        "- **Peixe** (CSV)\n",
        "- **Camarão** (Website)\n\n",
        "O açougueiro (Document Loader) pega todos esses diferentes tipos de \"proteína\" e entrega tudo **embalado da mesma forma** para você levar para casa!\n\n",
        "### Estrutura de um Document no LangChain\n",
        "Todo documento carregado vira um objeto `Document` com:\n",
        "- **page_content**: O texto em si\n",
        "- **metadata**: Informações extras (nome do arquivo, página, etc.)\n\n",
        "```python\n",
        "Document(\n",
        "    page_content=\"Aqui fica o texto do documento...\",\n",
        "    metadata={\"source\": \"arquivo.pdf\", \"page\": 1}\n",
        ")\n",
        "```\n\n",
        "**Dica do Pedro**: É como se cada documento virasse uma \"ficha\" padronizada, independente de onde veio! 📄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar alguns documentos de exemplo para brincar\n",
        "import tempfile\n",
        "\n",
        "# Criando um arquivo de texto de exemplo\n",
        "texto_exemplo = \"\"\"\n",
        "LangChain é uma framework incrível para desenvolvimento de aplicações com IA.\n",
        "Ela permite integrar modelos de linguagem com diversas fontes de dados.\n",
        "Com Document Loaders, podemos carregar informações de PDFs, websites, \n",
        "bancos de dados e muito mais!\n",
        "\n",
        "No Brasil, o uso de IA está crescendo exponencialmente.\n",
        "Empresas de todos os tamanhos estão adotando soluções inteligentes.\n",
        "O LangChain facilita muito esse processo de implementação.\n",
        "\"\"\"\n",
        "\n",
        "# Salvando em um arquivo temporário\n",
        "with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
        "    f.write(texto_exemplo)\n",
        "    arquivo_texto = f.name\n",
        "\n",
        "print(f\"📝 Arquivo de exemplo criado: {arquivo_texto}\")\n",
        "print(\"🎯 Agora vamos carregar este arquivo usando TextLoader!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nosso primeiro Document Loader em ação!\n",
        "loader = TextLoader(arquivo_texto, encoding='utf-8')\n",
        "documentos = loader.load()\n",
        "\n",
        "print(\"🎉 Documento carregado com sucesso!\")\n",
        "print(f\"📊 Número de documentos: {len(documentos)}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📄 CONTEÚDO DO DOCUMENTO:\")\n",
        "print(\"=\"*50)\n",
        "print(documentos[0].page_content)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🏷️ METADATA:\")\n",
        "print(\"=\"*50)\n",
        "print(documentos[0].metadata)\n",
        "\n",
        "# Limpeza\n",
        "os.unlink(arquivo_texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🌐 Carregando Conteúdo da Web\n\n",
        "Agora vamos ver algo **liiindo**: carregar conteúdo diretamente de websites! \n",
        "\n",
        "O `WebBaseLoader` é como ter um **\"assistente virtual\"** que vai lá no site, lê tudo e traz o conteúdo organizadinho para você!\n\n",
        "### Como funciona por baixo dos panos:\n",
        "1. 🌐 Faz uma requisição HTTP para a URL\n",
        "2. 🧹 Usa BeautifulSoup para \"limpar\" o HTML\n",
        "3. 📝 Extrai apenas o texto útil\n",
        "4. 📦 Empacota tudo em um Document\n\n",
        "**Dica do Pedro**: É como ter um estagiário que vai na biblioteca, lê o livro inteiro e faz um resumo organizado para você! 📚👨‍🎓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando conteúdo de uma página web\n",
        "# Vamos usar uma página do Wikipedia sobre IA\n",
        "url = \"https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial\"\n",
        "\n",
        "try:\n",
        "    web_loader = WebBaseLoader(url)\n",
        "    doc_web = web_loader.load()\n",
        "    \n",
        "    print(\"🌐 Página web carregada com sucesso!\")\n",
        "    print(f\"📊 Número de documentos: {len(doc_web)}\")\n",
        "    print(f\"📏 Tamanho do conteúdo: {len(doc_web[0].page_content)} caracteres\")\n",
        "    \n",
        "    # Mostrando os primeiros 500 caracteres\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📄 PREVIEW DO CONTEÚDO:\")\n",
        "    print(\"=\"*50)\n",
        "    print(doc_web[0].page_content[:500] + \"...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"🏷️ METADATA:\")\n",
        "    print(\"=\"*30)\n",
        "    print(doc_web[0].metadata)\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"❌ Erro ao carregar página: {e}\")\n",
        "    print(\"🔄 Vamos continuar com um exemplo simulado...\")\n",
        "    \n",
        "    # Criando um documento simulado caso não consiga acessar a web\n",
        "    doc_web = [Document(\n",
        "        page_content=\"Inteligência artificial é a simulação de processos de inteligência humana por máquinas, especialmente sistemas de computador. Estes processos incluem aprendizado, raciocínio e autocorreção.\",\n",
        "        metadata={\"source\": url, \"title\": \"Inteligência Artificial\"}\n",
        "    )]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar e carregar um CSV de exemplo\n",
        "import csv\n",
        "\n",
        "# Dados de exemplo sobre vendas de IA no Brasil\n",
        "dados_csv = [\n",
        "    [\"Empresa\", \"Produto\", \"Vendas_2023\", \"Região\"],\n",
        "    [\"TechAI\", \"Chatbot Corporativo\", \"1500000\", \"São Paulo\"],\n",
        "    [\"BrasilBot\", \"Assistente Virtual\", \"800000\", \"Rio de Janeiro\"],\n",
        "    [\"SmartSul\", \"IA para Vendas\", \"1200000\", \"Porto Alegre\"],\n",
        "    [\"NordesteAI\", \"Automação Industrial\", \"950000\", \"Recife\"],\n",
        "    [\"CentroIA\", \"Análise Preditiva\", \"600000\", \"Brasília\"]\n",
        "]\n",
        "\n",
        "# Criando arquivo CSV temporário\n",
        "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(dados_csv)\n",
        "    arquivo_csv = f.name\n",
        "\n",
        "print(f\"📊 Arquivo CSV criado: {arquivo_csv}\")\n",
        "\n",
        "# Carregando o CSV\n",
        "csv_loader = CSVLoader(arquivo_csv, encoding='utf-8')\n",
        "docs_csv = csv_loader.load()\n",
        "\n",
        "print(f\"\\n✅ CSV carregado com sucesso!\")\n",
        "print(f\"📈 Número de documentos (linhas): {len(docs_csv)}\")\n",
        "\n",
        "# Mostrando os primeiros documentos\n",
        "for i, doc in enumerate(docs_csv[:3]):\n",
        "    print(f\"\\n📋 DOCUMENTO {i+1}:\")\n",
        "    print(f\"Conteúdo: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "# Limpeza\n",
        "os.unlink(arquivo_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✂️ Text Splitters: A Arte de Dividir Textos\n\n",
        "Agora vem a parte **mais importante**: os **Text Splitters**! 🎯\n\n",
        "### Por que precisamos dividir textos?\n",
        "\n",
        "Imagina que você tem um livro de 500 páginas sobre IA e quer que o ChatGPT responda perguntas sobre ele. O problema é:\n",
        "- 🧠 **Modelos de IA têm limite de tokens** (como ter uma \"boca pequena\")\n",
        "- 🎯 **Textos muito grandes perdem foco** (é difícil achar informação específica)\n",
        "- ⚡ **Processamento fica lento** (como mastigar um sanduíche gigante)\n\n",
        "### Analogia da Pizza 🍕\n",
        "Os Text Splitters são como **cortar uma pizza gigante**:\n",
        "- **RecursiveCharacterTextSplitter**: Corta seguindo as \"divisões naturais\" (fatias iguais respeitando os ingredientes)\n",
        "- **CharacterTextSplitter**: Corta em um ponto específico (como cortar sempre na borda do pepperoni)\n",
        "- **TokenTextSplitter**: Conta cada \"grão de queijo\" e corta quando chegar no limite\n\n",
        "### Parâmetros Importantes:\n",
        "- **chunk_size**: Tamanho de cada \"fatia\"\n",
        "- **chunk_overlap**: Quantos \"ingredientes\" ficam repetidos entre fatias (para não perder contexto)\n",
        "\n",
        "**Dica do Pedro**: O segredo está no **overlap**! É como deixar um pedacinho da fatia anterior na próxima para manter o \"sabor\" da conversa! 🧠✨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um texto longo para testar nossos splitters\n",
        "texto_longo = \"\"\"\n",
        "A Inteligência Artificial no Brasil está passando por uma revolução extraordinária. \n",
        "Empresas de todos os setores estão adotando soluções baseadas em IA para otimizar processos, \n",
        "melhorar a experiência do cliente e aumentar a competitividade.\n",
        "\n",
        "No setor financeiro, bancos como Itaú, Bradesco e Nubank utilizam algoritmos de machine learning \n",
        "para análise de crédito, detecção de fraudes e personalização de produtos. \n",
        "Essas tecnologias permitem decisões mais rápidas e precisas.\n",
        "\n",
        "O varejo brasileiro também abraçou a IA. Empresas como Magazine Luiza e Via Varejo \n",
        "implementaram chatbots inteligentes, sistemas de recomendação e análise preditiva de demanda. \n",
        "Isso resulta em melhor gestão de estoque e experiências de compra mais personalizadas.\n",
        "\n",
        "Na área da saúde, startups brasileiras desenvolvem soluções inovadoras. \n",
        "Sistemas de diagnóstico por imagem, análise de exames laboratoriais e \n",
        "monitoramento de pacientes usando IA estão revolucionando o atendimento médico.\n",
        "\n",
        "O agronegócio, setor vital da economia brasileira, também se beneficia enormemente da IA. \n",
        "Análise de imagens de satélite, previsão climática, otimização de plantio e \n",
        "monitoramento de pragas são apenas alguns exemplos de aplicação.\n",
        "\n",
        "LangChain facilita muito o desenvolvimento dessas soluções ao fornecer \n",
        "ferramentas padronizadas para integração de modelos de linguagem com dados empresariais.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"📝 Texto criado com {len(texto_longo)} caracteres\")\n",
        "print(f\"📊 Aproximadamente {len(texto_longo.split())} palavras\")\n",
        "print(\"\\n🎯 Agora vamos ver diferentes formas de dividir este texto!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RecursiveCharacterTextSplitter - O mais inteligente!\n",
        "splitter_recursivo = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,  # Cada pedaço terá até 300 caracteres\n",
        "    chunk_overlap=50,  # 50 caracteres de sobreposição\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Ordem de preferência para dividir\n",
        ")\n",
        "\n",
        "chunks_recursivo = splitter_recursivo.split_text(texto_longo)\n",
        "\n",
        "print(\"🧠 RECURSIVE CHARACTER TEXT SPLITTER\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📊 Número de chunks: {len(chunks_recursivo)}\")\n",
        "\n",
        "for i, chunk in enumerate(chunks_recursivo):\n",
        "    print(f\"\\n📄 CHUNK {i+1} ({len(chunk)} caracteres):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(chunk.strip())\n",
        "    if i >= 2:  # Mostrando apenas os primeiros 3\n",
        "        print(f\"\\n... (e mais {len(chunks_recursivo)-3} chunks)\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos comparar com Character TextSplitter\n",
        "splitter_simples = CharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\\n\"  # Só divide em parágrafos\n",
        ")\n",
        "\n",
        "chunks_simples = splitter_simples.split_text(texto_longo)\n",
        "\n",
        "print(\"⚡ CHARACTER TEXT SPLITTER\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📊 Número de chunks: {len(chunks_simples)}\")\n",
        "\n",
        "for i, chunk in enumerate(chunks_simples[:2]):\n",
        "    print(f\"\\n📄 CHUNK {i+1} ({len(chunk)} caracteres):\")\n",
        "    print(\"-\" * 30)\n",
        "    print(chunk.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando as diferenças entre os splitters\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Tamanhos dos chunks - Recursive\n",
        "tamanhos_recursivo = [len(chunk) for chunk in chunks_recursivo]\n",
        "ax1.bar(range(len(tamanhos_recursivo)), tamanhos_recursivo, \n",
        "        color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "ax1.set_title('📊 Recursive Character Text Splitter\\nTamanho dos Chunks', fontsize=12, pad=20)\n",
        "ax1.set_xlabel('Número do Chunk')\n",
        "ax1.set_ylabel('Tamanho (caracteres)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Tamanhos dos chunks - Character\n",
        "tamanhos_simples = [len(chunk) for chunk in chunks_simples]\n",
        "ax2.bar(range(len(tamanhos_simples)), tamanhos_simples, \n",
        "        color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "ax2.set_title('📊 Character Text Splitter\\nTamanho dos Chunks', fontsize=12, pad=20)\n",
        "ax2.set_xlabel('Número do Chunk')\n",
        "ax2.set_ylabel('Tamanho (caracteres)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📈 ESTATÍSTICAS:\")\n",
        "print(f\"Recursive - Chunks: {len(chunks_recursivo)}, Tamanho médio: {np.mean(tamanhos_recursivo):.1f}\")\n",
        "print(f\"Character - Chunks: {len(chunks_simples)}, Tamanho médio: {np.mean(tamanhos_simples):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧮 Token Text Splitter: Contando Cada \"Palavrinha\"\n\n",
        "O **TokenTextSplitter** é o mais preciso de todos! Ele conta **tokens** (as \"palavrinhas\" que a IA entende) em vez de caracteres.\n\n",
        "### Por que tokens são importantes?\n",
        "- 🧠 **Modelos de IA cobram por token** (como táxi que cobra por quilômetro)\n",
        "- 📏 **Cada modelo tem limite de tokens** (como elevador com limite de peso)\n",
        "- 🎯 **Token é a \"moeda\" da IA** (1 token ≈ 0.75 palavras em inglês)\n\n",
        "### Regra de ouro:\n",
        "- **1 token** ≈ 4 caracteres em inglês\n",
        "- **1 token** ≈ 0.75 palavras\n",
        "- **Em português pode variar** (palavras maiores = mais tokens)\n\n",
        "**Dica do Pedro**: É como comprar carne no açougue - você não paga pelo tamanho da bandeja, mas pelo peso real da carne! 🥩⚖️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos tentar usar o TokenTextSplitter\n",
        "# Nota: Pode precisar de configuração adicional dependendo do tokenizer\n",
        "\n",
        "try:\n",
        "    # Para usar tiktoken (tokenizer do OpenAI)\n",
        "    !pip install tiktoken -q\n",
        "    \n",
        "    from langchain.text_splitter import TokenTextSplitter\n",
        "    \n",
        "    token_splitter = TokenTextSplitter(\n",
        "        chunk_size=100,  # 100 tokens por chunk\n",
        "        chunk_overlap=20   # 20 tokens de overlap\n",
        "    )\n",
        "    \n",
        "    chunks_token = token_splitter.split_text(texto_longo)\n",
        "    \n",
        "    print(\"🧮 TOKEN TEXT SPLITTER\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"📊 Número de chunks: {len(chunks_token)}\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks_token[:3]):\n",
        "        # Estimativa de tokens (aproximada)\n",
        "        tokens_aprox = len(chunk) // 4\n",
        "        print(f\"\\n🎯 CHUNK {i+1} (~{tokens_aprox} tokens, {len(chunk)} caracteres):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(chunk.strip())\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"⚠️ TokenTextSplitter precisa do tiktoken instalado\")\n",
        "    print(\"📝 Vamos simular o comportamento...\")\n",
        "    \n",
        "    # Simulação simples baseada em palavras\n",
        "    palavras = texto_longo.split()\n",
        "    chunk_size_palavras = 75  # Aproximadamente 100 tokens\n",
        "    overlap_palavras = 15     # Aproximadamente 20 tokens\n",
        "    \n",
        "    chunks_token = []\n",
        "    for i in range(0, len(palavras), chunk_size_palavras - overlap_palavras):\n",
        "        chunk_palavras = palavras[i:i + chunk_size_palavras]\n",
        "        chunks_token.append(\" \".join(chunk_palavras))\n",
        "    \n",
        "    print(\"🧮 TOKEN TEXT SPLITTER (Simulado)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"📊 Número de chunks: {len(chunks_token)}\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks_token[:2]):\n",
        "        tokens_aprox = len(chunk.split())\n",
        "        print(f\"\\n🎯 CHUNK {i+1} (~{tokens_aprox} palavras/tokens):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando todos os splitters em um gráfico épico!\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Dados para comparação\n",
        "splitters_nomes = ['Recursive\\nCharacter', 'Character', 'Token\\n(Simulado)']\n",
        "num_chunks = [len(chunks_recursivo), len(chunks_simples), len(chunks_token)]\n",
        "cores = ['#3498db', '#e74c3c', '#f39c12']\n",
        "\n",
        "# Criando o gráfico de barras\n",
        "barras = ax.bar(splitters_nomes, num_chunks, color=cores, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# Adicionando valores nas barras\n",
        "for barra, valor in zip(barras, num_chunks):\n",
        "    altura = barra.get_height()\n",
        "    ax.text(barra.get_x() + barra.get_width()/2., altura + 0.1,\n",
        "            f'{valor} chunks', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "ax.set_title('🥊 BATALHA DOS TEXT SPLITTERS\\nNúmero de Chunks Gerados', \n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('Número de Chunks', fontsize=12)\n",
        "ax.set_xlabel('Tipo de Splitter', fontsize=12)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Deixando o gráfico mais bonito\n",
        "ax.set_ylim(0, max(num_chunks) * 1.2)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🏆 RESULTADO DA BATALHA:\")\n",
        "print(f\"🥇 Recursive Character: {len(chunks_recursivo)} chunks (Mais inteligente!)\")\n",
        "print(f\"🥈 Character: {len(chunks_simples)} chunks (Mais simples)\")\n",
        "print(f\"🥉 Token: {len(chunks_token)} chunks (Mais preciso para IA!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 Workflow Completo: Loader + Splitter\n\n",
        "Agora vamos juntar tudo que aprendemos! **Document Loader + Text Splitter** = Combo perfeito! 🎯\n\n",
        "### O Fluxo Completo:\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[📄 Documento Original] --> B[🔄 Document Loader]\n",
        "    B --> C[📝 Document Object]\n",
        "    C --> D[✂️ Text Splitter]\n",
        "    D --> E[📚 Chunks Menores]\n",
        "    E --> F[🤖 Pronto para IA!]\n",
        "```\n\n",
        "### Por que esse workflow é importante?\n",
        "- 🎯 **Prepara dados para RAG** (próximo módulo!)\n",
        "- ⚡ **Otimiza performance** da IA\n",
        "- 💰 **Economiza tokens** (e dinheiro!)\n",
        "- 🧠 **Mantém contexto** com overlap\n\n",
        "**Dica do Pedro**: É como preparar ingredientes antes de cozinhar - quanto melhor a preparação, melhor o resultado final! 👨‍🍳✨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow completo: Criando documentos, carregando e dividindo\n",
        "def processar_documento_completo(texto, chunk_size=200, chunk_overlap=30):\n",
        "    \"\"\"\n",
        "    Função que simula o workflow completo de processamento de documentos\n",
        "    \"\"\"\n",
        "    print(\"🚀 INICIANDO WORKFLOW COMPLETO\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Etapa 1: Simular Document Loader\n",
        "    print(\"📄 Etapa 1: Carregando documento...\")\n",
        "    documento = Document(\n",
        "        page_content=texto,\n",
        "        metadata={\"source\": \"documento_exemplo.txt\", \"tipo\": \"texto\"}\n",
        "    )\n",
        "    print(f\"✅ Documento carregado: {len(documento.page_content)} caracteres\")\n",
        "    \n",
        "    # Etapa 2: Text Splitter\n",
        "    print(\"\\n✂️ Etapa 2: Dividindo em chunks...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    \n",
        "    # Dividindo o documento\n",
        "    chunks = splitter.split_documents([documento])\n",
        "    print(f\"✅ Documento dividido em {len(chunks)} chunks\")\n",
        "    \n",
        "    # Etapa 3: Análise dos chunks\n",
        "    print(\"\\n📊 Etapa 3: Analisando resultados...\")\n",
        "    tamanhos = [len(chunk.page_content) for chunk in chunks]\n",
        "    \n",
        "    print(f\"📈 Tamanho médio dos chunks: {np.mean(tamanhos):.1f} caracteres\")\n",
        "    print(f\"📏 Menor chunk: {min(tamanhos)} caracteres\")\n",
        "    print(f\"📏 Maior chunk: {max(tamanhos)} caracteres\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Testando o workflow\n",
        "chunks_finais = processar_documento_completo(texto_longo, chunk_size=250, chunk_overlap=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizando os chunks finais\n",
        "print(\"🎯 CHUNKS FINAIS PROCESSADOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, chunk in enumerate(chunks_finais):\n",
        "    print(f\"\\n📄 CHUNK {i+1}\")\n",
        "    print(f\"📊 Tamanho: {len(chunk.page_content)} caracteres\")\n",
        "    print(f\"🏷️ Metadata: {chunk.metadata}\")\n",
        "    print(f\"📝 Conteúdo:\")\n",
        "    print(\"-\" * 40)\n",
        "    # Mostrando apenas os primeiros 150 caracteres para não poluir\n",
        "    preview = chunk.page_content.strip()[:150]\n",
        "    print(preview + \"...\" if len(chunk.page_content) > 150 else preview)\n",
        "    \n",
        "    if i >= 3:  # Limitando a exibição\n",
        "        print(f\"\\n... (e mais {len(chunks_finais)-4} chunks)\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n🎉 Processo concluído! {len(chunks_finais)} chunks prontos para uso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎮 EXERCÍCIO PRÁTICO 1: Criando seu Document Processor\n\n",
        "**Hora de colocar a mão na massa!** 💪\n\n",
        "### 🎯 Seu Desafio:\n",
        "Crie uma função que:\n",
        "1. Recebe um texto longo\n",
        "2. Testa 3 estratégias diferentes de splitting\n",
        "3. Compara os resultados\n",
        "4. Retorna a melhor estratégia\n",
        "\n",
        "### 📋 Critérios de \"melhor estratégia\":\n",
        "- Chunks com tamanhos mais uniformes\n",
        "- Menor número total de chunks\n",
        "- Melhor preservação de contexto\n",
        "\n",
        "**Dica do Pedro**: Pense como um chef que testa diferentes formas de cortar os ingredientes para ver qual fica melhor na receita! 👨‍🍳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 1: Complete a função abaixo\n",
        "def comparar_estrategias_splitting(texto, chunk_size=300):\n",
        "    \"\"\"\n",
        "    Compara diferentes estratégias de text splitting\n",
        "    \n",
        "    Args:\n",
        "        texto (str): Texto para dividir\n",
        "        chunk_size (int): Tamanho desejado dos chunks\n",
        "    \n",
        "    Returns:\n",
        "        dict: Resultados das comparações\n",
        "    \"\"\"\n",
        "    resultados = {}\n",
        "    \n",
        "    # TODO: Implemente 3 estratégias diferentes\n",
        "    # Estratégia 1: RecursiveCharacterTextSplitter\n",
        "    # Estratégia 2: CharacterTextSplitter  \n",
        "    # Estratégia 3: Sua escolha criativa!\n",
        "    \n",
        "    # TODO: Para cada estratégia, calcule:\n",
        "    # - Número de chunks\n",
        "    # - Tamanho médio dos chunks\n",
        "    # - Desvio padrão dos tamanhos (uniformidade)\n",
        "    \n",
        "    # TODO: Determine qual é a \"melhor\" estratégia\n",
        "    \n",
        "    # CÓDIGO DE EXEMPLO (substitua pelo seu):\n",
        "    splitter1 = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
        "    chunks1 = splitter1.split_text(texto)\n",
        "    \n",
        "    resultados['recursive'] = {\n",
        "        'chunks': len(chunks1),\n",
        "        'tamanho_medio': np.mean([len(c) for c in chunks1]),\n",
        "        'desvio_padrao': np.std([len(c) for c in chunks1])\n",
        "    }\n",
        "    \n",
        "    # COMPLETE COM AS OUTRAS ESTRATÉGIAS!\n",
        "    \n",
        "    return resultados\n",
        "\n",
        "# Teste sua função\n",
        "resultados_teste = comparar_estrategias_splitting(texto_longo)\n",
        "print(\"🧪 Resultados do seu teste:\")\n",
        "print(resultados_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Preparando para o Próximo Módulo: RAG\n\n",
        "**Liiindo!** Agora que sabemos carregar e dividir documentos, vamos entender como isso se conecta com o **RAG (Retrieval-Augmented Generation)** 🎯\n\n",
        "### O que vem pela frente no Módulo 7:\n",
        "- 🧠 **Vector Stores**: Como transformar nossos chunks em vetores\n",
        "- 🔍 **Embeddings**: A \"impressão digital\" semântica dos textos\n",
        "- 🎯 **Similarity Search**: Como achar informações relevantes\n",
        "- 🤖 **RAG Implementation**: Juntando tudo para criar IA com conhecimento específico\n\n",
        "### Como os chunks se transformam em conhecimento:\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[📚 Chunks] --> B[🧮 Embeddings]\n",
        "    B --> C[🗄️ Vector Store]\n",
        "    C --> D[🔍 Search]\n",
        "    D --> E[🤖 RAG Response]\n",
        "```\n\n",
        "**Dica do Pedro**: Os chunks que criamos hoje são como \"fichas de conhecimento\" que vamos usar para alimentar nossa IA no próximo módulo! É como criar um \"catálogo inteligente\" de informações! 📚🧠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparando nossos chunks para o próximo módulo\n",
        "def preparar_chunks_para_rag(chunks, max_chunks=10):\n",
        "    \"\"\"\n",
        "    Prepara chunks para uso em RAG, otimizando tamanho e qualidade\n",
        "    \"\"\"\n",
        "    print(\"🔄 PREPARANDO CHUNKS PARA RAG\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    # Filtrando chunks muito pequenos (pouco conteúdo útil)\n",
        "    chunks_filtrados = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 50]\n",
        "    \n",
        "    # Limitando número de chunks se necessário\n",
        "    if len(chunks_filtrados) > max_chunks:\n",
        "        chunks_filtrados = chunks_filtrados[:max_chunks]\n",
        "    \n",
        "    print(f\"✅ Chunks originais: {len(chunks)}\")\n",
        "    print(f\"✅ Chunks filtrados: {len(chunks_filtrados)}\")\n",
        "    print(f\"✅ Chunks finais: {len(chunks_filtrados)}\")\n",
        "    \n",
        "    # Estatísticas dos chunks finais\n",
        "    tamanhos = [len(chunk.page_content) for chunk in chunks_filtrados]\n",
        "    \n",
        "    print(f\"\\n📊 ESTATÍSTICAS FINAIS:\")\n",
        "    print(f\"📏 Tamanho médio: {np.mean(tamanhos):.1f} caracteres\")\n",
        "    print(f\"📐 Desvio padrão: {np.std(tamanhos):.1f}\")\n",
        "    print(f\"📈 Min/Max: {min(tamanhos)}/{max(tamanhos)} caracteres\")\n",
        "    \n",
        "    return chunks_filtrados\n",
        "\n",
        "# Preparando para RAG\n",
        "chunks_para_rag = preparar_chunks_para_rag(chunks_finais)\n",
        "\n",
        "print(\"\\n🎯 Chunks prontos para o Módulo 7 - Vector Stores e RAG!\")\n",
        "print(\"No próximo módulo vamos transformar estes chunks em vetores semânticos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎮 EXERCÍCIO PRÁTICO 2: Document Processor Avançado\n\n",
        "**Último desafio do módulo!** 🏆\n\n",
        "### 🎯 Missão Impossível:\n",
        "Crie um **Document Processor Universal** que:\n",
        "1. Detecta automaticamente o tipo de conteúdo\n",
        "2. Escolhe a estratégia de splitting ideal\n",
        "3. Otimiza os chunks para diferentes casos de uso\n",
        "4. Gera relatório de qualidade\n",
        "\n",
        "### 📋 Especificações:\n",
        "- **Entrada**: Texto de qualquer tipo\n",
        "- **Saída**: Chunks otimizados + relatório\n",
        "- **Bonus**: Visualização dos resultados\n",
        "\n",
        "**Dica do Pedro**: Pense como um \"sommelier de chunks\" - cada tipo de texto precisa de um tratamento especial! 🍷"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: Document Processor Avançado\n",
        "class DocumentProcessorAvancado:\n",
        "    def __init__(self):\n",
        "        self.estrategias = {\n",
        "            'narrativo': {'chunk_size': 400, 'overlap': 60},\n",
        "            'tecnico': {'chunk_size': 300, 'overlap': 50},\n",
        "            'listagem': {'chunk_size': 200, 'overlap': 30}\n",
        "        }\n",
        "    \n",
        "    def detectar_tipo_conteudo(self, texto):\n",
        "        \"\"\"\n",
        "        Detecta o tipo de conteúdo do texto\n",
        "        TODO: Implemente a lógica de detecção\n",
        "        \"\"\"\n",
        "        # DICAS:\n",
        "        # - Conte parágrafos longos vs curtos\n",
        "        # - Procure por listas (-, *, números)\n",
        "        # - Analise vocabulário técnico\n",
        "        \n",
        "        # IMPLEMENTAÇÃO SIMPLES (você pode melhorar!):\n",
        "        paragrafos = texto.split('\\n\\n')\n",
        "        tamanho_medio_paragrafo = np.mean([len(p) for p in paragrafos if p.strip()])\n",
        "        \n",
        "        if tamanho_medio_paragrafo > 300:\n",
        "            return 'narrativo'\n",
        "        elif any(palavra in texto.lower() for palavra in ['sistema', 'algoritmo', 'função', 'tecnologia']):\n",
        "            return 'tecnico'\n",
        "        else:\n",
        "            return 'listagem'\n",
        "    \n",
        "    def processar(self, texto):\n",
        "        \"\"\"\n",
        "        Processa o documento completo\n",
        "        TODO: Complete a implementação\n",
        "        \"\"\"\n",
        "        print(\"🤖 DOCUMENT PROCESSOR AVANÇADO\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Etapa 1: Detectar tipo\n",
        "        tipo = self.detectar_tipo_conteudo(texto)\n",
        "        print(f\"🔍 Tipo detectado: {tipo}\")\n",
        "        \n",
        "        # Etapa 2: Aplicar estratégia\n",
        "        config = self.estrategias[tipo]\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config['chunk_size'],\n",
        "            chunk_overlap=config['overlap']\n",
        "        )\n",
        "        \n",
        "        chunks = splitter.split_text(texto)\n",
        "        print(f\"✂️ Chunks gerados: {len(chunks)}\")\n",
        "        \n",
        "        # TODO: Adicione mais análises e otimizações\n",
        "        \n",
        "        return {\n",
        "            'tipo': tipo,\n",
        "            'chunks': chunks,\n",
        "            'config_usada': config,\n",
        "            'estatisticas': self._gerar_estatisticas(chunks)\n",
        "        }\n",
        "    \n",
        "    def _gerar_estatisticas(self, chunks):\n",
        "        tamanhos = [len(chunk) for chunk in chunks]\n",
        "        return {\n",
        "            'total_chunks': len(chunks),\n",
        "            'tamanho_medio': np.mean(tamanhos),\n",
        "            'desvio_padrao': np.std(tamanhos),\n",
        "            'min_max': (min(tamanhos), max(tamanhos))\n",
        "        }\n",
        "\n",
        "# Testando o processor avançado\n",
        "processor = DocumentProcessorAvancado()\n",
        "resultado = processor.processar(texto_longo)\n",
        "\n",
        "print(f\"\\n📊 RELATÓRIO FINAL:\")\n",
        "print(f\"Tipo: {resultado['tipo']}\")\n",
        "print(f\"Estatísticas: {resultado['estatisticas']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualização final dos resultados\n",
        "def criar_dashboard_chunks(resultado):\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    chunks = resultado['chunks']\n",
        "    stats = resultado['estatisticas']\n",
        "    \n",
        "    # Gráfico 1: Tamanho dos chunks\n",
        "    tamanhos = [len(chunk) for chunk in chunks]\n",
        "    ax1.bar(range(len(tamanhos)), tamanhos, color='skyblue', alpha=0.7)\n",
        "    ax1.set_title('📊 Tamanho dos Chunks')\n",
        "    ax1.set_xlabel('Chunk #')\n",
        "    ax1.set_ylabel('Caracteres')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gráfico 2: Histograma de tamanhos\n",
        "    ax2.hist(tamanhos, bins=10, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    ax2.set_title('📈 Distribuição de Tamanhos')\n",
        "    ax2.set_xlabel('Tamanho (caracteres)')\n",
        "    ax2.set_ylabel('Frequência')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gráfico 3: Estatísticas resumidas\n",
        "    stats_nomes = ['Total\\nChunks', 'Tamanho\\nMédio', 'Desvio\\nPadrão']\n",
        "    stats_valores = [stats['total_chunks'], stats['tamanho_medio'], stats['desvio_padrao']]\n",
        "    \n",
        "    cores_stats = ['#3498db', '#2ecc71', '#f39c12']\n",
        "    ax3.bar(stats_nomes, stats_valores, color=cores_stats, alpha=0.8)\n",
        "    ax3.set_title('📋 Estatísticas Resumidas')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gráfico 4: Qualidade dos chunks\n",
        "    labels = ['Chunks\\nPequenos', 'Chunks\\nIdeais', 'Chunks\\nGrandes']\n",
        "    pequenos = sum(1 for t in tamanhos if t < 150)\n",
        "    ideais = sum(1 for t in tamanhos if 150 <= t <= 400)\n",
        "    grandes = sum(1 for t in tamanhos if t > 400)\n",
        "    \n",
        "    valores_qualidade = [pequenos, ideais, grandes]\n",
        "    cores_qualidade = ['#e74c3c', '#2ecc71', '#f39c12']\n",
        "    \n",
        "    ax4.pie(valores_qualidade, labels=labels, colors=cores_qualidade, autopct='%1.1f%%')\n",
        "    ax4.set_title('🎯 Qualidade dos Chunks')\n",
        "    \n",
        "    plt.suptitle(f'📚 Dashboard - Document Processing ({resultado[\"tipo\"].upper()})', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'qualidade_score': (ideais / len(chunks)) * 100,\n",
        "        'uniformidade_score': 100 - (stats['desvio_padrao'] / stats['tamanho_medio']) * 100\n",
        "    }\n",
        "\n",
        "# Criando o dashboard\n",
        "scores = criar_dashboard_chunks(resultado)\n",
        "\n",
        "print(f\"\\n🏆 SCORES DE QUALIDADE:\")\n",
        "print(f\"📊 Qualidade dos Chunks: {scores['qualidade_score']:.1f}%\")\n",
        "print(f\"📐 Uniformidade: {scores['uniformidade_score']:.1f}%\")\n",
        "print(f\"\\n🎉 Processamento concluído com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 Resumo do Módulo: O que Aprendemos Hoje?\n\n",
        "**Parabéns!** Você completou o Módulo 6 e agora é um expert em Document Loading e Splitters! 🎉\n\n",
        "### 🏆 Conquistas Desbloqueadas:\n",
        "- ✅ **Document Loaders**: Aprendeu a carregar TXT, CSV, PDFs e páginas web\n",
        "- ✅ **Text Splitters**: Dominou 3 estratégias diferentes de divisão de texto\n",
        "- ✅ **Chunk Optimization**: Sabe otimizar chunks para diferentes casos de uso\n",
        "- ✅ **Workflow Completo**: Integrou loading + splitting em um processo eficiente\n",
        "- ✅ **Preparação para RAG**: Chunks prontos para o próximo módulo!\n\n",
        "### 🧠 Conceitos-Chave Dominados:\n",
        "1. **Document** = `page_content` + `metadata`\n",
        "2. **RecursiveCharacterTextSplitter** = Mais inteligente\n",
        "3. **CharacterTextSplitter** = Mais simples\n",
        "4. **TokenTextSplitter** = Mais preciso para IA\n",
        "5. **Chunk Overlap** = Mantém contexto entre pedaços\n",
        "\n",
        "### 🔮 Próximos Passos (Módulo 7):\n",
        "- 🧮 **Embeddings**: Transformar chunks em vetores semânticos\n",
        "- 🗄️ **Vector Stores**: Armazenar e buscar informações similares\n",
        "- 🤖 **RAG**: Criar IA com conhecimento específico dos seus documentos\n",
        "\n",
        "**Dica do Pedro**: Agora você tem a \"matéria-prima\" (chunks) perfeitamente preparada. No próximo módulo vamos transformar essa matéria-prima em \"conhecimento searchável\" para nossa IA! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Certificado de conclusão do módulo! 🏆\n",
        "print(\"🎓\" * 50)\n",
        "print(\"🎓\" + \" \" * 48 + \"🎓\")\n",
        "print(\"🎓\" + \" \" * 15 + \"CERTIFICADO\" + \" \" * 15 + \"🎓\")\n",
        "print(\"🎓\" + \" \" * 48 + \"🎓\")\n",
        "print(\"🎓  Módulo 6: Document Loading e Splitters      🎓\")\n",
        "print(\"🎓\" + \" \" * 48 + \"🎓\")\n",
        "print(\"🎓  ✅ Document Loaders - DOMINADO             🎓\")\n",
        "print(\"🎓  ✅ Text Splitters - DOMINADO               🎓\")\n",
        "print(\"🎓  ✅ Chunk Optimization - DOMINADO           🎓\")\n",
        "print(\"🎓  ✅ Workflow Completo - DOMINADO            🎓\")\n",
        "print(\"🎓\" + \" \" * 48 + \"🎓\")\n",
        "print(\"🎓  🚀 PRONTO PARA O MÓDULO 7: RAG!            🎓\")\n",
        "print(\"🎓\" + \" \" * 48 + \"🎓\")\n",
        "print(\"🎓\" * 50)\n",
        "\n",
        "print(\"\\n🎉 Parabéns! Você está cada vez mais perto de dominar o LangChain!\")\n",
        "print(\"📚 No próximo módulo: Vector Stores, Embeddings e RAG Implementation!\")\n",
        "print(\"🤖 Bora transformar esses chunks em conhecimento inteligente!\")\n",
        "\n",
        "# Preparando dados para o próximo módulo\n",
        "print(\"\\n💾 Salvando progresso para o próximo módulo...\")\n",
        "progresso_modulo6 = {\n",
        "    'chunks_processados': len(chunks_finais),\n",
        "    'estrategias_testadas': 3,\n",
        "    'exercicios_concluidos': 2,\n",
        "    'status': 'CONCLUÍDO COM SUCESSO! 🎉'\n",
        "}\n",
        "\n",
        "print(f\"✅ Progresso salvo: {progresso_modulo6}\")\n",
        "print(\"\\n🚀 Nos vemos no Módulo 7! Bora fazer RAG de verdade!\")"
      ]
    }
  ]
}