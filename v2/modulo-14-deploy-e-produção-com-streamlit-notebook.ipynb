{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Deploy de IA com Streamlit: Da Sua M√°quina Para o Mundo!\n\n**M√≥dulo 14 - Deploy e Produ√ß√£o com Streamlit**\n\n![](/imagens/langchain-modulo-14_img_01.png)\n\nOpa! Chegamos no momento mais emocionante do curso! T√°, mas o que adianta ter criado aqueles projetos lindos com LangChain se eles ficam s√≥ na sua m√°quina, n√©?\n\n√â como ter uma receita incr√≠vel de brigadeiro mas nunca servir pra ningu√©m! Hoje vamos aprender a colocar suas aplica√ß√µes de IA no ar para todo mundo usar!\n\n## O que vamos ver hoje:\n- ‚úÖ Como transformar seus projetos LangChain em apps web\n- ‚úÖ Deploy no Streamlit Cloud (de gra√ßa!)\n- ‚úÖ Configura√ß√£o de seguran√ßa e vari√°veis de ambiente\n- ‚úÖ Monitoramento e otimiza√ß√£o\n- ‚úÖ Troubleshooting dos problemas mais comuns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Por que Streamlit?\n\nT√°, mas por que n√£o usar Flask, Django ou qualquer outro framework? Simples!\n\nStreamlit √© como aquele amigo que sempre facilita sua vida:\n- **Zero HTML/CSS/JavaScript**: S√≥ Python puro!\n- **Deploy em 5 minutos**: Literalmente!\n- **Componentes prontos**: Widgets, gr√°ficos, tudo j√° funciona\n- **Integra√ß√£o perfeita**: Com pandas, matplotlib, plotly...\n\n√â tipo a diferen√ßa entre fazer um bolo do zero vs usar uma mistura pronta. Os dois funcionam, mas um √© muito mais r√°pido!\n\n**Dica!** Streamlit foi feito pensando em cientistas de dados e engenheiros de ML. Por isso funciona t√£o bem com nossos projetos de IA!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos instalar tudo que precisamos\n",
        "!pip install streamlit langchain google-generativeai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì± Criando Nossa Primeira App\n\nBora come√ßar criando uma vers√£o web do nosso chatbot do m√≥dulo anterior. √â como transformar um WhatsApp pessoal em um site que todo mundo pode usar!\n\n```mermaid\ngraph LR\n    A[Usu√°rio] --> B[Streamlit UI]\n    B --> C[LangChain]\n    C --> D[Gemini API]\n    D --> C\n    C --> B\n    B --> A\n```\n\n**Dica!** No Colab, vamos criar os arquivos usando magic commands. Em casa, voc√™ vai criar arquivos normais mesmo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando nossa primeira app Streamlit\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Meu Chatbot IA\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# T√≠tulo da aplica√ß√£o\n",
        "st.title(\"ü§ñ Chatbot com LangChain\")\n",
        "st.write(\"Ol√°! Eu sou seu assistente de IA. Como posso ajudar?\")\n",
        "\n",
        "# Sidebar para configura√ß√µes\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    api_key = st.text_input(\"API Key do Google:\", type=\"password\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "\n",
        "# Inicializar o chat apenas se tiver API key\n",
        "if api_key:\n",
        "    # Configurar o modelo\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "    \n",
        "    # Sistema de mensagens\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    \n",
        "    # Mostrar mensagens anteriores\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "    \n",
        "    # Input do usu√°rio\n",
        "    if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "        # Adicionar mensagem do usu√°rio\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        \n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "        \n",
        "        # Gerar resposta\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Pensando...\"):\n",
        "                messages = [\n",
        "                    SystemMessage(content=\"Voc√™ √© um assistente prestativo e amig√°vel.\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "        \n",
        "        # Salvar resposta\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "else:\n",
        "    st.warning(\"Por favor, insira sua API Key do Google na barra lateral para come√ßar!\")\n",
        "    st.info(\"üí° **Dica:** Voc√™ pode obter sua API key em https://makersuite.google.com/app/apikey\")\n",
        "'''\n",
        "\n",
        "# Salvar o arquivo\n",
        "with open('app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"‚úÖ Arquivo app.py criado com sucesso!\")\n",
        "print(\"Para rodar: streamlit run app.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Gerenciando Vari√°veis de Ambiente\n\nT√°, mas deixar a API key vis√≠vel no c√≥digo √© como deixar a chave de casa debaixo do tapete com um bilhete \"chave aqui\"! üòÖ\n\nVamos usar vari√°veis de ambiente, que √© o jeito profissional de guardar informa√ß√µes sens√≠veis.\n\n**Dica!** Em produ√ß√£o, NUNCA coloque API keys diretamente no c√≥digo. √â quest√£o de seguran√ßa!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando arquivo .env para vari√°veis de ambiente\n",
        "env_content = '''\n",
        "GOOGLE_API_KEY=sua_api_key_aqui\n",
        "APP_TITLE=Meu Chatbot Incr√≠vel\n",
        "DEBUG=False\n",
        "'''\n",
        "\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"üìù Arquivo .env criado!\")\n",
        "print(\"Lembre-se de:\")\n",
        "print(\"1. Adicionar sua API key real\")\n",
        "print(\"2. Nunca commitar o .env no Git\")\n",
        "print(\"3. Criar um .env.example para outros desenvolvedores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vers√£o melhorada da nossa app com vari√°veis de ambiente\n",
        "app_secure_code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=os.getenv(\"APP_TITLE\", \"Chatbot IA\"),\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# T√≠tulo da aplica√ß√£o\n",
        "st.title(\"ü§ñ Chatbot Profissional com LangChain\")\n",
        "st.write(\"Vers√£o segura com vari√°veis de ambiente!\")\n",
        "\n",
        "# Verificar se temos API key\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"‚ùå API Key n√£o encontrada!\")\n",
        "    st.info(\"Configure a vari√°vel GOOGLE_API_KEY no arquivo .env\")\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar para configura√ß√µes\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "    modelo = st.selectbox(\n",
        "        \"Modelo\",\n",
        "        [\"gemini-2.0-flash-exp\", \"gemini-1.5-pro\"]\n",
        "    )\n",
        "    \n",
        "    if st.button(\"üóëÔ∏è Limpar Conversa\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "# Configurar o modelo\n",
        "try:\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=modelo,\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "except Exception as e:\n",
        "    st.error(f\"Erro ao configurar modelo: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usu√°rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usu√°rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            try:\n",
        "                messages = [\n",
        "                    SystemMessage(content=\"Voc√™ √© um assistente prestativo e amig√°vel.\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "                \n",
        "                # Salvar resposta\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "                \n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro ao gerar resposta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"üí° **Dica:** Esta aplica√ß√£o usa LangChain + Streamlit para m√°xima performance!\")\n",
        "'''\n",
        "\n",
        "# Salvar vers√£o segura\n",
        "with open('app_secure.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_secure_code)\n",
        "\n",
        "print(\"üîí Vers√£o segura criada: app_secure.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Arquivo requirements.txt\n\nO requirements.txt √© como uma lista de compras para o servidor. Ele diz exatamente quais bibliotecas sua aplica√ß√£o precisa para funcionar.\n\n√â tipo quando voc√™ vai fazer um bolo e anota todos os ingredientes - sem isso, o servidor n√£o vai saber o que instalar!\n\n**Dica!** Sempre fixe as vers√µes das bibliotecas cr√≠ticas para evitar surpresas em produ√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando requirements.txt\n",
        "requirements = '''\n",
        "streamlit>=1.28.0\n",
        "langchain>=0.3.0\n",
        "langchain-google-genai>=2.0.0\n",
        "python-dotenv>=1.0.0\n",
        "pandas>=2.0.0\n",
        "numpy>=1.24.0\n",
        "'''\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements.strip())\n",
        "\n",
        "print(\"üìã requirements.txt criado!\")\n",
        "\n",
        "# Vamos tamb√©m criar um arquivo de configura√ß√£o do Streamlit\n",
        "config_content = '''\n",
        "[general]\n",
        "email = \"seu-email@exemplo.com\"\n",
        "\n",
        "[server]\n",
        "headless = true\n",
        "enableCORS = false\n",
        "port = 8501\n",
        "\n",
        "[theme]\n",
        "primaryColor = \"#FF6B6B\"\n",
        "backgroundColor = \"#FFFFFF\"\n",
        "secondaryBackgroundColor = \"#F0F2F6\"\n",
        "textColor = \"#262730\"\n",
        "'''\n",
        "\n",
        "# Criar pasta .streamlit se n√£o existir\n",
        "import os\n",
        "if not os.path.exists('.streamlit'):\n",
        "    os.makedirs('.streamlit')\n",
        "\n",
        "with open('.streamlit/config.toml', 'w') as f:\n",
        "    f.write(config_content.strip())\n",
        "\n",
        "print(\"‚öôÔ∏è Configura√ß√£o do Streamlit criada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Deploy no Streamlit Cloud\n\nAgora vem a parte mais emocionante! Vamos colocar nossa aplica√ß√£o no ar de gra√ßa!\n\nO Streamlit Cloud √© como um Heroku espec√≠fico para apps Streamlit. √â gratuito, f√°cil de usar e se integra direto com o GitHub.\n\n![](/imagens/langchain-modulo-14_img_02.png)\n\n### Passo a passo:\n\n1. **Criar reposit√≥rio no GitHub**\n2. **Fazer upload dos arquivos**\n3. **Conectar no Streamlit Cloud**\n4. **Configurar vari√°veis de ambiente**\n5. **Deploy autom√°tico!**\n\n**Dica!** O deploy √© autom√°tico sempre que voc√™ fizer push no GitHub. √â deploy cont√≠nuo na veia!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um .gitignore para n√£o subir arquivos sens√≠veis\n",
        "gitignore_content = '''\n",
        "# Arquivos de ambiente\n",
        ".env\n",
        ".env.local\n",
        ".env.*.local\n",
        "\n",
        "# Cache do Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*$py.class\n",
        "*.so\n",
        "\n",
        "# Jupyter Notebook\n",
        ".ipynb_checkpoints\n",
        "\n",
        "# Streamlit\n",
        ".streamlit/secrets.toml\n",
        "\n",
        "# IDE\n",
        ".vscode/\n",
        ".idea/\n",
        "\n",
        "# OS\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "'''\n",
        "\n",
        "with open('.gitignore', 'w') as f:\n",
        "    f.write(gitignore_content.strip())\n",
        "\n",
        "print(\"üö´ .gitignore criado - seus segredos est√£o seguros!\")\n",
        "\n",
        "# Criar um README.md explicativo\n",
        "readme_content = '''\n",
        "# ü§ñ Chatbot com LangChain e Streamlit\n",
        "\n",
        "Uma aplica√ß√£o web moderna que combina o poder do LangChain com a simplicidade do Streamlit!\n",
        "\n",
        "## üöÄ Como usar\n",
        "\n",
        "1. Clone este reposit√≥rio\n",
        "2. Instale as depend√™ncias: `pip install -r requirements.txt`\n",
        "3. Configure sua API key no arquivo `.env`\n",
        "4. Execute: `streamlit run app_secure.py`\n",
        "\n",
        "## üîß Configura√ß√£o\n",
        "\n",
        "Crie um arquivo `.env` com:\n",
        "```\n",
        "GOOGLE_API_KEY=sua_api_key_aqui\n",
        "APP_TITLE=Seu T√≠tulo Aqui\n",
        "```\n",
        "\n",
        "## üì± Deploy\n",
        "\n",
        "Esta aplica√ß√£o est√° pronta para deploy no Streamlit Cloud!\n",
        "\n",
        "## üõ†Ô∏è Tecnologias\n",
        "\n",
        "- **Streamlit**: Interface web\n",
        "- **LangChain**: Framework de IA\n",
        "- **Google Gemini**: Modelo de linguagem\n",
        "- **Python**: Linguagem de programa√ß√£o\n",
        "'''\n",
        "\n",
        "with open('README.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(readme_content.strip())\n",
        "\n",
        "print(\"üìö README.md criado - documenta√ß√£o completa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Secrets no Streamlit Cloud\n\nNo Streamlit Cloud, as vari√°veis de ambiente s√£o chamadas de \"secrets\". √â como um cofre digital onde voc√™ guarda suas API keys.\n\nT√°, mas como configurar? √â simples:\n\n1. V√° no painel do Streamlit Cloud\n2. Clique em \"Settings\" da sua app\n3. V√° na aba \"Secrets\"\n4. Cole suas vari√°veis no formato TOML\n\n**Dica!** O Streamlit Cloud usa formato TOML para secrets, n√£o .env!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de como acessar secrets no Streamlit Cloud\n",
        "secrets_example = '''\n",
        "import streamlit as st\n",
        "\n",
        "# No Streamlit Cloud, use st.secrets\n",
        "# Localmente, pode usar dotenv\n",
        "\n",
        "def get_api_key():\n",
        "    \"\"\"Fun√ß√£o para pegar API key independente do ambiente\"\"\"\n",
        "    try:\n",
        "        # Primeiro tenta pegar do Streamlit secrets\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        # Se n√£o conseguir, tenta do ambiente local\n",
        "        import os\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Usar a fun√ß√£o\n",
        "api_key = get_api_key()\n",
        "\n",
        "if api_key:\n",
        "    st.success(\"‚úÖ API Key carregada com sucesso!\")\n",
        "else:\n",
        "    st.error(\"‚ùå API Key n√£o encontrada!\")\n",
        "'''\n",
        "\n",
        "print(\"Exemplo de c√≥digo para acessar secrets:\")\n",
        "print(secrets_example)\n",
        "\n",
        "# Exemplo do arquivo secrets.toml\n",
        "secrets_toml = '''\n",
        "# .streamlit/secrets.toml\n",
        "GOOGLE_API_KEY = \"sua_api_key_aqui\"\n",
        "APP_TITLE = \"Meu Chatbot Incr√≠vel\"\n",
        "DEBUG = false\n",
        "'''\n",
        "\n",
        "print(\"\\nüìã Formato do secrets.toml:\")\n",
        "print(secrets_toml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Monitoramento e Analytics\n\nT√°, mas depois que sua app est√° no ar, como saber se est√° tudo funcionando? √â como abrir uma loja e n√£o saber quantos clientes entraram!\n\nVamos adicionar algumas m√©tricas b√°sicas para acompanhar o uso da nossa aplica√ß√£o.\n\n**Dica!** Streamlit tem analytics b√°sicos gr√°tis, mas voc√™ pode integrar com Google Analytics para mais detalhes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vers√£o com monitoramento b√°sico\n",
        "app_with_analytics = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot com Analytics\",\n",
        "    page_icon=\"üìä\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Fun√ß√£o para log simples\n",
        "def log_interaction(user_input, response_length):\n",
        "    \"\"\"Log b√°sico das intera√ß√µes\"\"\"\n",
        "    if \"interaction_count\" not in st.session_state:\n",
        "        st.session_state.interaction_count = 0\n",
        "    \n",
        "    st.session_state.interaction_count += 1\n",
        "    \n",
        "    # Em produ√ß√£o, voc√™ salvaria isso em um banco de dados\n",
        "    log_data = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"user_input_length\": len(user_input),\n",
        "        \"response_length\": response_length,\n",
        "        \"interaction_number\": st.session_state.interaction_count\n",
        "    }\n",
        "    \n",
        "    return log_data\n",
        "\n",
        "# Sidebar com analytics\n",
        "with st.sidebar:\n",
        "    st.header(\"üìä Analytics\")\n",
        "    \n",
        "    if \"interaction_count\" in st.session_state:\n",
        "        st.metric(\"Intera√ß√µes\", st.session_state.interaction_count)\n",
        "    \n",
        "    if \"messages\" in st.session_state:\n",
        "        st.metric(\"Mensagens\", len(st.session_state.messages))\n",
        "    \n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "\n",
        "# T√≠tulo da aplica√ß√£o\n",
        "st.title(\"üìä Chatbot com Monitoramento\")\n",
        "st.write(\"Agora com analytics integrados!\")\n",
        "\n",
        "# Verificar API key\n",
        "def get_api_key():\n",
        "    try:\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "api_key = get_api_key()\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"‚ùå API Key n√£o encontrada!\")\n",
        "    st.stop()\n",
        "\n",
        "# Configurar o modelo\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    google_api_key=api_key,\n",
        "    temperature=temperatura\n",
        ")\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usu√°rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usu√°rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            messages = [\n",
        "                SystemMessage(content=\"Voc√™ √© um assistente prestativo e amig√°vel.\"),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "            response = llm.invoke(messages)\n",
        "            st.write(response.content)\n",
        "            \n",
        "            # Log da intera√ß√£o\n",
        "            log_data = log_interaction(prompt, len(response.content))\n",
        "            \n",
        "            # Salvar resposta\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "# Footer com info\n",
        "st.markdown(\"---\")\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Status\", \"üü¢ Online\")\n",
        "\n",
        "with col2:\n",
        "    if \"interaction_count\" in st.session_state:\n",
        "        st.metric(\"Intera√ß√µes\", st.session_state.interaction_count)\n",
        "\n",
        "with col3:\n",
        "    st.metric(\"Modelo\", \"Gemini 2.0\")\n",
        "'''\n",
        "\n",
        "# Salvar vers√£o com analytics\n",
        "with open('app_analytics.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_with_analytics)\n",
        "\n",
        "print(\"üìä Vers√£o com analytics criada: app_analytics.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Troubleshooting: Problemas Comuns\n\nTodo deploy tem seus perrengues! √â como dirigir - voc√™ aprende a resolver os problemas mais comuns. Aqui est√£o os que eu mais vejo:\n\n### üî¥ Problemas e Solu√ß√µes:\n\n| Problema | Causa | Solu√ß√£o |\n|----------|-------|----------|\n| App n√£o carrega | requirements.txt errado | Verificar vers√µes das bibliotecas |\n| API Key n√£o funciona | Secrets mal configurados | Checar formato TOML |\n| App lenta | Modelo muito pesado | Usar modelo mais leve ou cache |\n| Memory error | Session state muito grande | Limitar hist√≥rico de mensagens |\n| Deploy falha | Arquivo muito grande | Otimizar c√≥digo e depend√™ncias |\n\n**Dica!** Sempre teste localmente antes de fazer deploy. √â como ensaiar antes da apresenta√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Script para debug e diagn√≥stico\n",
        "debug_script = '''\n",
        "import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "st.title(\"üîß Debug Dashboard\")\n",
        "st.write(\"Informa√ß√µes para troubleshooting\")\n",
        "\n",
        "# Informa√ß√µes do sistema\n",
        "st.header(\"üíª Sistema\")\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    st.write(f\"**Python:** {sys.version}\")\n",
        "    st.write(f\"**Streamlit:** {st.__version__}\")\n",
        "    st.write(f\"**Timestamp:** {datetime.now()}\")\n",
        "\n",
        "with col2:\n",
        "    st.write(f\"**Platform:** {sys.platform}\")\n",
        "    st.write(f\"**Encoding:** {sys.getdefaultencoding()}\")\n",
        "\n",
        "# Verificar vari√°veis de ambiente\n",
        "st.header(\"üîë Vari√°veis de Ambiente\")\n",
        "\n",
        "# Checar se API key existe (sem mostrar o valor)\n",
        "def check_api_key():\n",
        "    try:\n",
        "        key = st.secrets.get(\"GOOGLE_API_KEY\")\n",
        "        if key:\n",
        "            return f\"‚úÖ Encontrada (tamanho: {len(key)})\")\n",
        "        else:\n",
        "            return \"‚ùå N√£o encontrada nos secrets\"\n",
        "    except:\n",
        "        try:\n",
        "            key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "            if key:\n",
        "                return f\"‚úÖ Encontrada no .env (tamanho: {len(key)})\"\n",
        "            else:\n",
        "                return \"‚ùå N√£o encontrada no .env\"\n",
        "        except:\n",
        "            return \"‚ùå Erro ao verificar\"\n",
        "\n",
        "st.write(f\"**API Key:** {check_api_key()}\")\n",
        "\n",
        "# Testar imports\n",
        "st.header(\"üì¶ Bibliotecas\")\n",
        "\n",
        "libraries = [\n",
        "    \"langchain\",\n",
        "    \"langchain_google_genai\", \n",
        "    \"dotenv\",\n",
        "    \"pandas\",\n",
        "    \"numpy\"\n",
        "]\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        __import__(lib)\n",
        "        st.write(f\"‚úÖ {lib}\")\n",
        "    except ImportError as e:\n",
        "        st.write(f\"‚ùå {lib}: {e}\")\n",
        "\n",
        "# Session State\n",
        "st.header(\"üíæ Session State\")\n",
        "st.write(f\"**Chaves:** {list(st.session_state.keys())}\")\n",
        "\n",
        "if st.session_state:\n",
        "    st.json(dict(st.session_state))\n",
        "else:\n",
        "    st.write(\"Session state vazio\")\n",
        "\n",
        "# Bot√£o de teste\n",
        "if st.button(\"üß™ Test API Connection\"):\n",
        "    try:\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        \n",
        "        api_key = st.secrets.get(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
        "        \n",
        "        if api_key:\n",
        "            llm = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-2.0-flash-exp\",\n",
        "                google_api_key=api_key\n",
        "            )\n",
        "            \n",
        "            response = llm.invoke(\"Diga apenas 'OK' se voc√™ est√° funcionando\")\n",
        "            st.success(f\"‚úÖ API funcionando: {response.content}\")\n",
        "            \n",
        "        else:\n",
        "            st.error(\"‚ùå API Key n√£o encontrada\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Erro na API: {e}\")\n",
        "'''\n",
        "\n",
        "with open('debug.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(debug_script)\n",
        "\n",
        "print(\"üîß Script de debug criado: debug.py\")\n",
        "print(\"Use: streamlit run debug.py para diagnosticar problemas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Otimiza√ß√£o e Performance\n\nT√°, mas sua app est√° lenta? √â como um carro que n√£o acelera - tem v√°rias coisas que podem estar travando!\n\n### Principais otimiza√ß√µes:\n\n1. **Cache de dados**: Use `@st.cache_data`\n2. **Cache de recursos**: Use `@st.cache_resource` \n3. **Lazy loading**: Carregue apenas quando necess√°rio\n4. **Sess√£o otimizada**: Limite o tamanho do session_state\n\n**Dica!** O cache do Streamlit √© poderoso, mas use com cuidado em fun√ß√µes que dependem de API keys!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vers√£o otimizada da nossa app\n",
        "app_optimized = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot Otimizado\",\n",
        "    page_icon=\"‚ö°\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Cache da fun√ß√£o de API key\n",
        "@st.cache_data\n",
        "def get_api_key():\n",
        "    \"\"\"Cache da API key para evitar recarregar\"\"\"\n",
        "    try:\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Cache do modelo (mais importante!)\n",
        "@st.cache_resource\n",
        "def get_llm(temperatura=0.7):\n",
        "    \"\"\"Cache do modelo para n√£o recriar a cada intera√ß√£o\"\"\"\n",
        "    api_key = get_api_key()\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "\n",
        "# Fun√ß√£o para limitar hist√≥rico\n",
        "def limit_chat_history(messages, max_messages=20):\n",
        "    \"\"\"Limita o hist√≥rico para n√£o sobrecarregar a mem√≥ria\"\"\"\n",
        "    if len(messages) > max_messages:\n",
        "        # Manter apenas as √∫ltimas mensagens\n",
        "        return messages[-max_messages:]\n",
        "    return messages\n",
        "\n",
        "# T√≠tulo da aplica√ß√£o\n",
        "st.title(\"‚ö° Chatbot Ultra Otimizado\")\n",
        "st.write(\"Vers√£o com cache e otimiza√ß√µes de performance!\")\n",
        "\n",
        "# Verificar API key\n",
        "api_key = get_api_key()\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"‚ùå API Key n√£o encontrada!\")\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "    max_history = st.slider(\"M√°ximo de mensagens\", 5, 50, 20)\n",
        "    \n",
        "    if st.button(\"üóëÔ∏è Limpar Chat\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "    \n",
        "    if st.button(\"üîÑ Limpar Cache\"):\n",
        "        st.cache_data.clear()\n",
        "        st.cache_resource.clear()\n",
        "        st.success(\"Cache limpo!\")\n",
        "\n",
        "# Configurar o modelo (com cache)\n",
        "llm = get_llm(temperatura)\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Limitar hist√≥rico\n",
        "st.session_state.messages = limit_chat_history(st.session_state.messages, max_history)\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usu√°rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usu√°rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            try:\n",
        "                # Usar apenas as √∫ltimas mensagens para contexto\n",
        "                recent_messages = st.session_state.messages[-5:] if len(st.session_state.messages) > 5 else st.session_state.messages\n",
        "                \n",
        "                # Construir contexto\n",
        "                context = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in recent_messages[:-1]])\n",
        "                \n",
        "                messages = [\n",
        "                    SystemMessage(content=f\"Voc√™ √© um assistente prestativo. Contexto recente: {context}\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                \n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "                \n",
        "                # Salvar resposta\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "                \n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro: {e}\")\n",
        "\n",
        "# Footer com m√©tricas\n",
        "st.markdown(\"---\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Mensagens\", len(st.session_state.messages))\n",
        "\n",
        "with col2:\n",
        "    st.metric(\"Temperatura\", f\"{temperatura:.1f}\")\n",
        "\n",
        "with col3:\n",
        "    st.metric(\"Limite\", max_history)\n",
        "\n",
        "with col4:\n",
        "    st.metric(\"Status\", \"üü¢\")\n",
        "'''\n",
        "\n",
        "with open('app_optimized.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_optimized)\n",
        "\n",
        "print(\"‚ö° Vers√£o otimizada criada: app_optimized.py\")\n",
        "print(\"Melhorias inclu√≠das: cache, limita√ß√£o de hist√≥rico, contexto otimizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Customiza√ß√£o Avan√ßada\n\nVamos deixar nossa app com a cara profissional! √â como decorar sua casa - os m√≥veis b√°sicos funcionam, mas o estilo faz toda diferen√ßa.\n\n![](/imagens/langchain-modulo-14_img_03.png)\n\n**Dica!** CSS customizado pode ser injetado no Streamlit usando `st.markdown` com `unsafe_allow_html=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# App com design customizado\n",
        "app_custom = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"IA Assistant Pro\",\n",
        "    page_icon=\"üé®\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# CSS customizado\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Personalizar o header */\n",
        "    .main-header {\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 2rem;\n",
        "        border-radius: 10px;\n",
        "        margin-bottom: 2rem;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "    }\n",
        "    \n",
        "    /* Personalizar sidebar */\n",
        "    .css-1d391kg {\n",
        "        background-color: #f8f9fa;\n",
        "    }\n",
        "    \n",
        "    /* Bot√µes personalizados */\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        border: none;\n",
        "        border-radius: 25px;\n",
        "        padding: 0.5rem 1rem;\n",
        "        transition: all 0.3s;\n",
        "    }\n",
        "    \n",
        "    .stButton > button:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    \n",
        "    /* Chat messages */\n",
        "    .user-message {\n",
        "        background-color: #e3f2fd;\n",
        "        padding: 1rem;\n",
        "        border-radius: 15px;\n",
        "        margin: 0.5rem 0;\n",
        "        border-left: 4px solid #2196f3;\n",
        "    }\n",
        "    \n",
        "    .assistant-message {\n",
        "        background-color: #f3e5f5;\n",
        "        padding: 1rem;\n",
        "        border-radius: 15px;\n",
        "        margin: 0.5rem 0;\n",
        "        border-left: 4px solid #9c27b0;\n",
        "    }\n",
        "    \n",
        "    /* Footer */\n",
        "    .footer {\n",
        "        position: fixed;\n",
        "        left: 0;\n",
        "        bottom: 0;\n",
        "        width: 100%;\n",
        "        background-color: #667eea;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        padding: 10px;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Header customizado\n",
        "st.markdown(\"\"\"\n",
        "<div class=\"main-header\">\n",
        "    <h1>üé® IA Assistant Pro</h1>\n",
        "    <p>Powered by LangChain & Streamlit</p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Cache do modelo\n",
        "@st.cache_resource\n",
        "def get_llm(temperatura=0.7):\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "\n",
        "# Sidebar elegante\n",
        "with st.sidebar:\n",
        "    st.markdown(\"### ‚öôÔ∏è Configura√ß√µes\")\n",
        "    \n",
        "    temperatura = st.slider(\n",
        "        \"üéØ Criatividade\", \n",
        "        0.0, 1.0, 0.7,\n",
        "        help=\"Controla a criatividade das respostas\"\n",
        "    )\n",
        "    \n",
        "    modo = st.selectbox(\n",
        "        \"üé≠ Modo de Conversa\",\n",
        "        [\"Amig√°vel\", \"Profissional\", \"Criativo\", \"T√©cnico\"]\n",
        "    )\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    \n",
        "    if st.button(\"üóëÔ∏è Nova Conversa\", key=\"clear\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### üìä Estat√≠sticas\")\n",
        "    \n",
        "    if \"messages\" in st.session_state:\n",
        "        total_msgs = len(st.session_state.messages)\n",
        "        user_msgs = sum(1 for msg in st.session_state.messages if msg[\"role\"] == \"user\")\n",
        "        \n",
        "        st.metric(\"Total de mensagens\", total_msgs)\n",
        "        st.metric(\"Suas mensagens\", user_msgs)\n",
        "        st.metric(\"Respostas da IA\", total_msgs - user_msgs)\n",
        "\n",
        "# Definir persona baseada no modo\n",
        "personas = {\n",
        "    \"Amig√°vel\": \"Voc√™ √© um assistente amig√°vel e descontra√≠do, que usa emojis e linguagem casual.\",\n",
        "    \"Profissional\": \"Voc√™ √© um assistente profissional, formal e preciso em suas respostas.\",\n",
        "    \"Criativo\": \"Voc√™ √© um assistente criativo, que ama usar analogias e exemplos interessantes.\",\n",
        "    \"T√©cnico\": \"Voc√™ √© um assistente t√©cnico especializado, que fornece respostas detalhadas e precisas.\"\n",
        "}\n",
        "\n",
        "# Verificar API e configurar modelo\n",
        "try:\n",
        "    llm = get_llm(temperatura)\n",
        "except Exception as e:\n",
        "    st.error(f\"‚ùå Erro ao configurar IA: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Container para o chat\n",
        "chat_container = st.container()\n",
        "\n",
        "with chat_container:\n",
        "    # Mostrar mensagens anteriores\n",
        "    for message in st.session_state.messages:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"user-message\">\n",
        "                <strong>üßë Voc√™:</strong><br>\n",
        "                {message[\"content\"]}\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"assistant-message\">\n",
        "                <strong>ü§ñ Assistente:</strong><br>\n",
        "                {message[\"content\"]}\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Input do usu√°rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem aqui... üí¨\"):\n",
        "    # Adicionar mensagem do usu√°rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.spinner(\"ü§î Pensando...\"):\n",
        "        try:\n",
        "            messages = [\n",
        "                SystemMessage(content=personas[modo]),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "            \n",
        "            response = llm.invoke(messages)\n",
        "            \n",
        "            # Salvar resposta\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "            \n",
        "            # Recarregar para mostrar nova mensagem\n",
        "            st.rerun()\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro ao gerar resposta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"\"\"\n",
        "<div style=\"text-align: center; padding: 2rem; margin-top: 3rem; border-top: 1px solid #eee;\">\n",
        "    <p>üöÄ Feito com <strong>LangChain</strong> + <strong>Streamlit</strong></p>\n",
        "    <p><small>Deploy profissional em produ√ß√£o!</small></p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "'''\n",
        "\n",
        "with open('app_custom.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_custom)\n",
        "\n",
        "print(\"üé® App customizada criada: app_custom.py\")\n",
        "print(\"Features: CSS customizado, personas, m√©tricas, design profissional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì± App Multi-P√°gina\n\nT√°, mas e se voc√™ quiser criar uma aplica√ß√£o mais complexa? Com v√°rias p√°ginas, diferentes funcionalidades?\n\n√â como construir uma casa com v√°rios c√¥modos - cada um tem sua fun√ß√£o espec√≠fica!\n\nStreamlit 1.10+ tem suporte nativo para multi-p√°ginas. √â s√≥ criar uma pasta `pages/` e colocar os arquivos l√°!\n\n**Dica!** Cada arquivo na pasta `pages/` vira automaticamente uma p√°gina no seu app!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Criar pasta pages se n√£o existir\n",
        "if not os.path.exists('pages'):\n",
        "    os.makedirs('pages')\n",
        "\n",
        "# P√°gina principal (main.py)\n",
        "main_page = '''\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"IA Platform\",\n",
        "    page_icon=\"üè†\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"üè† IA Platform - Home\")\n",
        "st.write(\"Bem-vindo √† nossa plataforma de IA!\")\n",
        "\n",
        "# Cards de navega√ß√£o\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "with col1:\n",
        "    st.markdown(\"\"\"\n",
        "    ### üí¨ Chatbot\n",
        "    Converse com nossa IA avan√ßada\n",
        "    \n",
        "    [üëâ Acessar Chatbot](Chatbot)\n",
        "    \"\"\")\n",
        "\n",
        "with col2:\n",
        "    st.markdown(\"\"\"\n",
        "    ### üìä Analytics\n",
        "    Veja estat√≠sticas de uso\n",
        "    \n",
        "    [üëâ Ver Analytics](Analytics)\n",
        "    \"\"\")\n",
        "\n",
        "with col3:\n",
        "    st.markdown(\"\"\"\n",
        "    ### ‚öôÔ∏è Configura√ß√µes\n",
        "    Configure sua experi√™ncia\n",
        "    \n",
        "    [üëâ Configurar](Configura√ß√µes)\n",
        "    \"\"\")\n",
        "\n",
        "# Se√ß√£o de recursos\n",
        "st.markdown(\"---\")\n",
        "st.header(\"üöÄ Recursos Dispon√≠veis\")\n",
        "\n",
        "features = [\n",
        "    \"ü§ñ IA conversacional avan√ßada\",\n",
        "    \"üìä Analytics em tempo real\", \n",
        "    \"üîí Seguran√ßa de dados\",\n",
        "    \"‚ö° Performance otimizada\",\n",
        "    \"üé® Interface customiz√°vel\",\n",
        "    \"üì± Responsivo e moderno\"\n",
        "]\n",
        "\n",
        "for feature in features:\n",
        "    st.write(feature)\n",
        "'''\n",
        "\n",
        "# P√°gina do Chatbot\n",
        "chatbot_page = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot\",\n",
        "    page_icon=\"üí¨\"\n",
        ")\n",
        "\n",
        "st.title(\"üí¨ Chatbot Inteligente\")\n",
        "\n",
        "@st.cache_resource\n",
        "def get_llm():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "# Sistema de chat (vers√£o simplificada)\n",
        "if \"chat_messages\" not in st.session_state:\n",
        "    st.session_state.chat_messages = []\n",
        "\n",
        "for message in st.session_state.chat_messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        try:\n",
        "            llm = get_llm()\n",
        "            response = llm.invoke([HumanMessage(content=prompt)])\n",
        "            st.write(response.content)\n",
        "            st.session_state.chat_messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro: {e}\")\n",
        "'''\n",
        "\n",
        "# P√°gina de Analytics\n",
        "analytics_page = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Analytics\",\n",
        "    page_icon=\"üìä\"\n",
        ")\n",
        "\n",
        "st.title(\"üìä Analytics Dashboard\")\n",
        "\n",
        "# Dados fict√≠cios para demonstra√ß√£o\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D')\n",
        "users = np.random.randint(50, 200, len(dates))\n",
        "messages = np.random.randint(100, 500, len(dates))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Data': dates,\n",
        "    'Usu√°rios': users,\n",
        "    'Mensagens': messages\n",
        "})\n",
        "\n",
        "# M√©tricas principais\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Usu√°rios Ativos\", f\"{users[-1]}\", f\"{users[-1] - users[-2]:+d}\")\n",
        "\n",
        "with col2:\n",
        "    st.metric(\"Mensagens Hoje\", f\"{messages[-1]}\", f\"{messages[-1] - messages[-2]:+d}\")\n",
        "    \n",
        "with col3:\n",
        "    st.metric(\"Total Mensal\", f\"{df['Mensagens'].sum():,}\")\n",
        "    \n",
        "with col4:\n",
        "    st.metric(\"M√©dia/Dia\", f\"{df['Mensagens'].mean():.0f}\")\n",
        "\n",
        "# Gr√°ficos\n",
        "st.header(\"üìà Tend√™ncias\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"Usu√°rios\", \"Mensagens\"])\n",
        "\n",
        "with tab1:\n",
        "    st.line_chart(df.set_index('Data')['Usu√°rios'])\n",
        "    \n",
        "with tab2:\n",
        "    st.line_chart(df.set_index('Data')['Mensagens'])\n",
        "\n",
        "# Tabela de dados\n",
        "st.header(\"üìã Dados Detalhados\")\n",
        "st.dataframe(df, use_container_width=True)\n",
        "'''\n",
        "\n",
        "# P√°gina de Configura√ß√µes\n",
        "config_page = '''\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Configura√ß√µes\",\n",
        "    page_icon=\"‚öôÔ∏è\"\n",
        ")\n",
        "\n",
        "st.title(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "\n",
        "# Configura√ß√µes do usu√°rio\n",
        "st.header(\"üë§ Perfil do Usu√°rio\")\n",
        "\n",
        "with st.form(\"user_config\"):\n",
        "    name = st.text_input(\"Nome\", value=st.session_state.get('user_name', ''))\n",
        "    email = st.text_input(\"Email\", value=st.session_state.get('user_email', ''))\n",
        "    theme = st.selectbox(\"Tema\", [\"Claro\", \"Escuro\", \"Auto\"])\n",
        "    \n",
        "    if st.form_submit_button(\"üíæ Salvar\"):\n",
        "        st.session_state.user_name = name\n",
        "        st.session_state.user_email = email\n",
        "        st.session_state.theme = theme\n",
        "        st.success(\"Configura√ß√µes salvas!\")\n",
        "\n",
        "st.header(\"ü§ñ Configura√ß√µes da IA\")\n",
        "\n",
        "temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "max_tokens = st.number_input(\"M√°ximo de tokens\", 100, 4000, 1000)\n",
        "modelo = st.selectbox(\"Modelo\", [\"gemini-2.0-flash-exp\", \"gemini-1.5-pro\"])\n",
        "\n",
        "if st.button(\"üîÑ Resetar Configura√ß√µes\"):\n",
        "    for key in list(st.session_state.keys()):\n",
        "        del st.session_state[key]\n",
        "    st.success(\"Configura√ß√µes resetadas!\")\n",
        "    st.rerun()\n",
        "\n",
        "st.header(\"üîß Informa√ß√µes do Sistema\")\n",
        "\n",
        "info_col1, info_col2 = st.columns(2)\n",
        "\n",
        "with info_col1:\n",
        "    st.write(\"**Vers√£o:** 1.0.0\")\n",
        "    st.write(\"**Framework:** Streamlit + LangChain\")\n",
        "    st.write(\"**Modelo:** Gemini 2.0\")\n",
        "    \n",
        "with info_col2:\n",
        "    st.write(\"**Status:** üü¢ Online\")\n",
        "    st.write(\"**Uptime:** 99.9%\")\n",
        "    st.write(\"**Regi√£o:** Global\")\n",
        "'''\n",
        "\n",
        "# Salvar arquivos\n",
        "with open('main.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(main_page)\n",
        "\n",
        "with open('pages/1_üí¨_Chatbot.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(chatbot_page)\n",
        "\n",
        "with open('pages/2_üìä_Analytics.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(analytics_page)\n",
        "\n",
        "with open('pages/3_‚öôÔ∏è_Configura√ß√µes.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(config_page)\n",
        "\n",
        "print(\"üì± App multi-p√°gina criada!\")\n",
        "print(\"Estrutura:\")\n",
        "print(\"‚îú‚îÄ‚îÄ main.py (p√°gina principal)\")\n",
        "print(\"‚îî‚îÄ‚îÄ pages/\")\n",
        "print(\"    ‚îú‚îÄ‚îÄ 1_üí¨_Chatbot.py\")\n",
        "print(\"    ‚îú‚îÄ‚îÄ 2_üìä_Analytics.py\")\n",
        "print(\"    ‚îî‚îÄ‚îÄ 3_‚öôÔ∏è_Configura√ß√µes.py\")\n",
        "print(\"\\nPara rodar: streamlit run main.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Pr√°tico: Seu Primeiro Deploy\n\nBora colocar a m√£o na massa! Vamos criar uma aplica√ß√£o personalizada e fazer o deploy completo.\n\n### Desafio:\nCriar uma aplica√ß√£o que combine:\n- Um dos projetos que fizemos nos m√≥dulos anteriores (RAG, Agents, etc.)\n- Interface Streamlit profissional\n- Deploy no Streamlit Cloud\n\n**Tempo estimado:** 30 minutos\n\n![](/imagens/langchain-modulo-14_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template para o exerc√≠cio\n",
        "exercise_template = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "# Importe aqui as bibliotecas do seu projeto escolhido\n",
        "# Ex: para RAG -> from langchain.vectorstores import FAISS\n",
        "# Ex: para Agents -> from langchain.agents import initialize_agent\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Meu Projeto IA\",\n",
        "    page_icon=\"üöÄ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"üöÄ Meu Projeto de IA em Produ√ß√£o\")\n",
        "st.write(\"Baseado no [ESCOLHA: RAG/Agents/Memory/etc.] do curso LangChain\")\n",
        "\n",
        "# TODO: Implementar seu projeto aqui\n",
        "# Dicas:\n",
        "# 1. Use @st.cache_resource para objetos pesados\n",
        "# 2. Use @st.cache_data para dados que n√£o mudam\n",
        "# 3. Adicione tratamento de erros\n",
        "# 4. Coloque configura√ß√µes na sidebar\n",
        "# 5. Use session_state para manter estado\n",
        "\n",
        "# Exemplo de estrutura:\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    # Suas configura√ß√µes aqui\n",
        "\n",
        "# √Årea principal\n",
        "if st.button(\"üß™ Testar Funcionalidade\"):\n",
        "    st.info(\"Implemente aqui a funcionalidade principal do seu projeto!\")\n",
        "    # Seu c√≥digo aqui\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"üí° **Projeto desenvolvido no curso LangChain v0.3**\")\n",
        "'''\n",
        "\n",
        "with open('exercise_template.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(exercise_template)\n",
        "\n",
        "print(\"üìù Template do exerc√≠cio criado: exercise_template.py\")\n",
        "print(\"\\nüéØ DESAFIO:\")\n",
        "print(\"1. Escolha um projeto dos m√≥dulos anteriores\")\n",
        "print(\"2. Adapte para funcionar no Streamlit\")\n",
        "print(\"3. Adicione interface profissional\")\n",
        "print(\"4. Crie reposit√≥rio no GitHub\")\n",
        "print(\"5. Fa√ßa deploy no Streamlit Cloud\")\n",
        "print(\"\\nüí™ Voc√™ consegue! √â hora de colocar sua IA no ar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Exerc√≠cio Extra: RAG em Produ√ß√£o\n\nVamos pegar nosso sistema RAG do m√≥dulo 10 e transformar em uma aplica√ß√£o web completa!\n\nEste √© um exerc√≠cio mais avan√ßado que combina tudo que aprendemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica√ß√£o RAG completa para produ√ß√£o\n",
        "rag_app = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"RAG System Pro\",\n",
        "    page_icon=\"üìö\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"üìö Sistema RAG Profissional\")\n",
        "st.write(\"Fa√ßa upload de documentos e converse com eles usando IA!\")\n",
        "\n",
        "# Cache das fun√ß√µes pesadas\n",
        "@st.cache_resource\n",
        "def get_llm():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=0.3  # Menos criativo para RAG\n",
        "    )\n",
        "\n",
        "@st.cache_resource\n",
        "def get_embeddings():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/embedding-001\",\n",
        "        google_api_key=api_key\n",
        "    )\n",
        "\n",
        "def process_documents(uploaded_files):\n",
        "    \"\"\"Processa documentos uploaded\"\"\"\n",
        "    documents = []\n",
        "    \n",
        "    for uploaded_file in uploaded_files:\n",
        "        # Salvar arquivo temporariamente\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{uploaded_file.name.split('.')[-1]}\") as tmp_file:\n",
        "            tmp_file.write(uploaded_file.getvalue())\n",
        "            tmp_path = tmp_file.name\n",
        "        \n",
        "        try:\n",
        "            # Carregar baseado na extens√£o\n",
        "            if uploaded_file.name.endswith('.pdf'):\n",
        "                loader = PyPDFLoader(tmp_path)\n",
        "            else:\n",
        "                loader = TextLoader(tmp_path, encoding='utf-8')\n",
        "            \n",
        "            docs = loader.load()\n",
        "            \n",
        "            # Adicionar metadata\n",
        "            for doc in docs:\n",
        "                doc.metadata['source'] = uploaded_file.name\n",
        "            \n",
        "            documents.extend(docs)\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro ao processar {uploaded_file.name}: {e}\")\n",
        "        finally:\n",
        "            # Limpar arquivo tempor√°rio\n",
        "            os.unlink(tmp_path)\n",
        "    \n",
        "    return documents\n",
        "\n",
        "@st.cache_data\n",
        "def create_vectorstore(_documents):\n",
        "    \"\"\"Cria vectorstore dos documentos\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    \n",
        "    texts = text_splitter.split_documents(_documents)\n",
        "    embeddings = get_embeddings()\n",
        "    \n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"üìÅ Upload de Documentos\")\n",
        "    \n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Escolha seus arquivos\",\n",
        "        accept_multiple_files=True,\n",
        "        type=['pdf', 'txt', 'md']\n",
        "    )\n",
        "    \n",
        "    if uploaded_files:\n",
        "        st.success(f\"{len(uploaded_files)} arquivo(s) carregado(s)\")\n",
        "        \n",
        "        if st.button(\"üîÑ Processar Documentos\"):\n",
        "            with st.spinner(\"Processando documentos...\"):\n",
        "                documents = process_documents(uploaded_files)\n",
        "                \n",
        "                if documents:\n",
        "                    vectorstore = create_vectorstore(documents)\n",
        "                    st.session_state.vectorstore = vectorstore\n",
        "                    st.session_state.documents = documents\n",
        "                    st.success(\"Documentos processados com sucesso!\")\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"‚öôÔ∏è Configura√ß√µes\")\n",
        "    \n",
        "    k_results = st.slider(\"Resultados por busca\", 1, 10, 4)\n",
        "    show_sources = st.checkbox(\"Mostrar fontes\", True)\n",
        "\n",
        "# √Årea principal\n",
        "if \"vectorstore\" not in st.session_state:\n",
        "    st.info(\"üëà Fa√ßa upload de documentos na barra lateral para come√ßar!\")\n",
        "    \n",
        "    # Exemplo de uso\n",
        "    st.markdown(\"\"\"\n",
        "    ### üöÄ Como usar:\n",
        "    \n",
        "    1. **Upload**: Envie arquivos PDF ou TXT na barra lateral\n",
        "    2. **Processe**: Clique em \"Processar Documentos\"\n",
        "    3. **Converse**: Fa√ßa perguntas sobre o conte√∫do\n",
        "    4. **Explore**: Use as configura√ß√µes para ajustar os resultados\n",
        "    \n",
        "    ### üìã Formatos suportados:\n",
        "    - üìÑ PDF\n",
        "    - üìù TXT\n",
        "    - üìã Markdown (MD)\n",
        "    \n",
        "    ### üí° Dicas:\n",
        "    - Documente menores = respostas mais precisas\n",
        "    - Use perguntas espec√≠ficas\n",
        "    - Ative \"Mostrar fontes\" para verificar as refer√™ncias\n",
        "    \"\"\")\n",
        "    \n",
        "else:\n",
        "    # Sistema de chat RAG\n",
        "    st.header(\"üí¨ Chat com Documentos\")\n",
        "    \n",
        "    # Informa√ß√µes dos documentos\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "        st.metric(\"Documentos\", len(st.session_state.documents))\n",
        "    \n",
        "    with col2:\n",
        "        total_chars = sum(len(doc.page_content) for doc in st.session_state.documents)\n",
        "        st.metric(\"Caracteres\", f\"{total_chars:,}\")\n",
        "    \n",
        "    with col3:\n",
        "        st.metric(\"Chunks\", st.session_state.vectorstore.index.ntotal)\n",
        "    \n",
        "    # Chat\n",
        "    if \"rag_messages\" not in st.session_state:\n",
        "        st.session_state.rag_messages = []\n",
        "    \n",
        "    # Mostrar mensagens\n",
        "    for message in st.session_state.rag_messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "            \n",
        "            if \"sources\" in message and show_sources:\n",
        "                with st.expander(\"üìö Ver fontes\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        st.write(f\"**Fonte {i}:** {source['source']}\")\n",
        "                        st.write(f\"*Conte√∫do:* {source['content'][:200]}...\")\n",
        "                        st.markdown(\"---\")\n",
        "    \n",
        "    # Input\n",
        "    if prompt := st.chat_input(\"Fa√ßa uma pergunta sobre os documentos...\"):\n",
        "        # Adicionar pergunta\n",
        "        st.session_state.rag_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        \n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "        \n",
        "        # Gerar resposta\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Buscando informa√ß√µes...\"):\n",
        "                try:\n",
        "                    # Configurar RAG chain\n",
        "                    llm = get_llm()\n",
        "                    retriever = st.session_state.vectorstore.as_retriever(\n",
        "                        search_kwargs={\"k\": k_results}\n",
        "                    )\n",
        "                    \n",
        "                    qa_chain = RetrievalQA.from_chain_type(\n",
        "                        llm=llm,\n",
        "                        chain_type=\"stuff\",\n",
        "                        retriever=retriever,\n",
        "                        return_source_documents=True\n",
        "                    )\n",
        "                    \n",
        "                    # Executar query\n",
        "                    result = qa_chain({\"query\": prompt})\n",
        "                    \n",
        "                    # Mostrar resposta\n",
        "                    st.write(result[\"result\"])\n",
        "                    \n",
        "                    # Preparar fontes\n",
        "                    sources = []\n",
        "                    for doc in result[\"source_documents\"]:\n",
        "                        sources.append({\n",
        "                            \"source\": doc.metadata.get(\"source\", \"Desconhecido\"),\n",
        "                            \"content\": doc.page_content\n",
        "                        })\n",
        "                    \n",
        "                    # Salvar mensagem\n",
        "                    st.session_state.rag_messages.append({\n",
        "                        \"role\": \"assistant\", \n",
        "                        \"content\": result[\"result\"],\n",
        "                        \"sources\": sources\n",
        "                    })\n",
        "                    \n",
        "                    # Mostrar fontes se habilitado\n",
        "                    if show_sources:\n",
        "                        with st.expander(\"üìö Ver fontes\"):\n",
        "                            for i, source in enumerate(sources, 1):\n",
        "                                st.write(f\"**Fonte {i}:** {source['source']}\")\n",
        "                                st.write(f\"*Conte√∫do:* {source['content'][:200]}...\")\n",
        "                                st.markdown(\"---\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    st.error(f\"Erro ao processar pergunta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"üöÄ **Sistema RAG Profissional** - Powered by LangChain + Streamlit\")\n",
        "'''\n",
        "\n",
        "with open('rag_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(rag_app)\n",
        "\n",
        "# Requirements espec√≠fico para RAG\n",
        "rag_requirements = '''\n",
        "streamlit>=1.28.0\n",
        "langchain>=0.3.0\n",
        "langchain-google-genai>=2.0.0\n",
        "python-dotenv>=1.0.0\n",
        "faiss-cpu>=1.7.4\n",
        "PyPDF2>=3.0.1\n",
        "pypdf>=3.17.0\n",
        "'''\n",
        "\n",
        "with open('requirements_rag.txt', 'w') as f:\n",
        "    f.write(rag_requirements.strip())\n",
        "\n",
        "print(\"üìö Sistema RAG completo criado!\")\n",
        "print(\"Arquivos:\")\n",
        "print(\"‚îú‚îÄ‚îÄ rag_app.py (aplica√ß√£o principal)\")\n",
        "print(\"‚îî‚îÄ‚îÄ requirements_rag.txt (depend√™ncias)\")\n",
        "print(\"\\nüöÄ Para rodar: streamlit run rag_app.py\")\n",
        "print(\"\\nüí° Este √© um exemplo completo de RAG em produ√ß√£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Resumo e Pr√≥ximos Passos\n\nCaramba! Que jornada incr√≠vel! Hoje aprendemos a transformar nossos projetos de IA em aplica√ß√µes web profissionais e coloc√°-las no ar!\n\n### üéâ O que conseguimos fazer:\n\n‚úÖ **Streamlit B√°sico**: Interface web sem HTML/CSS\n‚úÖ **Seguran√ßa**: Vari√°veis de ambiente e secrets\n‚úÖ **Deploy**: Streamlit Cloud gratuito\n‚úÖ **Otimiza√ß√£o**: Cache e performance\n‚úÖ **Customiza√ß√£o**: CSS e design profissional\n‚úÖ **Multi-p√°ginas**: Apps complexas\n‚úÖ **Monitoramento**: Analytics b√°sicos\n‚úÖ **Troubleshooting**: Resolu√ß√£o de problemas\n‚úÖ **RAG em Produ√ß√£o**: Sistema completo\n\n### üöÄ Pr√≥ximos passos:\n\n1. **M√≥dulo 15**: Vamos refazer tudo na vers√£o v1.0 do LangChain\n2. **M√≥dulo 16**: LangGraph para workflows complexos\n3. **M√≥dulo 17**: LangSmith para monitoramento profissional\n\n**Dica final**: Suas aplica√ß√µes agora est√£o prontas para o mundo real! Use tudo que aprendemos para criar solu√ß√µes incr√≠veis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Gr√°fico do progresso do curso\n",
        "modules = ['Intro', 'ChatModel', 'LCEL', 'Prompts', 'Parsers', 'Chains', \n",
        "          'Memory', 'Docs', 'Vector', 'RAG', 'Agents', 'Proj1', 'Proj2', 'Deploy']\n",
        "progress = [100] * 14  # Todos os 14 m√≥dulos conclu√≠dos\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico de barras do progresso\n",
        "colors = ['#4CAF50' if p == 100 else '#FFC107' for p in progress]\n",
        "bars = ax1.bar(range(len(modules)), progress, color=colors)\n",
        "ax1.set_title('üéØ Progresso do Curso LangChain v0.3', fontsize=16, fontweight='bold')\n",
        "ax1.set_ylabel('Progresso (%)')\n",
        "ax1.set_xlabel('M√≥dulos')\n",
        "ax1.set_xticks(range(len(modules)))\n",
        "ax1.set_xticklabels(modules, rotation=45, ha='right')\n",
        "ax1.set_ylim(0, 110)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionar percentuais nas barras\n",
        "for bar, pct in zip(bars, progress):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{pct}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gr√°fico de pizza dos pr√≥ximos m√≥dulos\n",
        "remaining_modules = ['LangChain v1.0\\n(M√≥dulo 15)', 'LangGraph\\n(M√≥dulo 16)', 'LangSmith\\n(M√≥dulo 17)']\n",
        "sizes = [40, 30, 30]\n",
        "colors_pie = ['#FF9800', '#2196F3', '#9C27B0']\n",
        "explode = (0.1, 0.1, 0.1)\n",
        "\n",
        "ax2.pie(sizes, explode=explode, labels=remaining_modules, colors=colors_pie,\n",
        "        autopct='%1.0f%%', shadow=True, startangle=90)\n",
        "ax2.set_title('üîÆ Pr√≥ximos M√≥dulos', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ PARAB√âNS! Voc√™ completou 14 de 17 m√≥dulos!\")\n",
        "print(f\"üìä Progresso total: {14/17*100:.1f}%\")\n",
        "print(\"\\nüöÄ Voc√™ j√° sabe:\")\n",
        "print(\"‚úÖ Criar sistemas de IA completos\")\n",
        "print(\"‚úÖ Fazer deploy em produ√ß√£o\")\n",
        "print(\"‚úÖ Otimizar performance\")\n",
        "print(\"‚úÖ Criar interfaces profissionais\")\n",
        "print(\"\\nüéØ Faltam apenas 3 m√≥dulos para dominar completamente o LangChain!\")"
      ]
    }
  ]
}