{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Deploy de IA com Streamlit: Da Sua MÃ¡quina Para o Mundo!\n\n**MÃ³dulo 14 - Deploy e ProduÃ§Ã£o com Streamlit**\n\n![](/imagens/langchain-modulo-14_img_01.png)\n\nOpa! Chegamos no momento mais emocionante do curso! TÃ¡, mas o que adianta ter criado aqueles projetos lindos com LangChain se eles ficam sÃ³ na sua mÃ¡quina, nÃ©?\n\nÃ‰ como ter uma receita incrÃ­vel de brigadeiro mas nunca servir pra ninguÃ©m! Hoje vamos aprender a colocar suas aplicaÃ§Ãµes de IA no ar para todo mundo usar!\n\n## O que vamos ver hoje:\n- âœ… Como transformar seus projetos LangChain em apps web\n- âœ… Deploy no Streamlit Cloud (de graÃ§a!)\n- âœ… ConfiguraÃ§Ã£o de seguranÃ§a e variÃ¡veis de ambiente\n- âœ… Monitoramento e otimizaÃ§Ã£o\n- âœ… Troubleshooting dos problemas mais comuns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Por que Streamlit?\n\nTÃ¡, mas por que nÃ£o usar Flask, Django ou qualquer outro framework? Simples!\n\nStreamlit Ã© como aquele amigo que sempre facilita sua vida:\n- **Zero HTML/CSS/JavaScript**: SÃ³ Python puro!\n- **Deploy em 5 minutos**: Literalmente!\n- **Componentes prontos**: Widgets, grÃ¡ficos, tudo jÃ¡ funciona\n- **IntegraÃ§Ã£o perfeita**: Com pandas, matplotlib, plotly...\n\nÃ‰ tipo a diferenÃ§a entre fazer um bolo do zero vs usar uma mistura pronta. Os dois funcionam, mas um Ã© muito mais rÃ¡pido!\n\n**Dica!** Streamlit foi feito pensando em cientistas de dados e engenheiros de ML. Por isso funciona tÃ£o bem com nossos projetos de IA!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos instalar tudo que precisamos\n",
        "!pip install streamlit langchain google-generativeai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“± Criando Nossa Primeira App\n\nBora comeÃ§ar criando uma versÃ£o web do nosso chatbot do mÃ³dulo anterior. Ã‰ como transformar um WhatsApp pessoal em um site que todo mundo pode usar!\n\n```mermaid\ngraph LR\n    A[UsuÃ¡rio] --> B[Streamlit UI]\n    B --> C[LangChain]\n    C --> D[Gemini API]\n    D --> C\n    C --> B\n    B --> A\n```\n\n**Dica!** No Colab, vamos criar os arquivos usando magic commands. Em casa, vocÃª vai criar arquivos normais mesmo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando nossa primeira app Streamlit\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# ConfiguraÃ§Ã£o da pÃ¡gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Meu Chatbot IA\",\n",
        "    page_icon=\"ğŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# TÃ­tulo da aplicaÃ§Ã£o\n",
        "st.title(\"ğŸ¤– Chatbot com LangChain\")\n",
        "st.write(\"OlÃ¡! Eu sou seu assistente de IA. Como posso ajudar?\")\n",
        "\n",
        "# Sidebar para configuraÃ§Ãµes\n",
        "with st.sidebar:\n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    api_key = st.text_input(\"API Key do Google:\", type=\"password\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "\n",
        "# Inicializar o chat apenas se tiver API key\n",
        "if api_key:\n",
        "    # Configurar o modelo\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "    \n",
        "    # Sistema de mensagens\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    \n",
        "    # Mostrar mensagens anteriores\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "    \n",
        "    # Input do usuÃ¡rio\n",
        "    if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "        # Adicionar mensagem do usuÃ¡rio\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        \n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "        \n",
        "        # Gerar resposta\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Pensando...\"):\n",
        "                messages = [\n",
        "                    SystemMessage(content=\"VocÃª Ã© um assistente prestativo e amigÃ¡vel.\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "        \n",
        "        # Salvar resposta\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "else:\n",
        "    st.warning(\"Por favor, insira sua API Key do Google na barra lateral para comeÃ§ar!\")\n",
        "    st.info(\"ğŸ’¡ **Dica:** VocÃª pode obter sua API key em https://makersuite.google.com/app/apikey\")\n",
        "'''\n",
        "\n",
        "# Salvar o arquivo\n",
        "with open('app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"âœ… Arquivo app.py criado com sucesso!\")\n",
        "print(\"Para rodar: streamlit run app.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”‘ Gerenciando VariÃ¡veis de Ambiente\n\nTÃ¡, mas deixar a API key visÃ­vel no cÃ³digo Ã© como deixar a chave de casa debaixo do tapete com um bilhete \"chave aqui\"! ğŸ˜…\n\nVamos usar variÃ¡veis de ambiente, que Ã© o jeito profissional de guardar informaÃ§Ãµes sensÃ­veis.\n\n**Dica!** Em produÃ§Ã£o, NUNCA coloque API keys diretamente no cÃ³digo. Ã‰ questÃ£o de seguranÃ§a!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando arquivo .env para variÃ¡veis de ambiente\n",
        "env_content = '''\n",
        "GOOGLE_API_KEY=sua_api_key_aqui\n",
        "APP_TITLE=Meu Chatbot IncrÃ­vel\n",
        "DEBUG=False\n",
        "'''\n",
        "\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"ğŸ“ Arquivo .env criado!\")\n",
        "print(\"Lembre-se de:\")\n",
        "print(\"1. Adicionar sua API key real\")\n",
        "print(\"2. Nunca commitar o .env no Git\")\n",
        "print(\"3. Criar um .env.example para outros desenvolvedores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VersÃ£o melhorada da nossa app com variÃ¡veis de ambiente\n",
        "app_secure_code = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Carregar variÃ¡veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# ConfiguraÃ§Ã£o da pÃ¡gina\n",
        "st.set_page_config(\n",
        "    page_title=os.getenv(\"APP_TITLE\", \"Chatbot IA\"),\n",
        "    page_icon=\"ğŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# TÃ­tulo da aplicaÃ§Ã£o\n",
        "st.title(\"ğŸ¤– Chatbot Profissional com LangChain\")\n",
        "st.write(\"VersÃ£o segura com variÃ¡veis de ambiente!\")\n",
        "\n",
        "# Verificar se temos API key\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"âŒ API Key nÃ£o encontrada!\")\n",
        "    st.info(\"Configure a variÃ¡vel GOOGLE_API_KEY no arquivo .env\")\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar para configuraÃ§Ãµes\n",
        "with st.sidebar:\n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "    modelo = st.selectbox(\n",
        "        \"Modelo\",\n",
        "        [\"gemini-2.0-flash-exp\", \"gemini-1.5-pro\"]\n",
        "    )\n",
        "    \n",
        "    if st.button(\"ğŸ—‘ï¸ Limpar Conversa\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "\n",
        "# Configurar o modelo\n",
        "try:\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=modelo,\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "except Exception as e:\n",
        "    st.error(f\"Erro ao configurar modelo: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usuÃ¡rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usuÃ¡rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            try:\n",
        "                messages = [\n",
        "                    SystemMessage(content=\"VocÃª Ã© um assistente prestativo e amigÃ¡vel.\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "                \n",
        "                # Salvar resposta\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "                \n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro ao gerar resposta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"ğŸ’¡ **Dica:** Esta aplicaÃ§Ã£o usa LangChain + Streamlit para mÃ¡xima performance!\")\n",
        "'''\n",
        "\n",
        "# Salvar versÃ£o segura\n",
        "with open('app_secure.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_secure_code)\n",
        "\n",
        "print(\"ğŸ”’ VersÃ£o segura criada: app_secure.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Arquivo requirements.txt\n\nO requirements.txt Ã© como uma lista de compras para o servidor. Ele diz exatamente quais bibliotecas sua aplicaÃ§Ã£o precisa para funcionar.\n\nÃ‰ tipo quando vocÃª vai fazer um bolo e anota todos os ingredientes - sem isso, o servidor nÃ£o vai saber o que instalar!\n\n**Dica!** Sempre fixe as versÃµes das bibliotecas crÃ­ticas para evitar surpresas em produÃ§Ã£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criando requirements.txt\n",
        "requirements = '''\n",
        "streamlit>=1.28.0\n",
        "langchain>=0.3.0\n",
        "langchain-google-genai>=2.0.0\n",
        "python-dotenv>=1.0.0\n",
        "pandas>=2.0.0\n",
        "numpy>=1.24.0\n",
        "'''\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements.strip())\n",
        "\n",
        "print(\"ğŸ“‹ requirements.txt criado!\")\n",
        "\n",
        "# Vamos tambÃ©m criar um arquivo de configuraÃ§Ã£o do Streamlit\n",
        "config_content = '''\n",
        "[general]\n",
        "email = \"seu-email@exemplo.com\"\n",
        "\n",
        "[server]\n",
        "headless = true\n",
        "enableCORS = false\n",
        "port = 8501\n",
        "\n",
        "[theme]\n",
        "primaryColor = \"#FF6B6B\"\n",
        "backgroundColor = \"#FFFFFF\"\n",
        "secondaryBackgroundColor = \"#F0F2F6\"\n",
        "textColor = \"#262730\"\n",
        "'''\n",
        "\n",
        "# Criar pasta .streamlit se nÃ£o existir\n",
        "import os\n",
        "if not os.path.exists('.streamlit'):\n",
        "    os.makedirs('.streamlit')\n",
        "\n",
        "with open('.streamlit/config.toml', 'w') as f:\n",
        "    f.write(config_content.strip())\n",
        "\n",
        "print(\"âš™ï¸ ConfiguraÃ§Ã£o do Streamlit criada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Deploy no Streamlit Cloud\n\nAgora vem a parte mais emocionante! Vamos colocar nossa aplicaÃ§Ã£o no ar de graÃ§a!\n\nO Streamlit Cloud Ã© como um Heroku especÃ­fico para apps Streamlit. Ã‰ gratuito, fÃ¡cil de usar e se integra direto com o GitHub.\n\n![](/imagens/langchain-modulo-14_img_02.png)\n\n### Passo a passo:\n\n1. **Criar repositÃ³rio no GitHub**\n2. **Fazer upload dos arquivos**\n3. **Conectar no Streamlit Cloud**\n4. **Configurar variÃ¡veis de ambiente**\n5. **Deploy automÃ¡tico!**\n\n**Dica!** O deploy Ã© automÃ¡tico sempre que vocÃª fizer push no GitHub. Ã‰ deploy contÃ­nuo na veia!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um .gitignore para nÃ£o subir arquivos sensÃ­veis\n",
        "gitignore_content = '''\n",
        "# Arquivos de ambiente\n",
        ".env\n",
        ".env.local\n",
        ".env.*.local\n",
        "\n",
        "# Cache do Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*$py.class\n",
        "*.so\n",
        "\n",
        "# Jupyter Notebook\n",
        ".ipynb_checkpoints\n",
        "\n",
        "# Streamlit\n",
        ".streamlit/secrets.toml\n",
        "\n",
        "# IDE\n",
        ".vscode/\n",
        ".idea/\n",
        "\n",
        "# OS\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "'''\n",
        "\n",
        "with open('.gitignore', 'w') as f:\n",
        "    f.write(gitignore_content.strip())\n",
        "\n",
        "print(\"ğŸš« .gitignore criado - seus segredos estÃ£o seguros!\")\n",
        "\n",
        "# Criar um README.md explicativo\n",
        "readme_content = '''\n",
        "# ğŸ¤– Chatbot com LangChain e Streamlit\n",
        "\n",
        "Uma aplicaÃ§Ã£o web moderna que combina o poder do LangChain com a simplicidade do Streamlit!\n",
        "\n",
        "## ğŸš€ Como usar\n",
        "\n",
        "1. Clone este repositÃ³rio\n",
        "2. Instale as dependÃªncias: `pip install -r requirements.txt`\n",
        "3. Configure sua API key no arquivo `.env`\n",
        "4. Execute: `streamlit run app_secure.py`\n",
        "\n",
        "## ğŸ”§ ConfiguraÃ§Ã£o\n",
        "\n",
        "Crie um arquivo `.env` com:\n",
        "```\n",
        "GOOGLE_API_KEY=sua_api_key_aqui\n",
        "APP_TITLE=Seu TÃ­tulo Aqui\n",
        "```\n",
        "\n",
        "## ğŸ“± Deploy\n",
        "\n",
        "Esta aplicaÃ§Ã£o estÃ¡ pronta para deploy no Streamlit Cloud!\n",
        "\n",
        "## ğŸ› ï¸ Tecnologias\n",
        "\n",
        "- **Streamlit**: Interface web\n",
        "- **LangChain**: Framework de IA\n",
        "- **Google Gemini**: Modelo de linguagem\n",
        "- **Python**: Linguagem de programaÃ§Ã£o\n",
        "'''\n",
        "\n",
        "with open('README.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(readme_content.strip())\n",
        "\n",
        "print(\"ğŸ“š README.md criado - documentaÃ§Ã£o completa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Secrets no Streamlit Cloud\n\nNo Streamlit Cloud, as variÃ¡veis de ambiente sÃ£o chamadas de \"secrets\". Ã‰ como um cofre digital onde vocÃª guarda suas API keys.\n\nTÃ¡, mas como configurar? Ã‰ simples:\n\n1. VÃ¡ no painel do Streamlit Cloud\n2. Clique em \"Settings\" da sua app\n3. VÃ¡ na aba \"Secrets\"\n4. Cole suas variÃ¡veis no formato TOML\n\n**Dica!** O Streamlit Cloud usa formato TOML para secrets, nÃ£o .env!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de como acessar secrets no Streamlit Cloud\n",
        "secrets_example = '''\n",
        "import streamlit as st\n",
        "\n",
        "# No Streamlit Cloud, use st.secrets\n",
        "# Localmente, pode usar dotenv\n",
        "\n",
        "def get_api_key():\n",
        "    \"\"\"FunÃ§Ã£o para pegar API key independente do ambiente\"\"\"\n",
        "    try:\n",
        "        # Primeiro tenta pegar do Streamlit secrets\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        # Se nÃ£o conseguir, tenta do ambiente local\n",
        "        import os\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Usar a funÃ§Ã£o\n",
        "api_key = get_api_key()\n",
        "\n",
        "if api_key:\n",
        "    st.success(\"âœ… API Key carregada com sucesso!\")\n",
        "else:\n",
        "    st.error(\"âŒ API Key nÃ£o encontrada!\")\n",
        "'''\n",
        "\n",
        "print(\"Exemplo de cÃ³digo para acessar secrets:\")\n",
        "print(secrets_example)\n",
        "\n",
        "# Exemplo do arquivo secrets.toml\n",
        "secrets_toml = '''\n",
        "# .streamlit/secrets.toml\n",
        "GOOGLE_API_KEY = \"sua_api_key_aqui\"\n",
        "APP_TITLE = \"Meu Chatbot IncrÃ­vel\"\n",
        "DEBUG = false\n",
        "'''\n",
        "\n",
        "print(\"\\nğŸ“‹ Formato do secrets.toml:\")\n",
        "print(secrets_toml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Monitoramento e Analytics\n\nTÃ¡, mas depois que sua app estÃ¡ no ar, como saber se estÃ¡ tudo funcionando? Ã‰ como abrir uma loja e nÃ£o saber quantos clientes entraram!\n\nVamos adicionar algumas mÃ©tricas bÃ¡sicas para acompanhar o uso da nossa aplicaÃ§Ã£o.\n\n**Dica!** Streamlit tem analytics bÃ¡sicos grÃ¡tis, mas vocÃª pode integrar com Google Analytics para mais detalhes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VersÃ£o com monitoramento bÃ¡sico\n",
        "app_with_analytics = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Carregar variÃ¡veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# ConfiguraÃ§Ã£o da pÃ¡gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot com Analytics\",\n",
        "    page_icon=\"ğŸ“Š\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# FunÃ§Ã£o para log simples\n",
        "def log_interaction(user_input, response_length):\n",
        "    \"\"\"Log bÃ¡sico das interaÃ§Ãµes\"\"\"\n",
        "    if \"interaction_count\" not in st.session_state:\n",
        "        st.session_state.interaction_count = 0\n",
        "    \n",
        "    st.session_state.interaction_count += 1\n",
        "    \n",
        "    # Em produÃ§Ã£o, vocÃª salvaria isso em um banco de dados\n",
        "    log_data = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"user_input_length\": len(user_input),\n",
        "        \"response_length\": response_length,\n",
        "        \"interaction_number\": st.session_state.interaction_count\n",
        "    }\n",
        "    \n",
        "    return log_data\n",
        "\n",
        "# Sidebar com analytics\n",
        "with st.sidebar:\n",
        "    st.header(\"ğŸ“Š Analytics\")\n",
        "    \n",
        "    if \"interaction_count\" in st.session_state:\n",
        "        st.metric(\"InteraÃ§Ãµes\", st.session_state.interaction_count)\n",
        "    \n",
        "    if \"messages\" in st.session_state:\n",
        "        st.metric(\"Mensagens\", len(st.session_state.messages))\n",
        "    \n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "\n",
        "# TÃ­tulo da aplicaÃ§Ã£o\n",
        "st.title(\"ğŸ“Š Chatbot com Monitoramento\")\n",
        "st.write(\"Agora com analytics integrados!\")\n",
        "\n",
        "# Verificar API key\n",
        "def get_api_key():\n",
        "    try:\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "api_key = get_api_key()\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"âŒ API Key nÃ£o encontrada!\")\n",
        "    st.stop()\n",
        "\n",
        "# Configurar o modelo\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    google_api_key=api_key,\n",
        "    temperature=temperatura\n",
        ")\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usuÃ¡rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usuÃ¡rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            messages = [\n",
        "                SystemMessage(content=\"VocÃª Ã© um assistente prestativo e amigÃ¡vel.\"),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "            response = llm.invoke(messages)\n",
        "            st.write(response.content)\n",
        "            \n",
        "            # Log da interaÃ§Ã£o\n",
        "            log_data = log_interaction(prompt, len(response.content))\n",
        "            \n",
        "            # Salvar resposta\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "\n",
        "# Footer com info\n",
        "st.markdown(\"---\")\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Status\", \"ğŸŸ¢ Online\")\n",
        "\n",
        "with col2:\n",
        "    if \"interaction_count\" in st.session_state:\n",
        "        st.metric(\"InteraÃ§Ãµes\", st.session_state.interaction_count)\n",
        "\n",
        "with col3:\n",
        "    st.metric(\"Modelo\", \"Gemini 2.0\")\n",
        "'''\n",
        "\n",
        "# Salvar versÃ£o com analytics\n",
        "with open('app_analytics.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_with_analytics)\n",
        "\n",
        "print(\"ğŸ“Š VersÃ£o com analytics criada: app_analytics.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ Troubleshooting: Problemas Comuns\n\nTodo deploy tem seus perrengues! Ã‰ como dirigir - vocÃª aprende a resolver os problemas mais comuns. Aqui estÃ£o os que eu mais vejo:\n\n### ğŸ”´ Problemas e SoluÃ§Ãµes:\n\n| Problema | Causa | SoluÃ§Ã£o |\n|----------|-------|----------|\n| App nÃ£o carrega | requirements.txt errado | Verificar versÃµes das bibliotecas |\n| API Key nÃ£o funciona | Secrets mal configurados | Checar formato TOML |\n| App lenta | Modelo muito pesado | Usar modelo mais leve ou cache |\n| Memory error | Session state muito grande | Limitar histÃ³rico de mensagens |\n| Deploy falha | Arquivo muito grande | Otimizar cÃ³digo e dependÃªncias |\n\n**Dica!** Sempre teste localmente antes de fazer deploy. Ã‰ como ensaiar antes da apresentaÃ§Ã£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Script para debug e diagnÃ³stico\n",
        "debug_script = '''\n",
        "import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "st.title(\"ğŸ”§ Debug Dashboard\")\n",
        "st.write(\"InformaÃ§Ãµes para troubleshooting\")\n",
        "\n",
        "# InformaÃ§Ãµes do sistema\n",
        "st.header(\"ğŸ’» Sistema\")\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    st.write(f\"**Python:** {sys.version}\")\n",
        "    st.write(f\"**Streamlit:** {st.__version__}\")\n",
        "    st.write(f\"**Timestamp:** {datetime.now()}\")\n",
        "\n",
        "with col2:\n",
        "    st.write(f\"**Platform:** {sys.platform}\")\n",
        "    st.write(f\"**Encoding:** {sys.getdefaultencoding()}\")\n",
        "\n",
        "# Verificar variÃ¡veis de ambiente\n",
        "st.header(\"ğŸ”‘ VariÃ¡veis de Ambiente\")\n",
        "\n",
        "# Checar se API key existe (sem mostrar o valor)\n",
        "def check_api_key():\n",
        "    try:\n",
        "        key = st.secrets.get(\"GOOGLE_API_KEY\")\n",
        "        if key:\n",
        "            return f\"âœ… Encontrada (tamanho: {len(key)})\")\n",
        "        else:\n",
        "            return \"âŒ NÃ£o encontrada nos secrets\"\n",
        "    except:\n",
        "        try:\n",
        "            key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "            if key:\n",
        "                return f\"âœ… Encontrada no .env (tamanho: {len(key)})\"\n",
        "            else:\n",
        "                return \"âŒ NÃ£o encontrada no .env\"\n",
        "        except:\n",
        "            return \"âŒ Erro ao verificar\"\n",
        "\n",
        "st.write(f\"**API Key:** {check_api_key()}\")\n",
        "\n",
        "# Testar imports\n",
        "st.header(\"ğŸ“¦ Bibliotecas\")\n",
        "\n",
        "libraries = [\n",
        "    \"langchain\",\n",
        "    \"langchain_google_genai\", \n",
        "    \"dotenv\",\n",
        "    \"pandas\",\n",
        "    \"numpy\"\n",
        "]\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        __import__(lib)\n",
        "        st.write(f\"âœ… {lib}\")\n",
        "    except ImportError as e:\n",
        "        st.write(f\"âŒ {lib}: {e}\")\n",
        "\n",
        "# Session State\n",
        "st.header(\"ğŸ’¾ Session State\")\n",
        "st.write(f\"**Chaves:** {list(st.session_state.keys())}\")\n",
        "\n",
        "if st.session_state:\n",
        "    st.json(dict(st.session_state))\n",
        "else:\n",
        "    st.write(\"Session state vazio\")\n",
        "\n",
        "# BotÃ£o de teste\n",
        "if st.button(\"ğŸ§ª Test API Connection\"):\n",
        "    try:\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        \n",
        "        api_key = st.secrets.get(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
        "        \n",
        "        if api_key:\n",
        "            llm = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-2.0-flash-exp\",\n",
        "                google_api_key=api_key\n",
        "            )\n",
        "            \n",
        "            response = llm.invoke(\"Diga apenas 'OK' se vocÃª estÃ¡ funcionando\")\n",
        "            st.success(f\"âœ… API funcionando: {response.content}\")\n",
        "            \n",
        "        else:\n",
        "            st.error(\"âŒ API Key nÃ£o encontrada\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ Erro na API: {e}\")\n",
        "'''\n",
        "\n",
        "with open('debug.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(debug_script)\n",
        "\n",
        "print(\"ğŸ”§ Script de debug criado: debug.py\")\n",
        "print(\"Use: streamlit run debug.py para diagnosticar problemas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš¡ OtimizaÃ§Ã£o e Performance\n\nTÃ¡, mas sua app estÃ¡ lenta? Ã‰ como um carro que nÃ£o acelera - tem vÃ¡rias coisas que podem estar travando!\n\n### Principais otimizaÃ§Ãµes:\n\n1. **Cache de dados**: Use `@st.cache_data`\n2. **Cache de recursos**: Use `@st.cache_resource` \n3. **Lazy loading**: Carregue apenas quando necessÃ¡rio\n4. **SessÃ£o otimizada**: Limite o tamanho do session_state\n\n**Dica!** O cache do Streamlit Ã© poderoso, mas use com cuidado em funÃ§Ãµes que dependem de API keys!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VersÃ£o otimizada da nossa app\n",
        "app_optimized = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# ConfiguraÃ§Ã£o da pÃ¡gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot Otimizado\",\n",
        "    page_icon=\"âš¡\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Cache da funÃ§Ã£o de API key\n",
        "@st.cache_data\n",
        "def get_api_key():\n",
        "    \"\"\"Cache da API key para evitar recarregar\"\"\"\n",
        "    try:\n",
        "        return st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        return os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Cache do modelo (mais importante!)\n",
        "@st.cache_resource\n",
        "def get_llm(temperatura=0.7):\n",
        "    \"\"\"Cache do modelo para nÃ£o recriar a cada interaÃ§Ã£o\"\"\"\n",
        "    api_key = get_api_key()\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "\n",
        "# FunÃ§Ã£o para limitar histÃ³rico\n",
        "def limit_chat_history(messages, max_messages=20):\n",
        "    \"\"\"Limita o histÃ³rico para nÃ£o sobrecarregar a memÃ³ria\"\"\"\n",
        "    if len(messages) > max_messages:\n",
        "        # Manter apenas as Ãºltimas mensagens\n",
        "        return messages[-max_messages:]\n",
        "    return messages\n",
        "\n",
        "# TÃ­tulo da aplicaÃ§Ã£o\n",
        "st.title(\"âš¡ Chatbot Ultra Otimizado\")\n",
        "st.write(\"VersÃ£o com cache e otimizaÃ§Ãµes de performance!\")\n",
        "\n",
        "# Verificar API key\n",
        "api_key = get_api_key()\n",
        "\n",
        "if not api_key:\n",
        "    st.error(\"âŒ API Key nÃ£o encontrada!\")\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "    max_history = st.slider(\"MÃ¡ximo de mensagens\", 5, 50, 20)\n",
        "    \n",
        "    if st.button(\"ğŸ—‘ï¸ Limpar Chat\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "    \n",
        "    if st.button(\"ğŸ”„ Limpar Cache\"):\n",
        "        st.cache_data.clear()\n",
        "        st.cache_resource.clear()\n",
        "        st.success(\"Cache limpo!\")\n",
        "\n",
        "# Configurar o modelo (com cache)\n",
        "llm = get_llm(temperatura)\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Limitar histÃ³rico\n",
        "st.session_state.messages = limit_chat_history(st.session_state.messages, max_history)\n",
        "\n",
        "# Mostrar mensagens anteriores\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Input do usuÃ¡rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    # Adicionar mensagem do usuÃ¡rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Pensando...\"):\n",
        "            try:\n",
        "                # Usar apenas as Ãºltimas mensagens para contexto\n",
        "                recent_messages = st.session_state.messages[-5:] if len(st.session_state.messages) > 5 else st.session_state.messages\n",
        "                \n",
        "                # Construir contexto\n",
        "                context = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in recent_messages[:-1]])\n",
        "                \n",
        "                messages = [\n",
        "                    SystemMessage(content=f\"VocÃª Ã© um assistente prestativo. Contexto recente: {context}\"),\n",
        "                    HumanMessage(content=prompt)\n",
        "                ]\n",
        "                \n",
        "                response = llm.invoke(messages)\n",
        "                st.write(response.content)\n",
        "                \n",
        "                # Salvar resposta\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "                \n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro: {e}\")\n",
        "\n",
        "# Footer com mÃ©tricas\n",
        "st.markdown(\"---\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Mensagens\", len(st.session_state.messages))\n",
        "\n",
        "with col2:\n",
        "    st.metric(\"Temperatura\", f\"{temperatura:.1f}\")\n",
        "\n",
        "with col3:\n",
        "    st.metric(\"Limite\", max_history)\n",
        "\n",
        "with col4:\n",
        "    st.metric(\"Status\", \"ğŸŸ¢\")\n",
        "'''\n",
        "\n",
        "with open('app_optimized.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_optimized)\n",
        "\n",
        "print(\"âš¡ VersÃ£o otimizada criada: app_optimized.py\")\n",
        "print(\"Melhorias incluÃ­das: cache, limitaÃ§Ã£o de histÃ³rico, contexto otimizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¨ CustomizaÃ§Ã£o AvanÃ§ada\n\nVamos deixar nossa app com a cara profissional! Ã‰ como decorar sua casa - os mÃ³veis bÃ¡sicos funcionam, mas o estilo faz toda diferenÃ§a.\n\n![](/imagens/langchain-modulo-14_img_03.png)\n\n**Dica!** CSS customizado pode ser injetado no Streamlit usando `st.markdown` com `unsafe_allow_html=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# App com design customizado\n",
        "app_custom = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# ConfiguraÃ§Ã£o da pÃ¡gina\n",
        "st.set_page_config(\n",
        "    page_title=\"IA Assistant Pro\",\n",
        "    page_icon=\"ğŸ¨\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# CSS customizado\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Personalizar o header */\n",
        "    .main-header {\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 2rem;\n",
        "        border-radius: 10px;\n",
        "        margin-bottom: 2rem;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "    }\n",
        "    \n",
        "    /* Personalizar sidebar */\n",
        "    .css-1d391kg {\n",
        "        background-color: #f8f9fa;\n",
        "    }\n",
        "    \n",
        "    /* BotÃµes personalizados */\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        border: none;\n",
        "        border-radius: 25px;\n",
        "        padding: 0.5rem 1rem;\n",
        "        transition: all 0.3s;\n",
        "    }\n",
        "    \n",
        "    .stButton > button:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "    }\n",
        "    \n",
        "    /* Chat messages */\n",
        "    .user-message {\n",
        "        background-color: #e3f2fd;\n",
        "        padding: 1rem;\n",
        "        border-radius: 15px;\n",
        "        margin: 0.5rem 0;\n",
        "        border-left: 4px solid #2196f3;\n",
        "    }\n",
        "    \n",
        "    .assistant-message {\n",
        "        background-color: #f3e5f5;\n",
        "        padding: 1rem;\n",
        "        border-radius: 15px;\n",
        "        margin: 0.5rem 0;\n",
        "        border-left: 4px solid #9c27b0;\n",
        "    }\n",
        "    \n",
        "    /* Footer */\n",
        "    .footer {\n",
        "        position: fixed;\n",
        "        left: 0;\n",
        "        bottom: 0;\n",
        "        width: 100%;\n",
        "        background-color: #667eea;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        padding: 10px;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Header customizado\n",
        "st.markdown(\"\"\"\n",
        "<div class=\"main-header\">\n",
        "    <h1>ğŸ¨ IA Assistant Pro</h1>\n",
        "    <p>Powered by LangChain & Streamlit</p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Cache do modelo\n",
        "@st.cache_resource\n",
        "def get_llm(temperatura=0.7):\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=temperatura\n",
        "    )\n",
        "\n",
        "# Sidebar elegante\n",
        "with st.sidebar:\n",
        "    st.markdown(\"### âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    \n",
        "    temperatura = st.slider(\n",
        "        \"ğŸ¯ Criatividade\", \n",
        "        0.0, 1.0, 0.7,\n",
        "        help=\"Controla a criatividade das respostas\"\n",
        "    )\n",
        "    \n",
        "    modo = st.selectbox(\n",
        "        \"ğŸ­ Modo de Conversa\",\n",
        "        [\"AmigÃ¡vel\", \"Profissional\", \"Criativo\", \"TÃ©cnico\"]\n",
        "    )\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    \n",
        "    if st.button(\"ğŸ—‘ï¸ Nova Conversa\", key=\"clear\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### ğŸ“Š EstatÃ­sticas\")\n",
        "    \n",
        "    if \"messages\" in st.session_state:\n",
        "        total_msgs = len(st.session_state.messages)\n",
        "        user_msgs = sum(1 for msg in st.session_state.messages if msg[\"role\"] == \"user\")\n",
        "        \n",
        "        st.metric(\"Total de mensagens\", total_msgs)\n",
        "        st.metric(\"Suas mensagens\", user_msgs)\n",
        "        st.metric(\"Respostas da IA\", total_msgs - user_msgs)\n",
        "\n",
        "# Definir persona baseada no modo\n",
        "personas = {\n",
        "    \"AmigÃ¡vel\": \"VocÃª Ã© um assistente amigÃ¡vel e descontraÃ­do, que usa emojis e linguagem casual.\",\n",
        "    \"Profissional\": \"VocÃª Ã© um assistente profissional, formal e preciso em suas respostas.\",\n",
        "    \"Criativo\": \"VocÃª Ã© um assistente criativo, que ama usar analogias e exemplos interessantes.\",\n",
        "    \"TÃ©cnico\": \"VocÃª Ã© um assistente tÃ©cnico especializado, que fornece respostas detalhadas e precisas.\"\n",
        "}\n",
        "\n",
        "# Verificar API e configurar modelo\n",
        "try:\n",
        "    llm = get_llm(temperatura)\n",
        "except Exception as e:\n",
        "    st.error(f\"âŒ Erro ao configurar IA: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Sistema de mensagens\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Container para o chat\n",
        "chat_container = st.container()\n",
        "\n",
        "with chat_container:\n",
        "    # Mostrar mensagens anteriores\n",
        "    for message in st.session_state.messages:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"user-message\">\n",
        "                <strong>ğŸ§‘ VocÃª:</strong><br>\n",
        "                {message[\"content\"]}\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"assistant-message\">\n",
        "                <strong>ğŸ¤– Assistente:</strong><br>\n",
        "                {message[\"content\"]}\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Input do usuÃ¡rio\n",
        "if prompt := st.chat_input(\"Digite sua mensagem aqui... ğŸ’¬\"):\n",
        "    # Adicionar mensagem do usuÃ¡rio\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    # Gerar resposta\n",
        "    with st.spinner(\"ğŸ¤” Pensando...\"):\n",
        "        try:\n",
        "            messages = [\n",
        "                SystemMessage(content=personas[modo]),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "            \n",
        "            response = llm.invoke(messages)\n",
        "            \n",
        "            # Salvar resposta\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "            \n",
        "            # Recarregar para mostrar nova mensagem\n",
        "            st.rerun()\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro ao gerar resposta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"\"\"\n",
        "<div style=\"text-align: center; padding: 2rem; margin-top: 3rem; border-top: 1px solid #eee;\">\n",
        "    <p>ğŸš€ Feito com <strong>LangChain</strong> + <strong>Streamlit</strong></p>\n",
        "    <p><small>Deploy profissional em produÃ§Ã£o!</small></p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "'''\n",
        "\n",
        "with open('app_custom.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(app_custom)\n",
        "\n",
        "print(\"ğŸ¨ App customizada criada: app_custom.py\")\n",
        "print(\"Features: CSS customizado, personas, mÃ©tricas, design profissional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“± App Multi-PÃ¡gina\n\nTÃ¡, mas e se vocÃª quiser criar uma aplicaÃ§Ã£o mais complexa? Com vÃ¡rias pÃ¡ginas, diferentes funcionalidades?\n\nÃ‰ como construir uma casa com vÃ¡rios cÃ´modos - cada um tem sua funÃ§Ã£o especÃ­fica!\n\nStreamlit 1.10+ tem suporte nativo para multi-pÃ¡ginas. Ã‰ sÃ³ criar uma pasta `pages/` e colocar os arquivos lÃ¡!\n\n**Dica!** Cada arquivo na pasta `pages/` vira automaticamente uma pÃ¡gina no seu app!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Criar pasta pages se nÃ£o existir\n",
        "if not os.path.exists('pages'):\n",
        "    os.makedirs('pages')\n",
        "\n",
        "# PÃ¡gina principal (main.py)\n",
        "main_page = '''\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"IA Platform\",\n",
        "    page_icon=\"ğŸ \",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸ  IA Platform - Home\")\n",
        "st.write(\"Bem-vindo Ã  nossa plataforma de IA!\")\n",
        "\n",
        "# Cards de navegaÃ§Ã£o\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "with col1:\n",
        "    st.markdown(\"\"\"\n",
        "    ### ğŸ’¬ Chatbot\n",
        "    Converse com nossa IA avanÃ§ada\n",
        "    \n",
        "    [ğŸ‘‰ Acessar Chatbot](Chatbot)\n",
        "    \"\"\")\n",
        "\n",
        "with col2:\n",
        "    st.markdown(\"\"\"\n",
        "    ### ğŸ“Š Analytics\n",
        "    Veja estatÃ­sticas de uso\n",
        "    \n",
        "    [ğŸ‘‰ Ver Analytics](Analytics)\n",
        "    \"\"\")\n",
        "\n",
        "with col3:\n",
        "    st.markdown(\"\"\"\n",
        "    ### âš™ï¸ ConfiguraÃ§Ãµes\n",
        "    Configure sua experiÃªncia\n",
        "    \n",
        "    [ğŸ‘‰ Configurar](ConfiguraÃ§Ãµes)\n",
        "    \"\"\")\n",
        "\n",
        "# SeÃ§Ã£o de recursos\n",
        "st.markdown(\"---\")\n",
        "st.header(\"ğŸš€ Recursos DisponÃ­veis\")\n",
        "\n",
        "features = [\n",
        "    \"ğŸ¤– IA conversacional avanÃ§ada\",\n",
        "    \"ğŸ“Š Analytics em tempo real\", \n",
        "    \"ğŸ”’ SeguranÃ§a de dados\",\n",
        "    \"âš¡ Performance otimizada\",\n",
        "    \"ğŸ¨ Interface customizÃ¡vel\",\n",
        "    \"ğŸ“± Responsivo e moderno\"\n",
        "]\n",
        "\n",
        "for feature in features:\n",
        "    st.write(feature)\n",
        "'''\n",
        "\n",
        "# PÃ¡gina do Chatbot\n",
        "chatbot_page = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Chatbot\",\n",
        "    page_icon=\"ğŸ’¬\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸ’¬ Chatbot Inteligente\")\n",
        "\n",
        "@st.cache_resource\n",
        "def get_llm():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "# Sistema de chat (versÃ£o simplificada)\n",
        "if \"chat_messages\" not in st.session_state:\n",
        "    st.session_state.chat_messages = []\n",
        "\n",
        "for message in st.session_state.chat_messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Digite sua mensagem...\"):\n",
        "    st.session_state.chat_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    \n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        try:\n",
        "            llm = get_llm()\n",
        "            response = llm.invoke([HumanMessage(content=prompt)])\n",
        "            st.write(response.content)\n",
        "            st.session_state.chat_messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro: {e}\")\n",
        "'''\n",
        "\n",
        "# PÃ¡gina de Analytics\n",
        "analytics_page = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Analytics\",\n",
        "    page_icon=\"ğŸ“Š\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸ“Š Analytics Dashboard\")\n",
        "\n",
        "# Dados fictÃ­cios para demonstraÃ§Ã£o\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D')\n",
        "users = np.random.randint(50, 200, len(dates))\n",
        "messages = np.random.randint(100, 500, len(dates))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Data': dates,\n",
        "    'UsuÃ¡rios': users,\n",
        "    'Mensagens': messages\n",
        "})\n",
        "\n",
        "# MÃ©tricas principais\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"UsuÃ¡rios Ativos\", f\"{users[-1]}\", f\"{users[-1] - users[-2]:+d}\")\n",
        "\n",
        "with col2:\n",
        "    st.metric(\"Mensagens Hoje\", f\"{messages[-1]}\", f\"{messages[-1] - messages[-2]:+d}\")\n",
        "    \n",
        "with col3:\n",
        "    st.metric(\"Total Mensal\", f\"{df['Mensagens'].sum():,}\")\n",
        "    \n",
        "with col4:\n",
        "    st.metric(\"MÃ©dia/Dia\", f\"{df['Mensagens'].mean():.0f}\")\n",
        "\n",
        "# GrÃ¡ficos\n",
        "st.header(\"ğŸ“ˆ TendÃªncias\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"UsuÃ¡rios\", \"Mensagens\"])\n",
        "\n",
        "with tab1:\n",
        "    st.line_chart(df.set_index('Data')['UsuÃ¡rios'])\n",
        "    \n",
        "with tab2:\n",
        "    st.line_chart(df.set_index('Data')['Mensagens'])\n",
        "\n",
        "# Tabela de dados\n",
        "st.header(\"ğŸ“‹ Dados Detalhados\")\n",
        "st.dataframe(df, use_container_width=True)\n",
        "'''\n",
        "\n",
        "# PÃ¡gina de ConfiguraÃ§Ãµes\n",
        "config_page = '''\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"ConfiguraÃ§Ãµes\",\n",
        "    page_icon=\"âš™ï¸\"\n",
        ")\n",
        "\n",
        "st.title(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "\n",
        "# ConfiguraÃ§Ãµes do usuÃ¡rio\n",
        "st.header(\"ğŸ‘¤ Perfil do UsuÃ¡rio\")\n",
        "\n",
        "with st.form(\"user_config\"):\n",
        "    name = st.text_input(\"Nome\", value=st.session_state.get('user_name', ''))\n",
        "    email = st.text_input(\"Email\", value=st.session_state.get('user_email', ''))\n",
        "    theme = st.selectbox(\"Tema\", [\"Claro\", \"Escuro\", \"Auto\"])\n",
        "    \n",
        "    if st.form_submit_button(\"ğŸ’¾ Salvar\"):\n",
        "        st.session_state.user_name = name\n",
        "        st.session_state.user_email = email\n",
        "        st.session_state.theme = theme\n",
        "        st.success(\"ConfiguraÃ§Ãµes salvas!\")\n",
        "\n",
        "st.header(\"ğŸ¤– ConfiguraÃ§Ãµes da IA\")\n",
        "\n",
        "temperatura = st.slider(\"Criatividade\", 0.0, 1.0, 0.7)\n",
        "max_tokens = st.number_input(\"MÃ¡ximo de tokens\", 100, 4000, 1000)\n",
        "modelo = st.selectbox(\"Modelo\", [\"gemini-2.0-flash-exp\", \"gemini-1.5-pro\"])\n",
        "\n",
        "if st.button(\"ğŸ”„ Resetar ConfiguraÃ§Ãµes\"):\n",
        "    for key in list(st.session_state.keys()):\n",
        "        del st.session_state[key]\n",
        "    st.success(\"ConfiguraÃ§Ãµes resetadas!\")\n",
        "    st.rerun()\n",
        "\n",
        "st.header(\"ğŸ”§ InformaÃ§Ãµes do Sistema\")\n",
        "\n",
        "info_col1, info_col2 = st.columns(2)\n",
        "\n",
        "with info_col1:\n",
        "    st.write(\"**VersÃ£o:** 1.0.0\")\n",
        "    st.write(\"**Framework:** Streamlit + LangChain\")\n",
        "    st.write(\"**Modelo:** Gemini 2.0\")\n",
        "    \n",
        "with info_col2:\n",
        "    st.write(\"**Status:** ğŸŸ¢ Online\")\n",
        "    st.write(\"**Uptime:** 99.9%\")\n",
        "    st.write(\"**RegiÃ£o:** Global\")\n",
        "'''\n",
        "\n",
        "# Salvar arquivos\n",
        "with open('main.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(main_page)\n",
        "\n",
        "with open('pages/1_ğŸ’¬_Chatbot.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(chatbot_page)\n",
        "\n",
        "with open('pages/2_ğŸ“Š_Analytics.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(analytics_page)\n",
        "\n",
        "with open('pages/3_âš™ï¸_ConfiguraÃ§Ãµes.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(config_page)\n",
        "\n",
        "print(\"ğŸ“± App multi-pÃ¡gina criada!\")\n",
        "print(\"Estrutura:\")\n",
        "print(\"â”œâ”€â”€ main.py (pÃ¡gina principal)\")\n",
        "print(\"â””â”€â”€ pages/\")\n",
        "print(\"    â”œâ”€â”€ 1_ğŸ’¬_Chatbot.py\")\n",
        "print(\"    â”œâ”€â”€ 2_ğŸ“Š_Analytics.py\")\n",
        "print(\"    â””â”€â”€ 3_âš™ï¸_ConfiguraÃ§Ãµes.py\")\n",
        "print(\"\\nPara rodar: streamlit run main.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cio PrÃ¡tico: Seu Primeiro Deploy\n\nBora colocar a mÃ£o na massa! Vamos criar uma aplicaÃ§Ã£o personalizada e fazer o deploy completo.\n\n### Desafio:\nCriar uma aplicaÃ§Ã£o que combine:\n- Um dos projetos que fizemos nos mÃ³dulos anteriores (RAG, Agents, etc.)\n- Interface Streamlit profissional\n- Deploy no Streamlit Cloud\n\n**Tempo estimado:** 30 minutos\n\n![](/imagens/langchain-modulo-14_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template para o exercÃ­cio\n",
        "exercise_template = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "# Importe aqui as bibliotecas do seu projeto escolhido\n",
        "# Ex: para RAG -> from langchain.vectorstores import FAISS\n",
        "# Ex: para Agents -> from langchain.agents import initialize_agent\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Meu Projeto IA\",\n",
        "    page_icon=\"ğŸš€\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸš€ Meu Projeto de IA em ProduÃ§Ã£o\")\n",
        "st.write(\"Baseado no [ESCOLHA: RAG/Agents/Memory/etc.] do curso LangChain\")\n",
        "\n",
        "# TODO: Implementar seu projeto aqui\n",
        "# Dicas:\n",
        "# 1. Use @st.cache_resource para objetos pesados\n",
        "# 2. Use @st.cache_data para dados que nÃ£o mudam\n",
        "# 3. Adicione tratamento de erros\n",
        "# 4. Coloque configuraÃ§Ãµes na sidebar\n",
        "# 5. Use session_state para manter estado\n",
        "\n",
        "# Exemplo de estrutura:\n",
        "with st.sidebar:\n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    # Suas configuraÃ§Ãµes aqui\n",
        "\n",
        "# Ãrea principal\n",
        "if st.button(\"ğŸ§ª Testar Funcionalidade\"):\n",
        "    st.info(\"Implemente aqui a funcionalidade principal do seu projeto!\")\n",
        "    # Seu cÃ³digo aqui\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"ğŸ’¡ **Projeto desenvolvido no curso LangChain v0.3**\")\n",
        "'''\n",
        "\n",
        "with open('exercise_template.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(exercise_template)\n",
        "\n",
        "print(\"ğŸ“ Template do exercÃ­cio criado: exercise_template.py\")\n",
        "print(\"\\nğŸ¯ DESAFIO:\")\n",
        "print(\"1. Escolha um projeto dos mÃ³dulos anteriores\")\n",
        "print(\"2. Adapte para funcionar no Streamlit\")\n",
        "print(\"3. Adicione interface profissional\")\n",
        "print(\"4. Crie repositÃ³rio no GitHub\")\n",
        "print(\"5. FaÃ§a deploy no Streamlit Cloud\")\n",
        "print(\"\\nğŸ’ª VocÃª consegue! Ã‰ hora de colocar sua IA no ar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ExercÃ­cio Extra: RAG em ProduÃ§Ã£o\n\nVamos pegar nosso sistema RAG do mÃ³dulo 10 e transformar em uma aplicaÃ§Ã£o web completa!\n\nEste Ã© um exercÃ­cio mais avanÃ§ado que combina tudo que aprendemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AplicaÃ§Ã£o RAG completa para produÃ§Ã£o\n",
        "rag_app = '''\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"RAG System Pro\",\n",
        "    page_icon=\"ğŸ“š\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"ğŸ“š Sistema RAG Profissional\")\n",
        "st.write(\"FaÃ§a upload de documentos e converse com eles usando IA!\")\n",
        "\n",
        "# Cache das funÃ§Ãµes pesadas\n",
        "@st.cache_resource\n",
        "def get_llm():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        google_api_key=api_key,\n",
        "        temperature=0.3  # Menos criativo para RAG\n",
        "    )\n",
        "\n",
        "@st.cache_resource\n",
        "def get_embeddings():\n",
        "    try:\n",
        "        api_key = st.secrets[\"GOOGLE_API_KEY\"]\n",
        "    except:\n",
        "        load_dotenv()\n",
        "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    \n",
        "    return GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/embedding-001\",\n",
        "        google_api_key=api_key\n",
        "    )\n",
        "\n",
        "def process_documents(uploaded_files):\n",
        "    \"\"\"Processa documentos uploaded\"\"\"\n",
        "    documents = []\n",
        "    \n",
        "    for uploaded_file in uploaded_files:\n",
        "        # Salvar arquivo temporariamente\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{uploaded_file.name.split('.')[-1]}\") as tmp_file:\n",
        "            tmp_file.write(uploaded_file.getvalue())\n",
        "            tmp_path = tmp_file.name\n",
        "        \n",
        "        try:\n",
        "            # Carregar baseado na extensÃ£o\n",
        "            if uploaded_file.name.endswith('.pdf'):\n",
        "                loader = PyPDFLoader(tmp_path)\n",
        "            else:\n",
        "                loader = TextLoader(tmp_path, encoding='utf-8')\n",
        "            \n",
        "            docs = loader.load()\n",
        "            \n",
        "            # Adicionar metadata\n",
        "            for doc in docs:\n",
        "                doc.metadata['source'] = uploaded_file.name\n",
        "            \n",
        "            documents.extend(docs)\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro ao processar {uploaded_file.name}: {e}\")\n",
        "        finally:\n",
        "            # Limpar arquivo temporÃ¡rio\n",
        "            os.unlink(tmp_path)\n",
        "    \n",
        "    return documents\n",
        "\n",
        "@st.cache_data\n",
        "def create_vectorstore(_documents):\n",
        "    \"\"\"Cria vectorstore dos documentos\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    \n",
        "    texts = text_splitter.split_documents(_documents)\n",
        "    embeddings = get_embeddings()\n",
        "    \n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"ğŸ“ Upload de Documentos\")\n",
        "    \n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Escolha seus arquivos\",\n",
        "        accept_multiple_files=True,\n",
        "        type=['pdf', 'txt', 'md']\n",
        "    )\n",
        "    \n",
        "    if uploaded_files:\n",
        "        st.success(f\"{len(uploaded_files)} arquivo(s) carregado(s)\")\n",
        "        \n",
        "        if st.button(\"ğŸ”„ Processar Documentos\"):\n",
        "            with st.spinner(\"Processando documentos...\"):\n",
        "                documents = process_documents(uploaded_files)\n",
        "                \n",
        "                if documents:\n",
        "                    vectorstore = create_vectorstore(documents)\n",
        "                    st.session_state.vectorstore = vectorstore\n",
        "                    st.session_state.documents = documents\n",
        "                    st.success(\"Documentos processados com sucesso!\")\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"âš™ï¸ ConfiguraÃ§Ãµes\")\n",
        "    \n",
        "    k_results = st.slider(\"Resultados por busca\", 1, 10, 4)\n",
        "    show_sources = st.checkbox(\"Mostrar fontes\", True)\n",
        "\n",
        "# Ãrea principal\n",
        "if \"vectorstore\" not in st.session_state:\n",
        "    st.info(\"ğŸ‘ˆ FaÃ§a upload de documentos na barra lateral para comeÃ§ar!\")\n",
        "    \n",
        "    # Exemplo de uso\n",
        "    st.markdown(\"\"\"\n",
        "    ### ğŸš€ Como usar:\n",
        "    \n",
        "    1. **Upload**: Envie arquivos PDF ou TXT na barra lateral\n",
        "    2. **Processe**: Clique em \"Processar Documentos\"\n",
        "    3. **Converse**: FaÃ§a perguntas sobre o conteÃºdo\n",
        "    4. **Explore**: Use as configuraÃ§Ãµes para ajustar os resultados\n",
        "    \n",
        "    ### ğŸ“‹ Formatos suportados:\n",
        "    - ğŸ“„ PDF\n",
        "    - ğŸ“ TXT\n",
        "    - ğŸ“‹ Markdown (MD)\n",
        "    \n",
        "    ### ğŸ’¡ Dicas:\n",
        "    - Documente menores = respostas mais precisas\n",
        "    - Use perguntas especÃ­ficas\n",
        "    - Ative \"Mostrar fontes\" para verificar as referÃªncias\n",
        "    \"\"\")\n",
        "    \n",
        "else:\n",
        "    # Sistema de chat RAG\n",
        "    st.header(\"ğŸ’¬ Chat com Documentos\")\n",
        "    \n",
        "    # InformaÃ§Ãµes dos documentos\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "        st.metric(\"Documentos\", len(st.session_state.documents))\n",
        "    \n",
        "    with col2:\n",
        "        total_chars = sum(len(doc.page_content) for doc in st.session_state.documents)\n",
        "        st.metric(\"Caracteres\", f\"{total_chars:,}\")\n",
        "    \n",
        "    with col3:\n",
        "        st.metric(\"Chunks\", st.session_state.vectorstore.index.ntotal)\n",
        "    \n",
        "    # Chat\n",
        "    if \"rag_messages\" not in st.session_state:\n",
        "        st.session_state.rag_messages = []\n",
        "    \n",
        "    # Mostrar mensagens\n",
        "    for message in st.session_state.rag_messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "            \n",
        "            if \"sources\" in message and show_sources:\n",
        "                with st.expander(\"ğŸ“š Ver fontes\"):\n",
        "                    for i, source in enumerate(message[\"sources\"], 1):\n",
        "                        st.write(f\"**Fonte {i}:** {source['source']}\")\n",
        "                        st.write(f\"*ConteÃºdo:* {source['content'][:200]}...\")\n",
        "                        st.markdown(\"---\")\n",
        "    \n",
        "    # Input\n",
        "    if prompt := st.chat_input(\"FaÃ§a uma pergunta sobre os documentos...\"):\n",
        "        # Adicionar pergunta\n",
        "        st.session_state.rag_messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        \n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "        \n",
        "        # Gerar resposta\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Buscando informaÃ§Ãµes...\"):\n",
        "                try:\n",
        "                    # Configurar RAG chain\n",
        "                    llm = get_llm()\n",
        "                    retriever = st.session_state.vectorstore.as_retriever(\n",
        "                        search_kwargs={\"k\": k_results}\n",
        "                    )\n",
        "                    \n",
        "                    qa_chain = RetrievalQA.from_chain_type(\n",
        "                        llm=llm,\n",
        "                        chain_type=\"stuff\",\n",
        "                        retriever=retriever,\n",
        "                        return_source_documents=True\n",
        "                    )\n",
        "                    \n",
        "                    # Executar query\n",
        "                    result = qa_chain({\"query\": prompt})\n",
        "                    \n",
        "                    # Mostrar resposta\n",
        "                    st.write(result[\"result\"])\n",
        "                    \n",
        "                    # Preparar fontes\n",
        "                    sources = []\n",
        "                    for doc in result[\"source_documents\"]:\n",
        "                        sources.append({\n",
        "                            \"source\": doc.metadata.get(\"source\", \"Desconhecido\"),\n",
        "                            \"content\": doc.page_content\n",
        "                        })\n",
        "                    \n",
        "                    # Salvar mensagem\n",
        "                    st.session_state.rag_messages.append({\n",
        "                        \"role\": \"assistant\", \n",
        "                        \"content\": result[\"result\"],\n",
        "                        \"sources\": sources\n",
        "                    })\n",
        "                    \n",
        "                    # Mostrar fontes se habilitado\n",
        "                    if show_sources:\n",
        "                        with st.expander(\"ğŸ“š Ver fontes\"):\n",
        "                            for i, source in enumerate(sources, 1):\n",
        "                                st.write(f\"**Fonte {i}:** {source['source']}\")\n",
        "                                st.write(f\"*ConteÃºdo:* {source['content'][:200]}...\")\n",
        "                                st.markdown(\"---\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    st.error(f\"Erro ao processar pergunta: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"ğŸš€ **Sistema RAG Profissional** - Powered by LangChain + Streamlit\")\n",
        "'''\n",
        "\n",
        "with open('rag_app.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(rag_app)\n",
        "\n",
        "# Requirements especÃ­fico para RAG\n",
        "rag_requirements = '''\n",
        "streamlit>=1.28.0\n",
        "langchain>=0.3.0\n",
        "langchain-google-genai>=2.0.0\n",
        "python-dotenv>=1.0.0\n",
        "faiss-cpu>=1.7.4\n",
        "PyPDF2>=3.0.1\n",
        "pypdf>=3.17.0\n",
        "'''\n",
        "\n",
        "with open('requirements_rag.txt', 'w') as f:\n",
        "    f.write(rag_requirements.strip())\n",
        "\n",
        "print(\"ğŸ“š Sistema RAG completo criado!\")\n",
        "print(\"Arquivos:\")\n",
        "print(\"â”œâ”€â”€ rag_app.py (aplicaÃ§Ã£o principal)\")\n",
        "print(\"â””â”€â”€ requirements_rag.txt (dependÃªncias)\")\n",
        "print(\"\\nğŸš€ Para rodar: streamlit run rag_app.py\")\n",
        "print(\"\\nğŸ’¡ Este Ã© um exemplo completo de RAG em produÃ§Ã£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Resumo e PrÃ³ximos Passos\n\nCaramba! Que jornada incrÃ­vel! Hoje aprendemos a transformar nossos projetos de IA em aplicaÃ§Ãµes web profissionais e colocÃ¡-las no ar!\n\n### ğŸ‰ O que conseguimos fazer:\n\nâœ… **Streamlit BÃ¡sico**: Interface web sem HTML/CSS\nâœ… **SeguranÃ§a**: VariÃ¡veis de ambiente e secrets\nâœ… **Deploy**: Streamlit Cloud gratuito\nâœ… **OtimizaÃ§Ã£o**: Cache e performance\nâœ… **CustomizaÃ§Ã£o**: CSS e design profissional\nâœ… **Multi-pÃ¡ginas**: Apps complexas\nâœ… **Monitoramento**: Analytics bÃ¡sicos\nâœ… **Troubleshooting**: ResoluÃ§Ã£o de problemas\nâœ… **RAG em ProduÃ§Ã£o**: Sistema completo\n\n### ğŸš€ PrÃ³ximos passos:\n\n1. **MÃ³dulo 15**: Vamos refazer tudo na versÃ£o v1.0 do LangChain\n2. **MÃ³dulo 16**: LangGraph para workflows complexos\n3. **MÃ³dulo 17**: LangSmith para monitoramento profissional\n\n**Dica final**: Suas aplicaÃ§Ãµes agora estÃ£o prontas para o mundo real! Use tudo que aprendemos para criar soluÃ§Ãµes incrÃ­veis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# GrÃ¡fico do progresso do curso\n",
        "modules = ['Intro', 'ChatModel', 'LCEL', 'Prompts', 'Parsers', 'Chains', \n",
        "          'Memory', 'Docs', 'Vector', 'RAG', 'Agents', 'Proj1', 'Proj2', 'Deploy']\n",
        "progress = [100] * 14  # Todos os 14 mÃ³dulos concluÃ­dos\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# GrÃ¡fico de barras do progresso\n",
        "colors = ['#4CAF50' if p == 100 else '#FFC107' for p in progress]\n",
        "bars = ax1.bar(range(len(modules)), progress, color=colors)\n",
        "ax1.set_title('ğŸ¯ Progresso do Curso LangChain v0.3', fontsize=16, fontweight='bold')\n",
        "ax1.set_ylabel('Progresso (%)')\n",
        "ax1.set_xlabel('MÃ³dulos')\n",
        "ax1.set_xticks(range(len(modules)))\n",
        "ax1.set_xticklabels(modules, rotation=45, ha='right')\n",
        "ax1.set_ylim(0, 110)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Adicionar percentuais nas barras\n",
        "for bar, pct in zip(bars, progress):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{pct}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# GrÃ¡fico de pizza dos prÃ³ximos mÃ³dulos\n",
        "remaining_modules = ['LangChain v1.0\\n(MÃ³dulo 15)', 'LangGraph\\n(MÃ³dulo 16)', 'LangSmith\\n(MÃ³dulo 17)']\n",
        "sizes = [40, 30, 30]\n",
        "colors_pie = ['#FF9800', '#2196F3', '#9C27B0']\n",
        "explode = (0.1, 0.1, 0.1)\n",
        "\n",
        "ax2.pie(sizes, explode=explode, labels=remaining_modules, colors=colors_pie,\n",
        "        autopct='%1.0f%%', shadow=True, startangle=90)\n",
        "ax2.set_title('ğŸ”® PrÃ³ximos MÃ³dulos', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ‰ PARABÃ‰NS! VocÃª completou 14 de 17 mÃ³dulos!\")\n",
        "print(f\"ğŸ“Š Progresso total: {14/17*100:.1f}%\")\n",
        "print(\"\\nğŸš€ VocÃª jÃ¡ sabe:\")\n",
        "print(\"âœ… Criar sistemas de IA completos\")\n",
        "print(\"âœ… Fazer deploy em produÃ§Ã£o\")\n",
        "print(\"âœ… Otimizar performance\")\n",
        "print(\"âœ… Criar interfaces profissionais\")\n",
        "print(\"\\nğŸ¯ Faltam apenas 3 mÃ³dulos para dominar completamente o LangChain!\")"
      ]
    }
  ]
}