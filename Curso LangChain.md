# üöÄ **LANGCHAIN: O GUIA COMPLETO PARA IA QUE FUNCIONA**

*Por Pedro Guth - Seu instrutor descontra√≠do de programa√ß√£o*

---

## **üìö √çNDICE**

1. [üéØ **INTRODU√á√ÉO**](#introdu√ß√£o)
2. [üîß **SETUP E CONFIGURA√á√ÉO**](#setup-e-configura√ß√£o)
3. [üí¨ **PROMPTS - A ARTE DE FALAR COM IA**](#prompts)
4. [üîó **CHAINS - CONECTANDO AS PE√áAS**](#chains)
5. [üß† **MEMORY - LEMBRANDO DAS CONVERSAS**](#memory)
6. [ü§ñ **AGENTS - OS FUNCION√ÅRIOS INTELIGENTES**](#agents)
7. [üìÑ **DOCUMENT LOADERS - IA QUE L√ä TUDO**](#document-loaders)
8. [üîç **RAG - IA QUE SABE O QUE N√ÉO SABE**](#rag)
9. [üöÄ **PROJETOS PR√ÅTICOS**](#projetos-pr√°ticos)
10. [üåê **DEPLOY E PRODU√á√ÉO**](#deploy)
11. [üöÄ **T√ìPICOS AVAN√áADOS**](#t√≥picos-avan√ßados)

---

## **üéØ INTRODU√á√ÉO**

### **T√°, mas o que √© LangChain mesmo?**

Imagina que voc√™ √© um pedreiro e tem um monte de tijolos, cimento, areia e ferramentas espalhadas pelo terreno. Voc√™ **pode** construir uma casa, mas vai ser um trabalho do caralho e vai demorar uma eternidade.

Agora imagina que algu√©m te d√° um **kit de constru√ß√£o** com tudo organizado, instru√ß√µes claras e at√© uns moldes prontos. Muito mais f√°cil, n√©?

**LangChain √© exatamente isso para IA!** üß±

Sem LangChain, voc√™ tem que:
- Conectar APIs manualmente
- Gerenciar mem√≥ria de conversas
- Implementar prompts do zero
- Fazer parsing de respostas
- E mais um monte de coisa chata

Com LangChain, voc√™ tem **componentes prontos** que se encaixam como pe√ßas de Lego. √â tipo ter um **"Lego da IA"** - voc√™ junta as pe√ßas e faz coisas incr√≠veis sem reinventar a roda.

### **Por que LangChain √© tipo um "pedreiro inteligente"?**

LangChain n√£o √© s√≥ uma biblioteca, √© um **framework completo** que te ajuda a:

1. **Organizar prompts** (como ter receitas de bolo prontas)
2. **Conectar diferentes IAs** (como ter um tradutor que fala com um analista)
3. **Lembrar de conversas** (como um gar√ßom que nunca esquece seu pedido)
4. **Usar ferramentas externas** (como dar superpoderes para a IA)
5. **Processar documentos** (como ter um assistente que l√™ tudo pra voc√™)

### **ChatGPT vs LangChain - A Diferen√ßa na Pr√°tica**

**ChatGPT**: √â como ter um amigo super inteligente, mas que s√≥ pode conversar. Ele n√£o pode:
- Acessar internet
- Executar c√≥digo
- Lembrar de conversas antigas
- Conectar com outros sistemas

**LangChain**: √â como ter um **ex√©rcito de amigos inteligentes** cada um com uma especialidade, e voc√™ √© o chefe que coordena tudo!

**üí° Dica do Pedro**: LangChain √© especialmente √∫til quando voc√™ quer fazer algo mais complexo que uma simples conversa. √â tipo a diferen√ßa entre pedir um Uber e ter um motorista particular que tamb√©m √© seu assistente pessoal.

---

## **üîß SETUP E CONFIGURA√á√ÉO**

### **üéØ Por que Google Colab?**

O Google Colab √© **perfeito** para aprender LangChain porque:
- ‚úÖ **100% gratuito**
- ‚úÖ **Sem instala√ß√£o** - funciona no navegador
- ‚úÖ **GPU gratuita** dispon√≠vel
- ‚úÖ **Compartilhamento f√°cil**
- ‚úÖ **Backup autom√°tico** no Google Drive

### **üÜì Op√ß√µes Dispon√≠veis no Colab:**

#### **1. üé≠ Mock LLM (Recomendado para Iniciantes)**
- ‚úÖ **Funciona imediatamente**
- ‚úÖ **100% gratuito**
- ‚úÖ **Respostas simuladas realistas**
- ‚úÖ **Perfeito para aprender conceitos**

#### **2. ü§ñ Google AI Studio (Recomendado para o projeto)**
- ‚úÖ **Funciona imediatamente**
- ‚úÖ **100% gratuito**
- ‚úÖ **Respostas r√°pidas**
- ‚úÖ **Perfeito para o curso**

#### **3. üåê Hugging Face (Modelos Reais)**
- ‚úÖ **30.000 requisi√ß√µes/m√™s gratuitas**
- ‚úÖ **Modelos reais de IA**
- ‚úÖ **F√°cil configura√ß√£o**
- ‚úÖ **Boa qualidade**

#### **4. üîë OpenAI (Para Quem Quiser)**
- ‚úÖ **Melhor qualidade**
- ‚úÖ **Modelos mais avan√ßados**
- ‚ùå **Custo por uso**
- ‚ùå **Precisa de API key**

### **üîß Instala√ß√£o das Depend√™ncias**

```python
# Instalando as depend√™ncias necess√°rias
!pip install --quiet langchain openai python-dotenv
!pip install --quiet langchain-community langchain-core
!pip install --quiet --upgrade langchain-huggingface transformers sentencepiece torch
!pip install --quiet huggingface_hub
!pip install --quiet langchain-openai openai
!pip install --quiet langchain-google-genai
!pip install --quiet langchain-huggingface

# Para document loaders
!pip install --quiet pypdf python-docx beautifulsoup4 requests youtube-transcript-api

# Para vector stores
!pip install --quiet chromadb faiss-cpu

# Para agents e ferramentas
!pip install --quiet wikipedia duckduckgo-search

# Para deploy e interfaces
!pip install --quiet streamlit gradio fastapi uvicorn

# Para processamento de dados
!pip install --quiet pandas numpy matplotlib seaborn
```

### **üí∞ Compara√ß√£o de Custos no Colab**

| Op√ß√£o | Custo | Limites | Qualidade | Facilidade |
|-------|-------|---------|-----------|------------|
| **Mock LLM** | $0 | Nenhum | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Hugging Face** | $0 | 30K/m√™s | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Google AI Studio** | $0 | 20K/m√™s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **OpenAI** | ~$5-20 | Sem limite | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

**üí° Dica do Pedro**: Comece com Mock LLM para aprender os conceitos. Quando estiver confort√°vel, migre para Google AI Studio ou OpenAI para respostas reais!

---

## **üí¨ PROMPTS - A ARTE DE FALAR COM IA**

### **T√°, mas o que √© um Prompt?**

Imagina que voc√™ est√° em um restaurante. Se voc√™ disser "quero comida", o gar√ßom vai ficar perdido. Mas se voc√™ disser "quero um X-Burger com batata frita e Coca-Cola", a√≠ sim ele sabe exatamente o que trazer.

**Prompt √© exatamente isso!** √â a forma como voc√™ "pede" as coisas para a IA. üçî

**Prompt ruim**: "Escreve algo sobre programa√ß√£o"
**Prompt bom**: "Escreve um artigo de 300 palavras sobre Python para iniciantes, com exemplos pr√°ticos e linguagem simples"

A diferen√ßa √© **ABISMAL**! √â como a diferen√ßa entre pedir "um carro" e pedir "um Honda Civic 2023, cor prata, autom√°tico, com ar condicionado".

### **Por que Prompts Importam Tanto?**

**Sem bons prompts**: A IA fica perdida, responde coisas gen√©ricas, n√£o entende o que voc√™ quer
**Com bons prompts**: A IA vira seu assistente pessoal, entende exatamente o que voc√™ precisa

√â como a diferen√ßa entre ter um **estagi√°rio perdido** e um **funcion√°rio experiente**! üòÖ

### **Regras de Ouro para Prompts Bons** üìã

1. **Seja espec√≠fico** - N√£o deixe a IA adivinhar
2. **Defina o formato** - Como voc√™ quer a resposta
3. **D√™ contexto** - Explique o que voc√™ precisa
4. **Use exemplos** - Mostre o que voc√™ quer
5. **Defina o tom** - Formal, informal, t√©cnico, etc.

**üí° Dica do Pedro**: Pense que voc√™ est√° explicando para um **estagi√°rio muito inteligente, mas que n√£o l√™ sua mente**!

### **Templates de Prompts - Como Ter Receitas Prontas**

Templates s√£o como **receitas de bolo** que voc√™ pode reutilizar. Em vez de escrever o prompt do zero toda vez, voc√™ cria um modelo e s√≥ muda os ingredientes.

**Exemplo pr√°tico**:
- **Sem template**: Escrever o prompt completo toda vez
- **Com template**: "Traduza {texto} do {idioma_origem} para {idioma_destino}"

√â como ter um **moldinho de bolo** - voc√™ s√≥ muda os ingredientes, mas a forma √© sempre a mesma! üç∞

```python
from langchain.prompts import PromptTemplate

# Template para tradu√ß√£o
template_traducao = PromptTemplate(
    input_variables=["texto", "idioma_origem", "idioma_destino"],
    template="""
    Traduza o seguinte texto do {idioma_origem} para {idioma_destino}:

    TEXTO: {texto}

    INSTRU√á√ïES:
    - Mantenha o tom e estilo original
    - Preserve a formata√ß√£o
    - Seja preciso e natural
    - Responda apenas com a tradu√ß√£o
    """
)
```

### **Few-Shot Examples - Ensinando com Exemplos**

Few-shot examples s√£o como **mostrar exemplos** para a IA antes de pedir algo. √â como ensinar algu√©m a cozinhar mostrando como fazer o prato primeiro.

**Analogia**: Se voc√™ quer que algu√©m aprenda a fazer bolo, voc√™:
1. **Mostra** como fazer um bolo
2. **Explica** cada passo
3. **Deixa** a pessoa fazer sozinha

Com IA √© a mesma coisa! üéÇ

### **Chain of Thought - Pensando em Voz Alta**

Chain of Thought √© como fazer a IA **pensar em voz alta**. Em vez de dar a resposta direta, ela explica o processo de pensamento.

**Por que isso √© √∫til?**
- **Transpar√™ncia**: Voc√™ v√™ como a IA chegou na resposta
- **Debugging**: Fica mais f√°cil identificar erros
- **Aprendizado**: Voc√™ entende o processo
- **Confian√ßa**: Voc√™ confia mais na resposta

√â como ter um **professor que explica o passo a passo** em vez de s√≥ dar a resposta! üß†

---

## **üîó CHAINS - CONECTANDO AS PE√áAS**

### **T√°, mas o que s√£o Chains?**

Imagina uma **linha de produ√ß√£o de uma f√°brica**. Cada m√°quina faz uma coisa espec√≠fica:
- M√°quina 1: Corta o metal
- M√°quina 2: Dobra o metal
- M√°quina 3: Pinta o metal
- M√°quina 4: Embalha o produto

O produto passa por cada m√°quina **em sequ√™ncia** e sai pronto no final.

**Chains s√£o exatamente isso para IA!** üè≠

**Chain** = Sequ√™ncia de opera√ß√µes onde **uma coisa leva √† outra**. √â como ter um **pipeline** de processamento.

### **Por que Chains s√£o Poderosas?**

**Sem Chains**: Voc√™ tem que fazer cada coisa manualmente, uma por vez
**Com Chains**: Voc√™ conecta tudo e deixa rolar automaticamente

√â como a diferen√ßa entre **fazer cada prato individualmente** e ter uma **linha de produ√ß√£o**! üçΩÔ∏è

### **LLMChain - A Chain Mais B√°sica**

LLMChain √© como a **m√°quina mais simples** da nossa linha de produ√ß√£o. Ela pega um prompt, envia para a IA e retorna a resposta.

√â como ter um **funcion√°rio que s√≥ faz uma tarefa espec√≠fica**:

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Template para tradu√ß√£o
template_traducao = PromptTemplate(
    input_variables=["texto"],
    template="Traduza o seguinte texto para portugu√™s: {texto}"
)

# NOVO: Usar operador pipe em vez de LLMChain
chain_traducao = template_traducao | llm

# Executar
resposta = chain_traducao.invoke({"texto": "Hello, how are you?"})
print(resposta.content)
```

### **SimpleSequentialChain - Conectando M√∫ltiplas Chains**

Agora vamos conectar **m√∫ltiplas m√°quinas** na nossa linha de produ√ß√£o! SimpleSequentialChain √© como ter v√°rias m√°quinas conectadas onde **a sa√≠da de uma √© a entrada da pr√≥xima**.

**Analogia**: √â como uma **linha de produ√ß√£o de pizza**:
1. **M√°quina 1**: Faz a massa
2. **M√°quina 2**: Adiciona o molho
3. **M√°quina 3**: Coloca os ingredientes
4. **M√°quina 4**: Assa a pizza

Cada m√°quina pega o resultado da anterior e adiciona sua parte! üçï

```python
# Chain 1: An√°lise de Sentimento
template_sentimento = PromptTemplate(
    input_variables=["texto"],
    template="""
    Analise o sentimento do seguinte texto e responda apenas com:
    - POSITIVO
    - NEGATIVO
    - NEUTRO

    TEXTO: {texto}
    """
)

chain_sentimento = template_sentimento | llm

# Chain 2: Gera√ß√£o de Resposta
template_resposta = PromptTemplate(
    input_variables=["sentimento"],
    template="""
    Com base no sentimento {sentimento}, gere uma resposta apropriada:

    - Se POSITIVO: Responda com entusiasmo e otimismo
    - Se NEGATIVO: Responda com empatia e sugest√µes de melhoria
    - Se NEUTRO: Responda de forma equilibrada e informativa

    Use linguagem informal e seja como o Pedro Guth.
    """
)

chain_resposta = template_resposta | llm

# Conectando as Chains em sequ√™ncia
chain_completa = chain_sentimento | chain_resposta
```

### **SequentialChain - Chains com M√∫ltiplas Entradas e Sa√≠das**

SequentialChain √© como ter uma **f√°brica mais complexa** onde:
- Voc√™ pode ter **m√∫ltiplas entradas**
- Voc√™ pode ter **m√∫ltiplas sa√≠das**
- As Chains podem **compartilhar informa√ß√µes**

**Analogia**: √â como uma **cozinha de restaurante** onde:
- Voc√™ tem v√°rios ingredientes (entradas)
- V√°rios chefs trabalham (chains)
- V√°rios pratos saem (sa√≠das)
- Os chefs compartilham informa√ß√µes entre si

### **RouterChain - O Gar√ßom Inteligente**

RouterChain √© como ter um **gar√ßom muito inteligente** que:
1. **Analisa o pedido** do cliente
2. **Decide** qual chef especializado deve preparar
3. **Envia** o pedido para o chef certo
4. **Retorna** o prato pronto

**Analogia**: √â como um **sistema de roteamento** que decide qual caminho seguir baseado no conte√∫do.

```python
# Especialista em tecnologia
tech_prompt = PromptTemplate(
    input_variables=["pergunta"],
    template="""
    Voc√™ √© um especialista em tecnologia. Responda √† pergunta:

    PERGUNTA: {pergunta}

    Use linguagem t√©cnica mas acess√≠vel. Seja como o Pedro Guth explicando.
    """
)
chain_tech = tech_prompt | llm

# Especialista em culin√°ria
culinaria_prompt = PromptTemplate(
    input_variables=["pergunta"],
    template="""
    Voc√™ √© um chef de cozinha experiente. Responda √† pergunta:

    PERGUNTA: {pergunta}

    Use linguagem culin√°ria e d√™ dicas pr√°ticas. Seja como o Pedro Guth na cozinha.
    """
)
chain_culinaria = culinaria_prompt | llm

# Router (o gar√ßom inteligente)
router_prompt = PromptTemplate(
    input_variables=["pergunta"],
    template="""
    Analise a pergunta e decida qual especialista deve responder:

    PERGUNTA: {pergunta}

    ESPECIALISTAS DISPON√çVEIS:
    - tecnologia: computadores, programa√ß√£o, apps, internet
    - culinaria: comida, receitas, cozinha, restaurantes

    Responda apenas com: tecnologia ou culinaria.
    """
)

router_chain = router_prompt | llm
```

---

## **üß† MEMORY - LEMBRANDO DAS CONVERSAS**

### **T√°, mas o que √© Memory?**

Imagina que voc√™ est√° conversando com um **gar√ßom muito bom** vs um **gar√ßom com amn√©sia**:

**Gar√ßom com amn√©sia**:
- Voc√™: "Quero um caf√©"
- Gar√ßom: "Aqui est√° seu caf√©"
- Voc√™: "Lembra que eu gosto sem a√ß√∫car?"
- Gar√ßom: "Desculpe, n√£o me lembro de voc√™..." üòÖ

**Gar√ßom com boa mem√≥ria**:
- Voc√™: "Quero um caf√©"
- Gar√ßom: "Aqui est√° seu caf√© sem a√ß√∫car, como sempre!"
- Voc√™: "Perfeito! E lembra que eu gosto quente?"
- Gar√ßom: "Claro! Extra quente, como da √∫ltima vez!" üòä

**Memory √© exatamente isso!** √â como dar **mem√≥ria** para a IA, para ela lembrar do que voc√™s conversaram antes.

### **Por que Memory √© Importante?**

**Sem Memory**: A IA esquece tudo a cada pergunta. √â como conversar com algu√©m que tem Alzheimer - voc√™ tem que explicar tudo de novo.

**Com Memory**: A IA lembra do contexto, suas prefer√™ncias, o que voc√™s falaram antes. √â como ter um **amigo que nunca esquece**!

### **Tipos de Memory**

1. **ConversationBufferMemory** - Lembra de tudo (como uma v√≥ com boa mem√≥ria)
2. **ConversationSummaryMemory** - Faz resumos (como um assistente que anota)
3. **VectorStoreMemory** - Lembra por similaridade (como um c√©rebro organizado)
4. **EntityMemory** - Lembra de pessoas/coisas espec√≠ficas (como um CRM)

### **ConversationBufferMemory - A V√≥ com Boa Mem√≥ria**

ConversationBufferMemory √© como ter uma **v√≥ com mem√≥ria de elefante** - ela lembra de **TUDO** que voc√™s conversaram, palavra por palavra.

**Vantagens**:
- Lembra de tudo
- Contexto completo
- Precis√£o total

**Desvantagens**:
- Pode ficar pesado com conversas longas
- Consome mais tokens
- Pode ficar confuso com muito contexto

```python
from langchain.memory import ConversationBufferMemory

# Criando ConversationBufferMemory
memory = ConversationBufferMemory()

# Usar memory diretamente com as mensagens
memory.save_context({"input": "Ol√°!"}, {"output": "Oi! Como vai?"})

# Carregar contexto anterior
chat_history = memory.load_memory_variables({})

# Criar mensagem com contexto
if chat_history.get("history"):
    mensagem_completa = f"Hist√≥rico: {chat_history['history']}\n\nPergunta atual: {pergunta}"
else:
    mensagem_completa = pergunta

# Enviar para o LLM
response = llm.invoke([HumanMessage(content=mensagem_completa)])

# Salvar na memory
memory.save_context({"input": pergunta}, {"output": response.content})
```

### **ConversationSummaryMemory - O Assistente que Anota**

ConversationSummaryMemory √© como ter um **assistente que faz anota√ß√µes** durante a reuni√£o. Em vez de lembrar de tudo palavra por palavra, ele faz **resumos inteligentes**.

**Analogia**: √â como a diferen√ßa entre:
- **V√≥ com boa mem√≥ria**: Lembra de tudo, mas pode ficar confusa com muita informa√ß√£o
- **Assistente que anota**: Faz resumos organizados, mant√©m o essencial

### **Vantagens da SummaryMemory**

1. **Mais eficiente** - Usa menos tokens
2. **Mais organizado** - Informa√ß√µes resumidas
3. **Melhor para conversas longas** - N√£o fica pesado
4. **Foco no essencial** - Mant√©m o que importa

```python
from langchain.memory import ConversationSummaryMemory

# Criando ConversationSummaryMemory
summary_memory = ConversationSummaryMemory(
    llm=llm,  # Modelo que vai fazer os resumos
    max_token_limit=1000  # Limite de tokens para o resumo
)

# Usar memory diretamente
summary_memory.save_context({"input": "Ol√°!"}, {"output": "Oi! Como vai?"})
resumo = summary_memory.load_memory_variables({})
```

### **Sistema de Chatbot com Memory Completo**

Agora vamos criar um **chatbot mais avan√ßado** que combina diferentes tipos de memory. √â como ter um **assistente pessoal inteligente**!

```python
from langchain.memory import ConversationSummaryBufferMemory

# Memory h√≠brida: combina buffer e summary
memory_avancada = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000,  # Limite maior para conversas mais longas
    return_messages=True
)

# Template para assistente pessoal
template_assistente = PromptTemplate(
    input_variables=["history", "input"],
    template="""
    Voc√™ √© o Pedro, um assistente pessoal descontra√≠do e √∫til.

    HIST√ìRICO DA CONVERSA:
    {history}

    USU√ÅRIO: {input}

    INSTRU√á√ïES:
    - Use o hist√≥rico para personalizar suas respostas
    - Lembre-se de informa√ß√µes importantes sobre o usu√°rio
    - Seja descontra√≠do, use analogias e piadas leves
    - Fale como o Pedro Guth
    - Seja √∫til e prestativo
    """
)

# Usar operador pipe em vez de LLMChain
chain_assistente = template_assistente | llm
```

---

## **ü§ñ AGENTS - OS FUNCION√ÅRIOS INTELIGENTES**

### **T√°, mas o que s√£o Agents?**

Imagina que voc√™ √© um **chefe de uma empresa** e tem funcion√°rios especializados:

**Sem Agents**: Voc√™ tem que fazer tudo sozinho - pesquisar, calcular, escrever, analisar...
**Com Agents**: Voc√™ delega para funcion√°rios especializados que sabem usar ferramentas!

**Agent** = IA que pode **tomar decis√µes** e **usar ferramentas** para completar tarefas.

### **Por que Agents s√£o Poderosos?**

**Sem Agents**: IA s√≥ conversa, n√£o pode fazer a√ß√µes
**Com Agents**: IA pode pesquisar, calcular, executar c√≥digo, etc.

√â como a diferen√ßa entre **conversar com algu√©m** e **ter um assistente que pode usar computador, calculadora, internet...**

### **Criando Nossa Primeira Ferramenta**

Vamos criar um **agente de pesquisa** que pode buscar informa√ß√µes na internet:

```python
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.tools import Tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# Ferramenta de pesquisa
search_tool = DuckDuckGoSearchRun()

# Criar ferramentas com descri√ß√£o
tools = [
    Tool(
        name="Pesquisa na Internet",
        func=search_tool.run,
        description="Use para buscar informa√ß√µes atualizadas na internet"
    )
]

# Memory para o agente
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Template para o agente
prompt = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um assistente √∫til que pode pesquisar na internet."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

# Criar agente com sintaxe moderna
agent = create_openai_functions_agent(llm, tools, prompt)

# Executor do agente
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True
)
```

### **Ferramentas Customizadas - Swiss Army Knife da IA**

Vamos criar ferramentas customizadas. √â como dar **ferramentas espec√≠ficas** para cada funcion√°rio:

```python
from langchain.tools import BaseTool
from typing import Optional
import math

# Ferramenta de c√°lculo
class CalculadoraTool(BaseTool):
    name = "calculadora"
    description = "Use para fazer c√°lculos matem√°ticos"

    def _run(self, query: str) -> str:
        try:
            # Avalia express√£o matem√°tica com seguran√ßa
            allowed_names = {
                'abs': abs, 'round': round, 'min': min, 'max': max,
                'sum': sum, 'pow': pow, 'sqrt': math.sqrt
            }
            result = eval(query, {"__builtins__": {}}, allowed_names)
            return f"Resultado: {result}"
        except Exception as e:
            return f"Erro no c√°lculo: {e}"

# Ferramenta de an√°lise de texto
class AnalisadorTextoTool(BaseTool):
    name = "analisador_texto"
    description = "Use para analisar textos (contar palavras, caracteres, etc.)"

    def _run(self, texto: str) -> str:
        palavras = len(texto.split())
        caracteres = len(texto)
        linhas = len(texto.split('\n'))

        return f"An√°lise do texto:\n- Palavras: {palavras}\n- Caracteres: {caracteres}\n- Linhas: {linhas}"
```

### **Agente Financeiro - Tomada de Decis√£o Inteligente**

Vamos criar um **agente financeiro** que pode analisar investimentos e dar conselhos:

```python
# Ferramenta de an√°lise financeira
class AnaliseFinanceiraTool(BaseTool):
    name = "analise_financeira"
    description = "Use para analisar investimentos e dar conselhos financeiros"

    def _run(self, pergunta: str) -> str:
        # Simula√ß√£o de an√°lise financeira
        if "cdb" in pergunta.lower():
            return "CDB √© um investimento de renda fixa seguro. Rentabilidade: 100-110% do CDI. Ideal para conservadores."
        elif "a√ß√µes" in pergunta.lower():
            return "A√ß√µes s√£o investimentos de renda vari√°vel. Maior risco, maior potencial de retorno. Ideal para arrojados."
        elif "tesouro" in pergunta.lower():
            return "Tesouro Direto √© o investimento mais seguro do Brasil. Garantido pelo governo federal."
        else:
            return "Para investimentos, considere seu perfil de risco e objetivos. Diversifica√ß√£o √© fundamental."

# Ferramenta de c√°lculo de juros compostos
class JurosCompostosTool(BaseTool):
    name = "juros_compostos"
    description = "Use para calcular juros compostos e proje√ß√µes financeiras"

    def _run(self, query: str) -> str:
        try:
            # Exemplo: calcular juros compostos
            if "calcular" in query.lower():
                # Simula√ß√£o simples
                principal = 1000
                taxa = 0.01  # 1% ao m√™s
                tempo = 12   # 12 meses

                montante = principal * (1 + taxa) ** tempo
                return f"Investimento de R$ {principal} por {tempo} meses a {taxa*100}% ao m√™s = R$ {montante:.2f}"
            else:
                return "Use 'calcular' para fazer proje√ß√µes financeiras"
        except:
            return "Erro no c√°lculo financeiro"
```

---

## **üìÑ DOCUMENT LOADERS - IA QUE L√ä TUDO**

### **T√°, mas o que s√£o Document Loaders?**

Imagina que voc√™ tem uma **biblioteca digital** que pode ler qualquer tipo de documento:

- üìÑ **PDFs** - Relat√≥rios, manuais, artigos
- üìä **CSVs** - Dados, planilhas, estat√≠sticas
- üé• **YouTube** - V√≠deos, palestras, tutoriais
- üåê **Websites** - P√°ginas, blogs, not√≠cias
- üìù **Word** - Documentos, contratos, propostas

**Document Loaders** s√£o como **tradutores especializados** que convertem qualquer formato em texto que a IA pode entender.

### **Por que Document Loaders s√£o Importantes?**

**Sem Document Loaders**: IA s√≥ sabe o que voc√™ digita
**Com Document Loaders**: IA pode ler e analisar qualquer documento!

√â como a diferen√ßa entre **ler apenas um livro** vs **ter acesso a uma biblioteca inteira**! üìö

### **Testando Diferentes Document Loaders**

```python
from langchain.document_loaders import (
    PyPDFLoader, 
    CSVLoader, 
    UnstructuredWordDocumentLoader,
    WebBaseLoader,
    YoutubeLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Testando CSV Loader
csv_loader = CSVLoader('produtos.csv')
documentos_csv = csv_loader.load()

# Testando Text Loader
from langchain.document_loaders import TextLoader
text_loader = TextLoader('langchain_info.txt', encoding='utf-8')
documentos_texto = text_loader.load()

# Testando Web Loader
urls = ["https://python.org"]
web_loader = WebBaseLoader(urls)
documentos_web = web_loader.load()
```

### **Vector Stores - A Mem√≥ria de Longo Prazo**

Vector Stores s√£o como um **c√©rebro organizado** que:
1. **Converte texto em n√∫meros** (vetores)
2. **Organiza por similaridade**
3. **Permite busca inteligente**

**Analogia**: √â como ter uma **biblioteca onde os livros se organizam sozinhos** por assunto, e voc√™ pode encontrar qualquer informa√ß√£o rapidamente!

### **Por que Vector Stores s√£o Importantes?**

**Sem Vector Stores**: IA esquece o que leu
**Com Vector Stores**: IA pode buscar e lembrar de qualquer informa√ß√£o

√â como a diferen√ßa entre **ler um livro e esquecer** vs **ter uma biblioteca na cabe√ßa**! üß†

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Text splitter para dividir documentos grandes
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Tamanho de cada peda√ßo
    chunk_overlap=200,  # Sobreposi√ß√£o entre peda√ßos
    length_function=len
)

# Carregando e dividindo documentos
text_loader = TextLoader('langchain_info.txt', encoding='utf-8')
documentos = text_loader.load()

# Dividindo em peda√ßos menores
textos_divididos = text_splitter.split_documents(documentos)

# Criando embeddings
embeddings = OpenAIEmbeddings(
    openai_api_key=os.getenv('OPENAI_API_KEY')
)

# Criando vector store
vectorstore = Chroma.from_documents(
    documents=textos_divididos,
    embedding=embeddings,
    collection_name="langchain_info"
)
```

### **Sistema de Busca Inteligente Completo**

Agora vamos criar um **sistema de busca inteligente** que combina document loaders com vector stores:

```python
class SistemaBuscaInteligente:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        self.vectorstore = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
    
    def carregar_documento(self, caminho_arquivo, tipo="texto"):
        """Carrega um documento e adiciona ao vector store"""
        try:
            if tipo == "csv":
                loader = CSVLoader(caminho_arquivo)
            elif tipo == "texto":
                loader = TextLoader(caminho_arquivo, encoding='utf-8')
            else:
                return "Tipo de documento n√£o suportado"
            
            documentos = loader.load()
            textos_divididos = self.text_splitter.split_documents(documentos)
            
            if self.vectorstore is None:
                self.vectorstore = Chroma.from_documents(
                    documents=textos_divididos,
                    embedding=self.embeddings,
                    collection_name="biblioteca_digital"
                )
            else:
                self.vectorstore.add_documents(textos_divididos)
            
            return f"‚úÖ Documento carregado! {len(textos_divididos)} peda√ßos adicionados."
            
        except Exception as e:
            return f"‚ùå Erro: {e}"
    
    def buscar(self, query, k=3):
        """Busca documentos similares"""
        try:
            if self.vectorstore is None:
                return "‚ùå Nenhum documento carregado ainda."
            
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
            
        except Exception as e:
            return f"‚ùå Erro na busca: {e}"
```

---

## **üîç RAG - IA QUE SABE O QUE N√ÉO SABE**

### **T√°, mas o que √© RAG?**

Imagina que voc√™ √© um **estudante muito inteligente** que est√° fazendo uma prova:

**Sem RAG**: Voc√™ s√≥ pode usar o que j√° sabe de cabe√ßa. Se n√£o souber, chuta ou inventa.
**Com RAG**: Voc√™ pode **consultar livros** antes de responder, garantindo que sua resposta seja precisa!

**RAG** = **Retrieval Augmented Generation**
- **Retrieval**: Busca informa√ß√µes relevantes
- **Augmented**: Aumenta o conhecimento da IA
- **Generation**: Gera respostas baseadas nas informa√ß√µes encontradas

### **Por que RAG √© Revolucion√°rio?**

**Problema tradicional**: IA s√≥ sabe o que foi treinada, n√£o pode acessar informa√ß√µes atualizadas
**Solu√ß√£o RAG**: IA busca informa√ß√µes relevantes e gera respostas precisas

√â como a diferen√ßa entre **um professor que s√≥ sabe o que estudou h√° 10 anos** vs **um professor que sempre consulta os livros mais recentes**! üìö

### **Criando Sistema RAG B√°sico**

```python
from langchain.chains import RetrievalQA

# Criando chain RAG
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # Tipo de chain para RAG
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": 3}  # Busca os 3 documentos mais relevantes
    ),
    return_source_documents=True,  # Retorna os documentos usados
    verbose=True
)

# Executando RAG
resultado = rag_chain({"query": "O que √© Python e para que serve?"})
print(f"ü§ñ Resposta: {resultado['result']}")
print(f"üìÑ Documentos consultados: {len(resultado['source_documents'])}")
```

### **Sistema de Suporte T√©cnico com RAG**

Vamos criar um **sistema de suporte t√©cnico inteligente** que usa RAG:

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Template para suporte t√©cnico
template_suporte = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    Voc√™ √© um especialista em suporte t√©cnico muito atencioso.
    
    Use as seguintes informa√ß√µes da base de conhecimento para responder:
    {context}
    
    PERGUNTA DO CLIENTE: {question}
    
    INSTRU√á√ïES:
    - Responda de forma clara e did√°tica
    - Use as informa√ß√µes da base de conhecimento
    - Seja prestativo e atencioso
    - Use linguagem informal como o Pedro Guth
    - Se n√£o encontrar informa√ß√£o espec√≠fica, seja honesto
    """
)

# Chain de suporte
chain_suporte = LLMChain(
    llm=llm,
    prompt=template_suporte
)

# Fun√ß√£o para buscar contexto relevante
def buscar_contexto(pergunta, k=2):
    """Busca documentos relevantes para a pergunta"""
    try:
        docs = vectorstore.similarity_search(pergunta, k=k)
        contexto = "\n\n".join([doc.page_content for doc in docs])
        return contexto
    except Exception as e:
        return f"Erro na busca: {e}"

# Gerando resposta com contexto
contexto = buscar_contexto(pergunta)
resposta = chain_suporte.run({
    "context": contexto,
    "question": pergunta
})
```

### **Chatbot com RAG - Sistema Completo**

Agora vamos criar um **chatbot completo** que combina RAG com memory:

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# Memory para o chatbot
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Chatbot com RAG
chatbot_rag = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

# Executando chatbot
resultado = chatbot_rag({"question": mensagem})
print(f"ü§ñ Chatbot: {resultado['answer'][:300]}...")

if resultado['source_documents']:
    print(f"üìÑ Consultou {len(resultado['source_documents'])} documentos")
```

---

## **üöÄ PROJETOS PR√ÅTICOS**

### **Projeto 1: Chatbot para E-commerce**

Vamos criar um **chatbot inteligente para e-commerce** que:

- üõçÔ∏è **Conhece todos os produtos**
- üí¨ **Atende clientes 24/7**
- üß† **Lembra das prefer√™ncias**
- üîç **Busca produtos inteligentemente**
- üí≥ **Ajuda com compras**

√â como ter um **vendedor digital super inteligente** que nunca dorme e conhece cada cliente! üõí

#### **Por que este projeto √© importante?**

**Problema real**: E-commerces perdem vendas por falta de atendimento
**Solu√ß√£o**: Chatbot que atende instantaneamente e converte vendas

√â como a diferen√ßa entre **uma loja fechada** e **uma loja com vendedor 24h**! üè™

#### **Criando Base de Produtos**

```python
produtos_ecommerce = """
CAT√ÅLOGO DE PRODUTOS - TECHSTORE

SMARTPHONES:

iPhone 15 Pro Max
- Pre√ßo: R$ 9.999
- Categoria: Smartphone
- Marca: Apple
- Caracter√≠sticas: Tela 6.7", Chip A17 Pro, C√¢mera tripla 48MP
- Cores: Tit√¢nio Natural, Tit√¢nio Azul, Tit√¢nio Branco, Tit√¢nio Preto
- Capacidades: 256GB, 512GB, 1TB
- Garantia: 1 ano

Samsung Galaxy S24 Ultra
- Pre√ßo: R$ 8.999
- Categoria: Smartphone
- Marca: Samsung
- Caracter√≠sticas: Tela 6.8", Snapdragon 8 Gen 3, C√¢mera qu√°drupla 200MP
- Cores: Tit√¢nio Cinza, Tit√¢nio Preto, Tit√¢nio Violeta, Tit√¢nio Amarelo
- Capacidades: 256GB, 512GB, 1TB
- Garantia: 1 ano

NOTEBOOKS:

MacBook Pro 14" M3 Pro
- Pre√ßo: R$ 18.999
- Categoria: Notebook
- Marca: Apple
- Caracter√≠sticas: Chip M3 Pro, 14", 18GB RAM, 512GB SSD
- Cores: Space Gray, Silver
- Garantia: 1 ano

ACESS√ìRIOS:

AirPods Pro 2¬™ Gera√ß√£o
- Pre√ßo: R$ 2.499
- Categoria: Fones de Ouvido
- Marca: Apple
- Caracter√≠sticas: Cancelamento de ru√≠do ativo, √Åudio espacial
- Cores: Branco
- Garantia: 1 ano

POL√çTICAS DA LOJA:

ENTREGA:
- Frete gr√°tis para compras acima de R$ 500
- Entrega em 1-3 dias √∫teis
- Rastreamento em tempo real

PAGAMENTO:
- Cart√£o de cr√©dito (at√© 12x)
- PIX (5% de desconto)
- Boleto banc√°rio

GARANTIA:
- Todos os produtos t√™m garantia de f√°brica
- Suporte t√©cnico especializado
- Troca em at√© 7 dias

ATENDIMENTO:
- Chat 24/7
- WhatsApp: (11) 99999-9999
- Email: atendimento@techstore.com
"""
```

#### **Criando o Chatbot de E-commerce**

```python
# Carregando e processando base de produtos
from langchain.document_loaders import TextLoader

loader = TextLoader('produtos_techstore.txt', encoding='utf-8')
documentos = loader.load()

# Dividindo em peda√ßos menores
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
textos_divididos = text_splitter.split_documents(documentos)

# Criando vector store
embeddings = OpenAIEmbeddings(
    openai_api_key=os.getenv('OPENAI_API_KEY')
)

vectorstore = Chroma.from_documents(
    documents=textos_divididos,
    embedding=embeddings,
    collection_name="produtos_techstore"
)

# Criando memory para o chatbot
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Criando chatbot com RAG
from langchain.chains import ConversationalRetrievalChain

chatbot_ecommerce = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

print("ü§ñ Chatbot de E-commerce criado!")
print("üõçÔ∏è Vendedor digital pronto para atender!")
```

#### **Testando o Chatbot de E-commerce**

```python
conversas_teste = [
    "Ol√°! Estou procurando um smartphone bom para fotos.",
    "Qual a diferen√ßa entre iPhone 15 Pro Max e Samsung Galaxy S24 Ultra?",
    "Quanto custa o iPhone 15 Pro Max?",
    "Voc√™s t√™m frete gr√°tis?",
    "Lembra do que eu estava procurando?",
    "Quais s√£o as formas de pagamento?",
    "Preciso de um notebook para trabalho. O que voc√™ recomenda?"
]

for i, mensagem in enumerate(conversas_teste, 1):
    print(f"\nüë§ Cliente {i}: {mensagem}")
    
    try:
        resultado = chatbot_ecommerce({"question": mensagem})
        print(f"ü§ñ Vendedor: {resultado['answer'][:300]}...")
        
        if resultado['source_documents']:
            print(f"üìÑ Consultou {len(resultado['source_documents'])} produtos")
        
    except Exception as e:
        print(f"‚ùå Erro: {e}")
    
    print("-" * 40)
```

### **Projeto 2: Assistente de Estudos**

#### **Base de Conhecimento Acad√™mica**

```python
material_estudo = """
INTRODU√á√ÉO √Ä INTELIG√äNCIA ARTIFICIAL

DEFINI√á√ÉO E CONCEITOS B√ÅSICOS:

Intelig√™ncia Artificial (IA) √© um campo da computa√ß√£o que busca criar sistemas
capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.
A IA combina ci√™ncia da computa√ß√£o, matem√°tica, psicologia e outras disciplinas.

HIST√ìRICO DA IA:

A IA nasceu na d√©cada de 1950, com o teste de Turing proposto por Alan Turing.
O termo "Intelig√™ncia Artificial" foi cunhado em 1956 na Confer√™ncia de Dartmouth.
Desde ent√£o, a IA passou por v√°rios "invernos" e "primaveras" de desenvolvimento.

TIPOS DE INTELIG√äNCIA ARTIFICIAL:

1. IA Fraca (Narrow AI):
   - Especializada em uma tarefa espec√≠fica
   - Exemplos: Siri, Alexa, sistemas de recomenda√ß√£o
   - N√£o possui consci√™ncia ou intelig√™ncia geral

2. IA Forte (AGI - Artificial General Intelligence):
   - Intelig√™ncia geral como humanos
   - Pode realizar qualquer tarefa intelectual
   - Ainda n√£o foi alcan√ßada

3. Superintelig√™ncia:
   - Intelig√™ncia superior √† humana
   - Conceito te√≥rico e controverso

MACHINE LEARNING:

Machine Learning √© um subcampo da IA que permite aos computadores aprender
sem serem explicitamente programados. Os algoritmos identificam padr√µes
nos dados para fazer previs√µes ou tomar decis√µes.

TIPOS DE MACHINE LEARNING:

1. Aprendizado Supervisionado:
   - Usa dados rotulados para treinar
   - Exemplos: classifica√ß√£o, regress√£o
   - Algoritmos: Random Forest, SVM, Redes Neurais

2. Aprendizado N√£o Supervisionado:
   - Encontra padr√µes em dados n√£o rotulados
   - Exemplos: clustering, redu√ß√£o de dimensionalidade
   - Algoritmos: K-means, PCA, Autoencoders

3. Aprendizado por Refor√ßo:
   - Aprende atrav√©s de tentativa e erro
   - Usa recompensas e puni√ß√µes
   - Exemplos: jogos, rob√≥tica, carros aut√¥nomos

DEEP LEARNING:

Deep Learning √© um subcampo do Machine Learning que usa redes neurais
com m√∫ltiplas camadas para aprender representa√ß√µes complexas dos dados.

CARACTER√çSTICAS:
- M√∫ltiplas camadas de neur√¥nios
- Aprendizado de features autom√°tico
- Requer grandes quantidades de dados
- Poder computacional significativo

APLICA√á√ïES PR√ÅTICAS:

1. Vis√£o Computacional:
   - Reconhecimento de imagens
   - Detec√ß√£o de objetos
   - Processamento de v√≠deo

2. Processamento de Linguagem Natural:
   - Tradu√ß√£o autom√°tica
   - An√°lise de sentimentos
   - Chatbots inteligentes

3. Sistemas de Recomenda√ß√£o:
   - Netflix, Spotify, Amazon
   - Personaliza√ß√£o de conte√∫do
   - Marketing direcionado

4. Medicina e Sa√∫de:
   - Diagn√≥stico por imagem
   - Descoberta de medicamentos
   - Medicina personalizada

DESAFIOS E LIMITA√á√ïES:

1. Vi√©s nos Dados:
   - Dados podem refletir preconceitos
   - Importante para justi√ßa e equidade

2. Interpretabilidade:
   - Modelos complexos s√£o dif√≠ceis de explicar
   - Importante para confian√ßa e responsabilidade

3. Privacidade:
   - Coleta e uso de dados pessoais
   - Necessidade de prote√ß√£o

4. Seguran√ßa:
   - Vulnerabilidades em sistemas de IA
   - Ataques adversariais

FUTURO DA IA:

O futuro da IA inclui:
- IA mais √©tica e respons√°vel
- Automa√ß√£o de mais tarefas
- Integra√ß√£o com IoT e rob√≥tica
- Desenvolvimento de AGI (controverso)
- Regulamenta√ß√µes e pol√≠ticas

CONCEITOS IMPORTANTES:

Overfitting: Modelo que memoriza dados de treino
Underfitting: Modelo muito simples para os dados
Cross-validation: T√©cnica para avaliar modelos
Feature Engineering: Cria√ß√£o de features relevantes
Hyperparameter Tuning: Otimiza√ß√£o de par√¢metros
"""

# Salvando material de estudo
with open('material_ia.txt', 'w', encoding='utf-8') as file:
    file.write(material_estudo)
```

#### **Fun√ß√µes Especializadas do Assistente**

```python
def gerar_resumo(texto, llm):
    """Gera resumo do material de estudo"""
    prompt = f"""
    Crie um resumo conciso e did√°tico do seguinte texto sobre IA:
    
    {texto}
    
    O resumo deve:
    - Ser f√°cil de entender
    - Destacar os pontos principais
    - Usar linguagem clara
    - Ter no m√°ximo 200 palavras
    """
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        return f"Erro ao gerar resumo: {e}"

def criar_flashcards(texto, llm):
    """Cria flashcards para estudo"""
    prompt = f"""
    Crie 5 flashcards para estudo baseados no seguinte texto:
    
    {texto}
    
    Formato:
    Frente: [Pergunta]
    Verso: [Resposta]
    
    As perguntas devem ser claras e as respostas concisas.
    """
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        return f"Erro ao criar flashcards: {e}"

def gerar_perguntas(texto, llm):
    """Gera perguntas de revis√£o"""
    prompt = f"""
    Crie 3 perguntas de revis√£o sobre o seguinte conte√∫do:
    
    {texto}
    
    As perguntas devem:
    - Testar compreens√£o
    - Ser de diferentes n√≠veis (f√°cil, m√©dio, dif√≠cil)
    - Ter respostas claras
    """
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content
    except Exception as e:
        return f"Erro ao gerar perguntas: {e}"
```

#### **Sistema Completo de Estudos**

```python
class AssistenteEstudos:
    def __init__(self, llm, vectorstore):
        self.llm = llm
        self.vectorstore = vectorstore
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
    
    def perguntar(self, pergunta):
        """Faz uma pergunta sobre o material"""
        try:
            # Buscando material relevante
            docs = self.vectorstore.similarity_search(pergunta, k=3)
            contexto = "\n\n".join([doc.page_content for doc in docs])
            
            # Gerando resposta
            prompt = f"""
            Voc√™ √© um professor especialista em IA. Responda √† pergunta do aluno
            usando o seguinte material de estudo:
            
            MATERIAL: {contexto}
            
            PERGUNTA: {pergunta}
            
            Responda de forma did√°tica e clara, como o Pedro Guth explicando.
            """
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            return response.content
            
        except Exception as e:
            return f"Erro: {e}"
    
    def gerar_resumo_tema(self, tema):
        """Gera resumo de um tema espec√≠fico"""
        try:
            docs = self.vectorstore.similarity_search(tema, k=3)
            texto = "\n\n".join([doc.page_content for doc in docs])
            return gerar_resumo(texto, self.llm)
        except Exception as e:
            return f"Erro: {e}"
    
    def criar_flashcards_tema(self, tema):
        """Cria flashcards de um tema"""
        try:
            docs = self.vectorstore.similarity_search(tema, k=2)
            texto = "\n\n".join([doc.page_content for doc in docs])
            return criar_flashcards(texto, self.llm)
        except Exception as e:
            return f"Erro: {e}"

# Criando inst√¢ncia do assistente
assistente = AssistenteEstudos(llm, vectorstore_estudos)
```

---

## **üéØ M√≥dulo 9: Deploy e Produ√ß√£o**

### **Aula 9.1: Deploy B√°sico - Colocando na Rua**

#### **O que √© Deploy?**

Imagina que voc√™ construiu uma **casa incr√≠vel** no seu computador:

**Sem Deploy**: A casa fica s√≥ no seu computador. Ningu√©m mais pode ver ou usar.
**Com Deploy**: A casa vai para a internet, todo mundo pode acessar e usar!

**Deploy** = **Colocar seu projeto na internet** para que outras pessoas possam usar.

#### **Por que Deploy √© Importante?**

**Problema**: Projetos incr√≠veis que ficam s√≥ no computador do desenvolvedor
**Solu√ß√£o**: Deploy para que todo mundo possa usar e se beneficiar

√â como a diferen√ßa entre **ter uma receita incr√≠vel guardada** vs **abrir um restaurante**! üçΩÔ∏è

### **Aula 9.2: Deploy com Streamlit - Interface Web Simples**

#### **Criando App Streamlit**

```python
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Chatbot IA - Pedro Guth",
    page_icon="ü§ñ",
    layout="wide"
)

# T√≠tulo
st.title("ü§ñ Chatbot IA - Pedro Guth")
st.markdown("---")

# Sidebar com configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # API Key
    api_key = st.text_input(
        "üîë OpenAI API Key",
        type="password",
        help="Sua chave da OpenAI"
    )
    
    # Modelo
    modelo = st.selectbox(
        "ü§ñ Modelo",
        ["gpt-3.5-turbo", "gpt-4"],
        index=0
    )
    
    # Temperature
    temperature = st.slider(
        "üå°Ô∏è Criatividade",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1
    )

# √Årea principal
if api_key:
    # Inicializando modelo
    llm = ChatOpenAI(
        model=modelo,
        temperature=temperature,
        api_key=api_key
    )
    
    # Chat
    st.header("üí¨ Conversa")
    
    # Inicializar chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Mostrar mensagens anteriores
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua mensagem..."):
        # Adicionar mensagem do usu√°rio
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Gerar resposta
        with st.chat_message("assistant"):
            with st.spinner("ü§ñ Pensando..."):
                try:
                    response = llm.invoke([HumanMessage(content=prompt)])
                    st.markdown(response.content)
                    st.session_state.messages.append({"role": "assistant", "content": response.content})
                except Exception as e:
                    st.error(f"‚ùå Erro: {e}")
else:
    st.warning("‚ö†Ô∏è Configure sua API Key na sidebar para come√ßar!")

# Footer
st.markdown("---")
st.markdown("*Desenvolvido com ‚ù§Ô∏è por Pedro Guth*")
```

### **Aula 9.3: Deploy com Gradio - Interface R√°pida**

#### **Criando Interface Gradio**

```python
import gradio as gr
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Carregando vari√°veis
load_dotenv()

def chatbot_gradio(message, history):
    """Fun√ß√£o do chatbot para Gradio"""
    try:
        # Inicializando modelo
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Gerando resposta
        response = llm.invoke([HumanMessage(content=message)])
        return response.content
        
    except Exception as e:
        return f"‚ùå Erro: {e}"

# Criando interface Gradio
demo = gr.ChatInterface(
    fn=chatbot_gradio,
    title="ü§ñ Chatbot IA - Pedro Guth",
    description="Chatbot inteligente desenvolvido com LangChain",
    examples=[
        ["O que √© LangChain?"],
        ["Como funciona a IA?"],
        ["Me conte uma piada sobre programa√ß√£o"]
    ],
    theme="soft"
)

if __name__ == "__main__":
    demo.launch()
```

### **Aula 9.4: Deploy com FastAPI - API Profissional**

#### **Criando API com FastAPI**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Carregando vari√°veis
load_dotenv()

# Criando app FastAPI
app = FastAPI(
    title="Chatbot IA API",
    description="API do Chatbot IA desenvolvido com LangChain",
    version="1.0.0"
)

# Modelos de dados
class ChatRequest(BaseModel):
    message: str
    model: Optional[str] = "gpt-3.5-turbo"
    temperature: Optional[float] = 0.7

class ChatResponse(BaseModel):
    response: str
    model: str
    tokens_used: Optional[int] = None

# Rota principal
@app.get("/")
async def root():
    return {"message": "Chatbot IA API - Pedro Guth"}

# Rota de chat
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # Verificar API key
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise HTTPException(status_code=500, detail="API key n√£o configurada")
        
        # Inicializando modelo
        llm = ChatOpenAI(
            model=request.model,
            temperature=request.temperature,
            api_key=api_key
        )
        
        # Gerando resposta
        response = llm.invoke([HumanMessage(content=request.message)])
        
        return ChatResponse(
            response=response.content,
            model=request.model
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Rota de sa√∫de
@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "chatbot-ia"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### **Aula 9.5: Monitoramento e Otimiza√ß√£o**

#### **Sistema de Monitoramento**

```python
import time
import logging
from datetime import datetime

# Configurando logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class Monitoramento:
    def __init__(self):
        self.requests = 0
        self.errors = 0
        self.start_time = datetime.now()
    
    def log_request(self, success=True):
        """Registra uma requisi√ß√£o"""
        self.requests += 1
        if not success:
            self.errors += 1
        
        logger.info(f"Requisi√ß√£o {self.requests}: {'Sucesso' if success else 'Erro'}")
    
    def get_stats(self):
        """Retorna estat√≠sticas"""
        uptime = datetime.now() - self.start_time
        error_rate = (self.errors / self.requests * 100) if self.requests > 0 else 0
        
        return {
            "total_requests": self.requests,
            "total_errors": self.errors,
            "error_rate": f"{error_rate:.2f}%",
            "uptime": str(uptime).split('.')[0]
        }

# Fun√ß√£o de chat com monitoramento
monitor = Monitoramento()

def chat_com_monitoramento(message):
    """Chat com monitoramento de performance"""
    start_time = time.time()
    
    try:
        # Inicializando modelo
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Gerando resposta
        response = llm.invoke([HumanMessage(content=message)])
        
        # Calculando tempo
        execution_time = time.time() - start_time
        
        # Logging
        logger.info(f"Resposta gerada em {execution_time:.2f}s")
        monitor.log_request(success=True)
        
        return response.content
        
    except Exception as e:
        logger.error(f"Erro: {e}")
        monitor.log_request(success=False)
        return f"‚ùå Erro: {e}"
```

---

## **üöÄ M√≥dulo 10: T√≥picos Avan√ßados - Indo Al√©m**

### **Aula 10.1: LangGraph - Fluxos Complexos**

#### **O que √© LangGraph?**

Imagina que voc√™ est√° construindo um **plano de metr√¥** para a IA:

**Sem LangGraph**: A IA s√≥ anda em linha reta (A ‚Üí B ‚Üí C)
**Com LangGraph**: A IA pode pegar diferentes linhas, fazer baldea√ß√µes, voltar atr√°s!

**LangGraph** = **Framework para criar fluxos complexos** onde a IA pode:
- üõ§Ô∏è **Tomar decis√µes** sobre qual caminho seguir
- üîÑ **Fazer loops** e voltar atr√°s
- üéØ **Manter estado** durante o processo
- üö¶ **Controlar fluxo** com condi√ß√µes

#### **Por que LangGraph √© Revolucion√°rio?**

**Problema tradicional**: Fluxos lineares e r√≠gidos
**Solu√ß√£o LangGraph**: Fluxos din√¢micos e inteligentes

√â como a diferen√ßa entre **um roteiro de filme fixo** vs **um jogo de RPG onde voc√™ escolhe o caminho**! üéÆ

#### **Criando Nosso Primeiro LangGraph**

```python
# Instalando LangGraph
!pip install langgraph

# Importando bibliotecas
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
from langchain.schema import HumanMessage

# Carregando vari√°veis
load_dotenv()

# Modelo
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    api_key=os.getenv('OPENAI_API_KEY')
)

# Definindo o estado do nosso sistema
class EstadoAtendimento(TypedDict):
    """Estado do sistema de atendimento"""
    pergunta: str
    categoria: str
    resposta: str
    acao: str
    historico: list

# Criando fun√ß√£o para categorizar a pergunta
def categorizar_pergunta(state: EstadoAtendimento) -> EstadoAtendimento:
    """Categoriza a pergunta do cliente"""
    pergunta = state["pergunta"]
    
    # L√≥gica de categoriza√ß√£o
    if any(palavra in pergunta.lower() for palavra in ["pre√ßo", "custo", "valor", "quanto"]):
        categoria = "precos"
    elif any(palavra in pergunta.lower() for palavra in ["produto", "item", "comprar", "venda"]):
        categoria = "produtos"
    elif any(palavra in pergunta.lower() for palavra in ["problema", "erro", "bug", "ajuda"]):
        categoria = "suporte"
    else:
        categoria = "geral"
    
    # Atualizando estado
    state["categoria"] = categoria
    state["historico"].append(f"Categorizado como: {categoria}")
    
    return state

# Criando fun√ß√µes especializadas para cada categoria
def atender_precos(state: EstadoAtendimento) -> EstadoAtendimento:
    """Atende perguntas sobre pre√ßos"""
    resposta = """
    üí∞ INFORMA√á√ïES SOBRE PRE√áOS:
    
    ‚Ä¢ iPhone 15 Pro Max: R$ 9.999
    ‚Ä¢ Samsung Galaxy S24 Ultra: R$ 8.999
    ‚Ä¢ MacBook Pro 14": R$ 18.999
    ‚Ä¢ AirPods Pro: R$ 2.499
    
    üí≥ Formas de pagamento:
    ‚Ä¢ Cart√£o de cr√©dito (at√© 12x)
    ‚Ä¢ PIX (5% de desconto)
    ‚Ä¢ Boleto banc√°rio
    
    üöö Frete gr√°tis para compras acima de R$ 500!
    """
    
    state["resposta"] = resposta
    state["acao"] = "informacao_precos"
    state["historico"].append("Fornecidas informa√ß√µes sobre pre√ßos")
    
    return state

def atender_produtos(state: EstadoAtendimento) -> EstadoAtendimento:
    """Atende perguntas sobre produtos"""
    resposta = """
    üì± NOSSOS PRODUTOS:
    
    üçé Apple:
    ‚Ä¢ iPhone 15 Pro Max - C√¢mera avan√ßada, Chip A17 Pro
    ‚Ä¢ MacBook Pro 14" - Chip M3 Pro, ideal para trabalho
    ‚Ä¢ AirPods Pro - Cancelamento de ru√≠do ativo
    
    üì± Samsung:
    ‚Ä¢ Galaxy S24 Ultra - C√¢mera 200MP, S Pen
    
    üíª Outros:
    ‚Ä¢ Dell XPS 13 Plus - Notebook premium
    
    üéØ Qual produto te interessa? Posso dar mais detalhes!
    """
    
    state["resposta"] = resposta
    state["acao"] = "catalogo_produtos"
    state["historico"].append("Apresentado cat√°logo de produtos")
    
    return state

def atender_suporte(state: EstadoAtendimento) -> EstadoAtendimento:
    """Atende problemas e suporte"""
    resposta = """
    üõ†Ô∏è SUPORTE T√âCNICO:
    
    üìû Canais de atendimento:
    ‚Ä¢ WhatsApp: (11) 99999-9999
    ‚Ä¢ Email: suporte@techstore.com
    ‚Ä¢ Chat: 24/7 (voc√™ est√° usando agora!)
    
    ‚è∞ Hor√°rio: Segunda a sexta, 8h √†s 18h
    
    üîß Tipos de suporte:
    ‚Ä¢ Problemas t√©cnicos
    ‚Ä¢ D√∫vidas sobre produtos
    ‚Ä¢ Garantia e trocas
    ‚Ä¢ Instala√ß√£o e configura√ß√£o
    
    Como posso te ajudar hoje?
    """
    
    state["resposta"] = resposta
    state["acao"] = "suporte_tecnico"
    state["historico"].append("Fornecido suporte t√©cnico")
    
    return state

def atender_geral(state: EstadoAtendimento) -> EstadoAtendimento:
    """Atende perguntas gerais"""
    resposta = """
    üëã OL√Å! Sou o assistente da TechStore!
    
    üõçÔ∏è Como posso te ajudar?
    
    ‚Ä¢ üí∞ Informa√ß√µes sobre pre√ßos
    ‚Ä¢ üì± Cat√°logo de produtos
    ‚Ä¢ üõ†Ô∏è Suporte t√©cnico
    ‚Ä¢ üìû Falar com atendente humano
    
    √â s√≥ me perguntar o que voc√™ precisa!
    """
    
    state["resposta"] = resposta
    state["acao"] = "atendimento_geral"
    state["historico"].append("Fornecido atendimento geral")
    
    return state

# Criando o grafo de decis√£o
def criar_grafo_atendimento():
    """Cria o grafo de atendimento"""
    
    # Criando o grafo
    workflow = StateGraph(EstadoAtendimento)
    
    # Adicionando n√≥s (esta√ß√µes)
    workflow.add_node("categorizar", categorizar_pergunta)
    workflow.add_node("precos", atender_precos)
    workflow.add_node("produtos", atender_produtos)
    workflow.add_node("suporte", atender_suporte)
    workflow.add_node("geral", atender_geral)
    
    # Definindo o ponto de entrada
    workflow.set_entry_point("categorizar")
    
    # Definindo as rotas (linhas do metr√¥)
    def rotear_por_categoria(state: EstadoAtendimento) -> str:
        """Decide qual rota seguir baseado na categoria"""
        categoria = state["categoria"]
        return categoria
    
    # Conectando as rotas
    workflow.add_conditional_edges(
        "categorizar",
        rotear_por_categoria,
        {
            "precos": "precos",
            "produtos": "produtos",
            "suporte": "suporte",
            "geral": "geral"
        }
    )
    
    # Conectando ao final
    workflow.add_edge("precos", END)
    workflow.add_edge("produtos", END)
    workflow.add_edge("suporte", END)
    workflow.add_edge("geral", END)
    
    # Compilando o grafo
    app = workflow.compile()
    
    return app

# Testando o LangGraph
app_atendimento = criar_grafo_atendimento()

perguntas_teste = [
    "Quanto custa o iPhone 15 Pro Max?",
    "Quais produtos voc√™s t√™m da Apple?",
    "Meu iPhone n√£o est√° carregando, o que fa√ßo?",
    "Ol√°! Como voc√™s est√£o?"
]

for i, pergunta in enumerate(perguntas_teste, 1):
    print(f"\n‚ùì Pergunta {i}: {pergunta}")
    
    try:
        # Estado inicial
        estado_inicial = {
            "pergunta": pergunta,
            "categoria": "",
            "resposta": "",
            "acao": "",
            "historico": []
        }
        
        # Executando o grafo
        resultado = app_atendimento.invoke(estado_inicial)
        
        print(f"üè∑Ô∏è  Categoria: {resultado['categoria']}")
        print(f"üéØ A√ß√£o: {resultado['acao']}")
        print(f"ü§ñ Resposta: {resultado['resposta'][:200]}...")
        print(f"üìã Hist√≥rico: {resultado['historico']}")
        
    except Exception as e:
        print(f"‚ùå Erro: {e}")
    
    print("-" * 40)
```

### **Aula 10.2: Integra√ß√£o com APIs Externas**

#### **Sistema de Integra√ß√£o com APIs**

```python
import requests
import json
from datetime import datetime

class SistemaIntegracao:
    def __init__(self):
        self.base_url = "https://jsonplaceholder.typicode.com"  # API de exemplo
    
    def buscar_usuarios(self):
        """Busca usu√°rios da API externa"""
        try:
            response = requests.get(f"{self.base_url}/users")
            return response.json()
        except Exception as e:
            return f"Erro ao buscar usu√°rios: {e}"
    
    def buscar_posts(self, user_id=1):
        """Busca posts de um usu√°rio"""
        try:
            response = requests.get(f"{self.base_url}/posts?userId={user_id}")
            return response.json()
        except Exception as e:
            return f"Erro ao buscar posts: {e}"
    
    def criar_post(self, titulo, corpo, user_id=1):
        """Cria um novo post"""
        try:
            data = {
                "title": titulo,
                "body": corpo,
                "userId": user_id
            }
            response = requests.post(f"{self.base_url}/posts", json=data)
            return response.json()
        except Exception as e:
            return f"Erro ao criar post: {e}"

# Criando ferramenta de API para LangChain
from langchain.tools import BaseTool
from typing import Optional

class APITool(BaseTool):
    name = "api_externa"
    description = "Use para acessar dados de APIs externas"
    sistema: SistemaIntegracao = None
    
    def _run(self, query: str) -> str:
        """Executa consultas na API externa"""
        try:
            if "usu√°rios" in query.lower() or "user" in query.lower():
                usuarios = self.sistema.buscar_usuarios()
                return f"Usu√°rios encontrados: {len(usuarios)}. Primeiro usu√°rio: {usuarios[0]['name']}"
            
            elif "posts" in query.lower():
                posts = self.sistema.buscar_posts()
                return f"Posts encontrados: {len(posts)}. Primeiro post: {posts[0]['title']}"
            
            else:
                return "Comando n√£o reconhecido. Use 'usu√°rios' ou 'posts'."
                
        except Exception as e:
            return f"Erro na API: {e}"

# Criando inst√¢ncia
sistema_api = SistemaIntegracao()
api_tool = APITool()
api_tool.sistema = sistema_api
```

### **Aula 10.3: Sistema de Workflow Inteligente Completo**

#### **Workflow com LangGraph e APIs**

```python
class EstadoWorkflow(TypedDict):
    """Estado do workflow inteligente"""
    comando: str
    tipo_acao: str
    dados: dict
    resultado: str
    status: str
    timestamp: str

def analisar_comando(state: EstadoWorkflow) -> EstadoWorkflow:
    """Analisa o comando e decide a a√ß√£o"""
    comando = state["comando"].lower()
    
    if "usu√°rio" in comando or "user" in comando:
        state["tipo_acao"] = "buscar_usuarios"
    elif "post" in comando:
        if "criar" in comando or "novo" in comando:
            state["tipo_acao"] = "criar_post"
        else:
            state["tipo_acao"] = "buscar_posts"
    else:
        state["tipo_acao"] = "comando_invalido"
    
    state["timestamp"] = datetime.now().isoformat()
    return state

def executar_acao(state: EstadoWorkflow) -> EstadoWorkflow:
    """Executa a a√ß√£o determinada"""
    tipo_acao = state["tipo_acao"]
    sistema = SistemaIntegracao()
    
    try:
        if tipo_acao == "buscar_usuarios":
            resultado = sistema.buscar_usuarios()
            state["resultado"] = f"Usu√°rios encontrados: {len(resultado)}"
            state["status"] = "sucesso"
            
        elif tipo_acao == "buscar_posts":
            resultado = sistema.buscar_posts()
            state["resultado"] = f"Posts encontrados: {len(resultado)}"
            state["status"] = "sucesso"
            
        elif tipo_acao == "criar_post":
            resultado = sistema.criar_post(
                titulo="Post autom√°tico",
                corpo="Criado pelo sistema de IA"
            )
            state["resultado"] = f"Post criado com ID: {resultado.get('id')}"
            state["status"] = "sucesso"
            
        else:
            state["resultado"] = "Comando n√£o reconhecido"
            state["status"] = "erro"
            
    except Exception as e:
        state["resultado"] = f"Erro: {e}"
        state["status"] = "erro"
    
    return state

def criar_workflow_completo():
    """Cria o workflow completo"""
    
    # Criando grafo
    workflow = StateGraph(EstadoWorkflow)
    
    # Adicionando n√≥s
    workflow.add_node("analisar", analisar_comando)
    workflow.add_node("executar", executar_acao)
    
    # Definindo entrada e sa√≠da
    workflow.set_entry_point("analisar")
    workflow.add_edge("analisar", "executar")
    workflow.add_edge("executar", END)
    
    # Compilando
    app = workflow.compile()
    
    return app

# Testando o workflow completo
workflow_app = criar_workflow_completo()

comandos_teste = [
    "Buscar todos os usu√°rios",
    "Mostrar posts do usu√°rio 1",
    "Criar um novo post",
    "Comando inv√°lido"
]

for i, comando in enumerate(comandos_teste, 1):
    print(f"\nüéØ Comando {i}: {comando}")
    
    try:
        # Estado inicial
        estado_inicial = {
            "comando": comando,
            "tipo_acao": "",
            "dados": {},
            "resultado": "",
            "status": "",
            "timestamp": ""
        }
        
        # Executando workflow
        resultado = workflow_app.invoke(estado_inicial)
        
        print(f"üîç A√ß√£o: {resultado['tipo_acao']}")
        print(f"üìä Status: {resultado['status']}")
        print(f"üìù Resultado: {resultado['resultado']}")
        print(f"‚è∞ Timestamp: {resultado['timestamp']}")
        
    except Exception as e:
        print(f"‚ùå Erro: {e}")
    
    print("-" * 30)
```

---

## **üéâ Conclus√£o - Voc√™ √© um Especialista em LangChain!**

### **O que Voc√™ Aprendeu**

Ao longo deste curso completo, voc√™ dominou:

‚úÖ **Fundamentos**: O que √© LangChain e por que √© revolucion√°rio
‚úÖ **Prompts**: A arte de falar com IA de forma eficiente
‚úÖ **Chains**: Como conectar funcionalidades como pe√ßas de Lego
‚úÖ **Memory**: Como fazer a IA lembrar como um amigo
‚úÖ **Agents**: Funcion√°rios inteligentes que podem usar ferramentas
‚úÖ **Document Loaders**: Como fazer a IA ler qualquer documento
‚úÖ **RAG**: IA que consulta documentos antes de responder
‚úÖ **Projetos Pr√°ticos**: Sistemas reais e funcionais
‚úÖ **Deploy**: Como colocar projetos na internet
‚úÖ **T√≥picos Avan√ßados**: LangGraph e integra√ß√µes complexas

### **Compara√ß√£o: Antes vs Depois**

| Aspecto | Antes do Curso | Depois do Curso |
|---------|----------------|-----------------|
| **Conhecimento** | B√°sico sobre IA | Especialista em LangChain |
| **Capacidade** | Usar ChatGPT | Criar aplica√ß√µes de IA |
| **Projetos** | Nenhum | Sistemas completos |
| **Mercado** | Consumidor | Produtor de solu√ß√µes |
| **Valor** | Baixo | Alto (desenvolvedor de IA) |

### **Pr√≥ximos Passos Recomendados**

1. **üîÑ Pr√°tica**: Continue construindo projetos
2. **üåê Comunidade**: Participe de f√≥runs e grupos
3. **üìö Atualiza√ß√µes**: Mantenha-se atualizado com novas vers√µes
4. **üíº Portf√≥lio**: Crie um portf√≥lio de projetos
5. **üöÄ Deploy**: Coloque seus projetos na internet

### **Recursos Adicionais**

- **üìñ Documenta√ß√£o Oficial**: [langchain.com](https://langchain.com)
- **üêô GitHub**: [github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
- **üí¨ Discord**: Comunidade oficial do LangChain
- **üì∫ YouTube**: Canais especializados em IA
- **üì± Redes Sociais**: Siga especialistas da √°rea

### **Mensagem Final**

**Parab√©ns, meu consagrado!** üéâ

Voc√™ acabou de completar um dos cursos mais completos de LangChain dispon√≠veis em portugu√™s. Agora voc√™ tem o conhecimento e as ferramentas para:

- üöÄ **Criar aplica√ß√µes de IA** que resolvem problemas reais
- üíº **Se destacar no mercado** como desenvolvedor especializado
- üåü **Inovar** com solu√ß√µes criativas e inteligentes
- üéØ **Impactar** a vida das pessoas com tecnologia

**Lembre-se**: A IA √© uma ferramenta poderosa, mas **voc√™** √© quem decide como us√°-la. Use esse conhecimento para criar coisas incr√≠veis e fazer a diferen√ßa no mundo!

**O futuro da programa√ß√£o √© agora, e voc√™ est√° na vanguarda!** üöÄ

---

**Desenvolvido com ‚ù§Ô∏è por Pedro Guth**

*"A melhor forma de prever o futuro √© cri√°-lo." - Peter Drucker*
